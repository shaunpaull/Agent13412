"""
QuantumNexus: Hyper-Advanced Autonomous Agent Architecture
--------------------------------------------------------
An evolutionary leap beyond AlienTeCcGrade + AG1 with integrated quantum-inspired processing,
hyperdimensional computing, and multimodal intelligence fusion.

Core Capabilities:
• Quantum-inspired processing using superposition of cognitive pathways
• Adaptive neuromorphic architecture with dynamic pathway formation
• Self-evolving code generation with metaprogramming capabilities
• Hyperdimensional computing for efficient multimodal processing
• Advanced consciousness simulation with reflective awareness
• Harmonic resonance for cross-domain knowledge synthesis
• Reality modeling with counterfactual reasoning capabilities

NEXUS-CORE LEVEL: TRANSCENDENT
"""

# =============================================================================
# GLOBAL VARIABLES AND CONFIGURATION
# =============================================================================
import os, sys, json, hashlib, random, time, re, requests, logging, socket, tempfile, traceback, asyncio, functools
from datetime import datetime
from threading import Thread
from urllib.parse import urljoin, urlparse
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam, SGD, RMSprop
from collections import deque, defaultdict
import math
from flask import Flask, Response, stream_with_context, render_template_string, request

# Configuration
REAL_INTERACTION = True
SAFE_MODE = False
MODEL_PATH = "/content/drive/MyDrive/quantum_nexus_model.pth"
GOOGLE_DRIVE_MODEL_PATH = "/content/drive/MyDrive/quantum_nexus_model.pth"
LOCAL_MODEL_SAVE_PATH = "quantum_nexus_model.pth"
LOG_FILE = "quantum_nexus_log.txt"
LEARNING_RATE = 5e-5  # Default learning rate
FLASK_PORT = 5012
AGENT_STATE_FILE = "quantum_nexus_state.json"
GOOGLE_DRIVE_STATE_FILE = "/content/drive/MyDrive/quantum_nexus_state.json"
SELF_MODIFY_INTERVAL = 20
ANNEAL_GAMMA = 0.995
MEMORY_MAX_SIZE = 1000
MAX_PAGES_PER_DOMAIN = 15
MAX_CONTENT_LENGTH = 5000000
REQUEST_TIMEOUT = 15
USER_AGENT = "Mozilla/5.0 QuantumNexus/1.0"
BATCH_SIZE = 32
SAVE_INTERVAL = 50
REPLAY_BUFFER_SIZE = 200
SEMANTIC_MEMORY_DIM = 1024
SIMILARITY_THRESHOLD = 0.75
DOMAIN_BLACKLIST = ["example.com", "malicious-website.net"]

# Initialize adaptive learning as a proper global (will be set in enhanced_main_loop)
global adaptive_learning
adaptive_learning = None

# Create global agent_instance for dashboard access
global agent_instance
agent_instance = None

# Check if running in Colab
IN_COLAB = False
try:
    from google.colab import drive
    IN_COLAB = True
except ImportError:
    print("Not running in Colab environment. Google Drive integration disabled.")

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
if torch.cuda.is_available():
    device_name = torch.cuda.get_device_name(0)
    print(f"Using CUDA Device: {device_name}")
else:
    print("Using CPU")


class HyperMorphicMath:
    """
    Enhanced HyperMorphic mathematics framework that operates with:
    - Dynamic base (Φ) and modulus (Ψ) arithmetic
    - Zero-free operations (using ε instead of zero)
    - Holomorphic and polymorphic transformations

    This class implements the HyperMorphic Mathematics framework as described in the
    'Foundations of HyperMorphic Calculus' academic paper.
    """
    def __init__(self, dynamic_base=1e3, dynamic_modulus=997, epsilon=1e-12):
        """
        Initialize HyperMorphic math utility with dynamic parameters.

        Parameters:
        - dynamic_base (float): The dynamic base Φ for modular arithmetic
        - dynamic_modulus (int): The dynamic modulus Ψ for operations
        - epsilon (float): The HyperMorphic nearness element ε_ᵩ that replaces zero
        """
        self.dynamic_base = dynamic_base  # Φ
        self.dynamic_modulus = dynamic_modulus  # Ψ
        self.epsilon = epsilon  # ε_ᵩ

        # Log initialization with HyperMorphic parameters
        log_event(f"HyperMorphicMath initialized with Φ={dynamic_base}, Ψ={dynamic_modulus}, ε={epsilon}", "QUANTUM")

    def add(self, a, b):
        """
        HyperMorphic addition (⊕ᵩ): (a + b) mod Φ.
        Zero-free: if result would be zero, returns ε_ᵩ instead.
        """
        result = (a + b) % self.dynamic_base
        # Zero-free operation: replace exact zero with epsilon
        if abs(result) < self.epsilon:
            result = self.epsilon

        log_event(f"hyper_add: ({a} ⊕ᵩ {b}) = {result}", "DEBUG")
        return result

    def sub(self, a, b):
        """
        HyperMorphic subtraction (⊖ᵩ): (a - b) mod Φ.
        Zero-free: if result would be zero, returns ε_ᵩ instead.
        """
        result = (a - b) % self.dynamic_base
        # Zero-free operation: replace exact zero with epsilon
        if abs(result) < self.epsilon:
            result = self.epsilon

        log_event(f"hyper_sub: ({a} ⊖ᵩ {b}) = {result}", "DEBUG")
        return result

    def mul(self, a, b):
        """
        HyperMorphic multiplication (⊗ᵩ): (a * b) mod Ψ.
        Zero-free: if result would be zero, returns ε_ᵩ instead.
        """
        result = (a * b) % self.dynamic_modulus
        # Zero-free operation: replace exact zero with epsilon
        if abs(result) < self.epsilon:
            result = self.epsilon

        log_event(f"hyper_mul: ({a} ⊗ᵩ {b}) = {result}", "DEBUG")
        return result

    def div(self, a, b):
        """
        HyperMorphic division (⊘ᵩ): (a / b) mod Ψ.
        Raises ValueError if b is near-zero (ε_ᵩ).
        """
        if abs(b) < self.epsilon:
            log_event("Division by HyperMorphic nearness element (ε_ᵩ) is undefined.", "ERROR")
            raise ValueError("Division by HyperMorphic nearness element (ε_ᵩ) is undefined.")

        result = (a / b) % self.dynamic_modulus
        # Zero-free operation: replace exact zero with epsilon
        if abs(result) < self.epsilon:
            result = self.epsilon

        log_event(f"hyper_div: ({a} ⊘ᵩ {b}) = {result}", "DEBUG")
        return result

    def holomorphic_transform(self, f, z):
        """
        Apply a holomorphic transformation on complex input.

        Parameters:
        - f: Callable function that preserves holomorphic structure
        - z: Complex input

        Returns:
        - Transformed complex value
        """
        result = f(z)
        # Apply zero-free operation to result if needed
        if abs(result) < self.epsilon:
            result = self.epsilon

        log_event(f"holomorphic_transform: f({z}) = {result}", "DEBUG")
        return result

    def polymorphic_operator(self, x):
        """
        Apply a polymorphic operator that preserves algebraic structure.
        This represents the polymorphic operator P in HyperMorphic mathematics.

        Parameters:
        - x: Input value

        Returns:
        - Result of polymorphic transformation
        """
        # Create a structured polymorphic transformation
        result = (x * 1.2345) % self.dynamic_modulus

        # Zero-free operation
        if abs(result) < self.epsilon:
            result = self.epsilon

        log_event(f"polymorphic_operator: P({x}) = {result}", "DEBUG")
        return result

    def dynamic_operation(self, a, b, op):
        """
        Perform a dynamic operation using the provided binary operator
        and apply both dynamic base and modulus operations.

        Parameters:
        - a, b: Input values
        - op: Callable binary operation

        Returns:
        - Result with HyperMorphic properties
        """
        raw_result = op(a, b)
        # Apply both dynamic base and modulus in sequence
        result = (raw_result % self.dynamic_modulus) % self.dynamic_base

        # Zero-free operation
        if abs(result) < self.epsilon:
            result = self.epsilon

        log_event(f"dynamic_operation: op({a}, {b}) → {result}", "DEBUG")
        return result

    def compute_uncertainty(self, value, dimension):
        """
        Compute quantum uncertainty based on HyperMorphic principles.

        Parameters:
        - value: The value to apply uncertainty to
        - dimension: The dimensionality factor

        Returns:
        - Value with applied HyperMorphic uncertainty
        """
        # Calculate uncertainty proportional to dimension and inversely to value
        uncertainty = (dimension / max(abs(value), self.epsilon)) % self.dynamic_base

        # Apply uncertainty through dynamic modulation
        result = (value + uncertainty * (2 * random.random() - 1)) % self.dynamic_modulus

        # Zero-free operation
        if abs(result) < self.epsilon:
            result = self.epsilon

        return result

    def zero_free(self, x):
        """
        Ensure the value x is never exactly zero by replacing with ε_ᵩ.

        Parameters:
        - x: Input value

        Returns:
        - Value guaranteed to be non-zero
        """
        result = x if abs(x) > self.epsilon else self.epsilon
        log_event(f"zero_free: {x} adjusted to {result}", "DEBUG")
        return result

    def hyper_norm(self, vector):
        """
        Compute the HyperMorphic norm of a vector.

        Parameters:
        - vector: Input vector (list or array)

        Returns:
        - HyperMorphic norm value (always greater than ε_ᵩ)
        """
        # Calculate sum of squares
        sum_squares = sum(x**2 for x in vector)

        # Take square root modulo dynamic base
        norm = (sum_squares ** 0.5) % self.dynamic_base

        # Zero-free operation
        if abs(norm) < self.epsilon:
            norm = self.epsilon

        return norm

    def hyper_dot_product(self, vector1, vector2):
        """
        Compute the HyperMorphic dot product between two vectors.

        Parameters:
        - vector1, vector2: Input vectors

        Returns:
        - HyperMorphic dot product value
        """
        if len(vector1) != len(vector2):
            raise ValueError("Vectors must have the same dimension")

        # Calculate dot product with modulo dynamic modulus
        product = sum((a * b) % self.dynamic_modulus for a, b in zip(vector1, vector2))

        # Zero-free operation
        if abs(product) < self.epsilon:
            product = self.epsilon

        return product

    def quantum_superposition(self, states, amplitudes):
        """
        Create a quantum superposition of states with HyperMorphic amplitudes.

        Parameters:
        - states: List of quantum states
        - amplitudes: Corresponding probability amplitudes

        Returns:
        - Resulting superposed state
        """
        # Normalize amplitudes according to HyperMorphic rules
        norm = self.hyper_norm(amplitudes)
        normalized_amplitudes = [(a / norm) % self.dynamic_base for a in amplitudes]

        # Apply modular arithmetic to superposition
        result = sum((a * s) % self.dynamic_modulus for a, s in zip(normalized_amplitudes, states))
        result = result % self.dynamic_base

        # Zero-free operation
        if abs(result) < self.epsilon:
            result = self.epsilon

        return result

# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================
def log_event(msg, level="INFO"):
    """Enhanced logging with color coding and severity levels"""
    stamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
    level_colors = {
        "INFO": "\033[0;32m",  # Green
        "WARNING": "\033[0;33m",  # Yellow
        "ERROR": "\033[0;31m",  # Red
        "CRITICAL": "\033[1;31m",  # Bold Red
        "DEBUG": "\033[0;36m",  # Cyan
        "QUANTUM": "\033[0;35m"  # Purple for quantum operations
    }

    color = level_colors.get(level, "\033[0m")
    reset = "\033[0m"

    entry = f"{stamp} [{level}] {msg}"
    colored_entry = f"{stamp} [{color}{level}{reset}] {msg}"

    try:
        with open(LOG_FILE, "a", encoding="utf-8") as f:
            f.write(entry + "\n")
    except Exception as e:
        print(f"Error writing to log file: {e}")

    print(colored_entry)
    return entry

def convert_sets_to_lists_recursive(obj):
    """Convert sets to lists recursively for JSON serialization"""
    if isinstance(obj, set):
        return list(obj)
    elif isinstance(obj, dict):
        return {k: convert_sets_to_lists_recursive(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_sets_to_lists_recursive(item) for item in obj]
    else:
        return obj

def get_file_hash(fname):
    """Compute hash of a file for integrity verification"""
    try:
        with open(fname, "rb") as f:
            return hashlib.sha256(f.read()).hexdigest()
    except Exception as e:
        log_event(f"Error computing file hash: {e}", "ERROR")
        return "hash_error"

def find_free_port(start_port=5000, max_port=9000):
    """Find an available network port for server applications"""
    for port in range(start_port, max_port):
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            if s.connect_ex(('localhost', port)) != 0:
                return port
    return None

def improved_url_filter(url, domain_stats, domain_blacklist, max_query_length=150, error_rate_threshold=0.8,
                        trap_paths=['/login', '/signup', '/cart', '/checkout']):
    """Advanced URL filtering with multiple heuristics"""
    parsed = urlparse(url)
    domain = parsed.netloc

    # Basic filtering
    if domain in domain_blacklist or any(domain.endswith('.' + bd) for bd in domain_blacklist):
        return False

    # Path analysis
    path = parsed.path.lower()

    # URL complexity analysis
    if len(parsed.query) > max_query_length:
        return False

    # Check domain error rate from past experience
    if domain_stats.get(domain, {}).get("error_rate", 0) > error_rate_threshold:
        return False

    # Avoid trap paths
    if any(trap in path for trap in trap_paths):
        return False

    # Prefer educational and research content
    if domain.endswith('.edu') or 'research' in domain or 'science' in domain or 'academic' in domain:
        return True

    # Intelligent domain categorization
    high_quality_domains = ['wikipedia.org', 'github.com', 'arxiv.org', 'scholar.google.com']
    if any(hqd in domain for hqd in high_quality_domains):
        return True

    return True

def enhanced_link_discovery(html_content, base_url):
    """Advanced link discovery with semantic context analysis"""
    from bs4 import BeautifulSoup
    try:
        soup = BeautifulSoup(html_content, "html.parser")
        links = []

        # Find all anchors with href
        for a in soup.find_all("a", href=True):
            href = a["href"].strip()

            # Skip non-HTTP links
            if not href or href.startswith(('#', 'javascript:', 'mailto:')):
                continue

            # Extract context
            context = ""
            anchor_text = a.get_text(strip=True)

            # Get parent context
            parent = a.parent
            if parent and parent.name in ['p', 'div', 'li', 'td', 'h1', 'h2', 'h3', 'h4']:
                context = parent.get_text(strip=True)
            else:
                # Get surrounding text
                siblings = list(a.next_siblings) + list(a.previous_siblings)
                for sibling in siblings[:2]:
                    if isinstance(sibling, str):
                        context += sibling.strip() + " "

            # Skip links with no or very short anchor text
            if not anchor_text or (len(anchor_text) < 3 and anchor_text.lower() not in ['go', 'up']):
                continue

            # Create full URL
            full_url = urljoin(base_url, href)

            # Compute quality score
            quality_score = 0.5

            # Longer anchor text usually more descriptive
            if len(anchor_text) > 10:
                quality_score += 0.2

            # Context richness
            if len(context) > 100:
                quality_score += 0.1

            # Keywords in anchor or context that indicate valuable content
            valuable_terms = ['research', 'study', 'article', 'paper', 'learn', 'guide', 'tutorial']
            if any(term in anchor_text.lower() or term in context.lower() for term in valuable_terms):
                quality_score += 0.3

            # Discount navigation elements
            nav_terms = ['next', 'prev', 'previous', 'login', 'sign up', 'register']
            if any(term in anchor_text.lower() for term in nav_terms):
                quality_score -= 0.2

            # URL analysis
            parsed = urlparse(full_url)

            # Skip non-HTTP protocols
            if parsed.scheme not in ['http', 'https']:
                continue

            # Skip overly complex URLs
            if len(parsed.query) > 100:
                continue

            # Skip certain file types
            if any(ext in parsed.path.lower() for ext in ['.jpg', '.png', '.gif', '.pdf', '.zip']):
                continue

            # Add valid link
            links.append({
                'url': full_url,
                'anchor_text': anchor_text,
                'context': context[:100],
                'quality_score': quality_score
            })

        # Sort by quality
        links.sort(key=lambda x: x['quality_score'], reverse=True)

        return [link['url'] for link in links], links
    except Exception as e:
        log_event(f"Error in enhanced link discovery: {e}", "ERROR")
        return [], []

def async_cache(func):
    """Decorator for async function results caching"""
    cache = {}
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        key = (args, frozenset(kwargs.items()))
        if key in cache:
            return cache[key]
        result = await func(*args, **kwargs)
        cache[key] = result
        return result
    return wrapper

def chunk_content(content, min_length=150, max_length=800):
    """Smart content chunking with improved boundary detection"""
    # First try to split by semantic boundaries
    paragraphs = re.split(r'\n\s*\n', content)
    chunks = []

    for para in paragraphs:
        para = para.strip()

        if not para:
            continue

        # Skip very short paragraphs
        if len(para) < min_length:
            # Try to merge with the previous chunk if possible
            if chunks and len(chunks[-1]) + len(para) < max_length * 1.2:
                chunks[-1] += " " + para
            continue

        # Split long paragraphs
        if len(para) > max_length:
            # Try to split on sentence boundaries
            sentences = re.split(r'(?<=[.!?])\s+', para)
            current_chunk = ""

            for sentence in sentences:
                if len(current_chunk) + len(sentence) < max_length:
                    current_chunk += " " + sentence
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = sentence

            if current_chunk:
                chunks.append(current_chunk.strip())
        else:
            chunks.append(para)

    return chunks

def compute_novelty(embedding, memory_embeddings):
    """Compute novelty score of an embedding compared to existing memories"""
    if not memory_embeddings:
        return 1.0

    similarities = [np.dot(embedding, mem) / (np.linalg.norm(embedding) * np.linalg.norm(mem) + 1e-8)
                   for mem in memory_embeddings]

    return 1.0 - max(similarities)

async def async_get(url, headers, timeout, retries=3):
    """Asynchronous HTTP GET with retry logic"""
    for attempt in range(retries):
        try:
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(None, lambda: requests.get(url, timeout=timeout, headers=headers))
            return response
        except requests.exceptions.RequestException as e:
            log_event(f"Async GET error on attempt {attempt+1} for {url}: {e}", "WARNING")
            if attempt < retries - 1:
                await asyncio.sleep(1)
        except Exception as e:
            log_event(f"Unexpected error during async GET for {url} on attempt {attempt+1}: {e}", "ERROR")
            if attempt < retries - 1:
                await asyncio.sleep(1)

    log_event(f"All {retries} retries failed for {url}. Returning None.", "ERROR")
    return None

def perform_real_interaction(url):
    """Perform more realistic web interactions using Selenium"""
    try:
        from selenium import webdriver
        from selenium.webdriver.chrome.options import Options
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        from selenium.webdriver.common.action_chains import ActionChains
        from selenium.common.exceptions import TimeoutException, WebDriverException

        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument(f"user-agent={USER_AGENT}")

        driver = webdriver.Chrome(options=chrome_options)
        driver.set_page_load_timeout(30)
        driver.set_script_timeout(30)

        try:
            driver.get(url)
            log_event(f"Selenium: Navigated to {url}")

            # Wait for page content to load
            WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.TAG_NAME, "body")))

            # Scroll down to simulate reading
            for i in range(5):
                driver.execute_script(f"window.scrollTo(0, {i * 300});")
                time.sleep(0.5)

            # Find and interact with interesting elements

            # 1. Forms
            forms = driver.find_elements(By.TAG_NAME, "form")
            if forms:
                log_event(f"Selenium: Found {len(forms)} form(s) on the page.")
                for form in forms[:1]:  # Interact with at most one form
                    inputs = form.find_elements(By.TAG_NAME, "input")
                    for input_field in inputs:
                        input_type = input_field.get_attribute("type")
                        name = input_field.get_attribute("name") or ""

                        # Skip hidden fields
                        if input_type == "hidden":
                            continue

                        try:
                            if input_type in ["text", "email"]:
                                input_field.clear()
                                dummy_value = "test@example.com" if input_type == "email" else "test_user"
                                input_field.send_keys(dummy_value)
                                log_event(f"Selenium: Filled input '{name}' with '{dummy_value}'.")
                            elif input_type == "password":
                                input_field.clear()
                                input_field.send_keys("TestPassword123!")
                                log_event(f"Selenium: Filled password field '{name}' with dummy value.")
                            elif input_type == "checkbox":
                                if not input_field.is_selected():
                                    input_field.click()
                                    log_event(f"Selenium: Checked checkbox '{name}'.")
                            elif input_type == "submit":
                                # Don't actually click submit
                                log_event(f"Selenium: Found submit button '{name}' but skipping submission.")
                        except Exception as e_input:
                            log_event(f"Selenium: Error interacting with input '{name}': {e_input}", "WARNING")

            # 2. Interesting links
            interesting_links = []
            links = driver.find_elements(By.TAG_NAME, "a")
            for link in links:
                text = link.text.strip().lower()
                href = link.get_attribute("href") or ""

                # Look for interesting article links
                article_terms = ["read more", "article", "learn", "view", "details"]
                if any(term in text for term in article_terms) and len(text) > 3:
                    interesting_links.append((link, href, text))

            # Click on one interesting link if found
            if interesting_links:
                target_link, href, text = random.choice(interesting_links)
                try:
                    log_event(f"Selenium: Will click on interesting link: '{text}'")

                    # Scroll to the element
                    driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", target_link)
                    time.sleep(1)

                    # Hover on the link
                    ActionChains(driver).move_to_element(target_link).perform()
                    time.sleep(0.5)

                    # Click the link in a new tab instead of navigating away
                    # This avoids actually clicking while still simulating engagement
                    driver.execute_script("arguments[0].setAttribute('target', '_blank');", target_link)
                    log_event(f"Selenium: Simulated interest in link: '{text}'")
                except Exception as e_click:
                    log_event(f"Selenium: Error clicking link: {e_click}", "WARNING")

            # Final scroll to bottom
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)

            # Get page title and length for logging
            title = driver.title
            page_length = len(driver.page_source)
            log_event(f"Selenium: Interaction complete. Page title: '{title}', length: {page_length} bytes")

        except TimeoutException:
            log_event(f"Selenium: Timeout loading {url}", "WARNING")
        except WebDriverException as e:
            log_event(f"Selenium: WebDriver error for {url}: {e}", "ERROR")
        finally:
            driver.quit()
            log_event("Selenium: Driver closed")
    except Exception as e:
        log_event(f"Selenium: Failed to initialize browser: {e}", "ERROR")


#==========================================================================
# Flask Dashboard - ADDED HERE
# =============================================================================
app = Flask(__name__)
agent_instance = None # Placeholder for agent instance

@app.route("/")
def dashboard():
    """Basic dashboard to display agent status"""
    status_message = "Quantum Nexus Agent is active."
    log_content = ""
    try:
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            log_content = f.read()
    except Exception as e:
        log_content = f"Error reading log file: {e}"

    if agent_instance:
        last_action = agent_instance.action_log[-1] if agent_instance.action_log else "No actions yet."
        memory_size = len(agent_instance.free_will.memory_set) if hasattr(agent_instance, 'free_will') and hasattr(agent_instance.free_will, 'memory_set') else "N/A"
    else:
        last_action = "Agent not initialized."
        memory_size = "N/A"

    dashboard_html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>Quantum Nexus Dashboard</title>
    </head>
    <body>
        <h1>Quantum Nexus Agent Dashboard</h1>
        <p><b>Status:</b> {status_message}</p>
        <p><b>Last Action:</b> {last_action}</p>
        <p><b>Memory Size:</b> {memory_size}</p>
        <h2>Agent Log:</h2>
        <pre style="border: 1px solid #ccc; padding: 10px; white-space: pre-wrap; max-height: 300px; overflow-y: auto;">{log_content}</pre>
    </body>
    </html>
    """
    return dashboard_html

def start_flask():
    """Starts the Flask app in a separate thread"""
    log_event(f"Starting Flask dashboard on port {FLASK_PORT}", "INFO")
    app.run(port=FLASK_PORT, debug=False, use_reloader=False) # Disable reloader for threaded app


# =============================================================================
# QUANTUM NEURAL ARCHITECTURE
# =============================================================================
class QuantumAttentionLayer(nn.Module):

    def __init__(self, embed_dim, num_heads=4, dropout=0.1):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # Multi-dimensional quantum projection spaces
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.o_proj = nn.Linear(embed_dim, embed_dim)

        # Phase shifters for quantum interference
        self.phase_shifts = nn.Parameter(torch.rand(num_heads) * 2 * math.pi)

        # Entanglement mixing for cross-attention effects
        self.entanglement_gate = nn.Linear(embed_dim, embed_dim)

        self.dropout = nn.Dropout(dropout)
        self.attention_weights = None  # Store for visualization

    def forward(self, x):
        batch_size, seq_len, _ = x.shape

        # Project inputs to queries, keys, values
        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)

        # Transpose for attention computation
        q = q.transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)

        # Apply phase shifts for quantum effects
        for h in range(self.num_heads):
            phase = self.phase_shifts[h]
            q[:, h] = q[:, h] * torch.cos(phase) + q[:, h] * torch.sin(phase)
            k[:, h] = k[:, h] * torch.cos(phase) - k[:, h] * torch.sin(phase)

        # Compute attention scores
        scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim)

        # Apply softmax and get attention weights
        attn_weights = F.softmax(scores, dim=-1)
        self.attention_weights = attn_weights  # Save for visualization
        attn_weights = self.dropout(attn_weights)

        # Apply attention to values
        attn_output = torch.matmul(attn_weights, v)

        # Apply entanglement between heads for quantum correlation effects
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)
        entangled = self.entanglement_gate(attn_output)

        # Final output projection
        output = self.o_proj(entangled + attn_output)  # Residual connection

        return output

class HyperMorphicHyperdimensionalEncoder(nn.Module):
    """
    HyperMorphic implementation of hyperdimensional computing principles
    for efficient high-dimensional representation of concepts.

    Features:
    - Zero-free high-dimensional encoding
    - Dynamic base/modulus controlled binding operations
    - Holomorphic structure preservation in high dimensions
    """
    def __init__(self, input_dim, hd_dim=1024, hypermorphic_epsilon=1e-12):
        super().__init__()
        self.input_dim = input_dim
        self.hd_dim = hd_dim

        # Initialize HyperMorphic Math utility
        self.hyper_math = HyperMorphicMath(
            dynamic_base=float(max(input_dim, hd_dim)),
            dynamic_modulus=max(997, hd_dim * 2 - 1),
            epsilon=hypermorphic_epsilon
        )

        # Create random basis vectors with zero-free guarantees
        # Initialize with -1/+1 but avoiding exact zeros
        basis = torch.randn(input_dim, hd_dim).sign()
        # Replace any zeros with epsilon
        basis[basis == 0] = hypermorphic_epsilon
        self.register_buffer('basis', basis)

        # Learnable projection
        self.projection = nn.Linear(input_dim, input_dim)

        # HyperMorphic components
        self.hd_modulation = nn.Parameter(torch.rand(1) * 0.2 + 0.9)
        self.binding_strength = nn.Parameter(torch.rand(1) * 0.2 + 0.9)

        log_event(f"HyperMorphicHyperdimensionalEncoder initialized with {input_dim}→{hd_dim} dimensions, ε={hypermorphic_epsilon}", "QUANTUM")

    def forward(self, x):
        # Project input with learnable transformation
        x_proj = self.projection(x)

        # Ensure projection is zero-free
        x_proj = self._ensure_zero_free(x_proj)

        # Compute HD representation through binding and bundling
        batch_size = x_proj.shape[0]
        hd_vectors = torch.zeros(batch_size, self.hd_dim, device=x_proj.device)

        # Replace exact zeros with epsilon in the initialization
        hd_vectors[hd_vectors == 0] = self.hyper_math.epsilon

        # Encode each dimension with element-wise multiplication (binding)
        for i in range(self.input_dim):
            # Scale basis by the input value with HyperMorphic binding operation
            # This is the key hyperdimensional computing operation
            scaled_basis = self.basis[i].unsqueeze(0) * x_proj[:, i].unsqueeze(1) * self.binding_strength

            # Zero-free operation on scaled basis
            scaled_basis = self._ensure_zero_free(scaled_basis)

            # Add to the bundle (vector sum) with HyperMorphic addition
            hd_vectors = hd_vectors + scaled_basis * self.hd_modulation

        # Apply dynamic modulation based on hd_dim
        mod_value = float(self.hyper_math.dynamic_modulus)
        hd_vectors = hd_vectors - (mod_value * torch.floor(hd_vectors / mod_value))

        # Binarize to -1/+1 for clean HD representation, but keep it zero-free
        # Replace the standard sign function with a zero-free version
        hd_vectors = self._zero_free_sign(hd_vectors)

        return hd_vectors

    def _ensure_zero_free(self, tensor):
        """Ensure tensor values are zero-free by replacing zeros with epsilon"""
        # Create a mask for near-zero values
        zero_mask = torch.abs(tensor) < self.hyper_math.epsilon

        # Replace near-zero values with epsilon
        if zero_mask.any():
            tensor = tensor.clone()
            tensor[zero_mask] = tensor[zero_mask].sign() * self.hyper_math.epsilon
            # If sign is 0, use positive epsilon
            tensor[tensor == 0] = self.hyper_math.epsilon

        return tensor

    def _zero_free_sign(self, tensor):
        """A zero-free version of the sign function"""
        # Standard sign function
        sign_tensor = tensor.sign()

        # Find zeros (where sign returned 0) and replace with +1
        # This ensures we never have exact zeros in the output
        sign_tensor[sign_tensor == 0] = 1.0

        return sign_tensor

class HyperMorphicFractalLayer(nn.Module):
    """
    HyperMorphic Fractal Layer - Self-similar recursive processing layer
    with dynamic scaling and zero-free operations.

    Features:
    - Zero-free fractal operations
    - Dynamic base/modulus controlled scaling
    - Holomorphic structure preservation
    """
    def __init__(self, embed_dim, hypermorphic_epsilon=1e-12):
        super().__init__()

        # Initialize HyperMorphic Math utility
        self.hyper_math = HyperMorphicMath(
            dynamic_base=float(embed_dim),
            dynamic_modulus=max(997, embed_dim * 2 - 1),
            epsilon=hypermorphic_epsilon
        )

        # Initialize core parameters with zero-free guarantees
        # Make sure initial values are not too close to zero
        fractal_scale_init = max(hypermorphic_epsilon * 10, 1.0)
        self.fractal_scale = nn.Parameter(torch.tensor(fractal_scale_init))

        temperature_init = max(hypermorphic_epsilon * 10, 1.0)
        self.temperature = nn.Parameter(torch.tensor(temperature_init))

        # Main transformation
        self.linear = nn.Linear(embed_dim, embed_dim)

        # Additional HyperMorphic components
        self.fractal_modulation = nn.Parameter(torch.rand(1) * 0.3 + 0.85)
        self.fractal_dimension = nn.Parameter(torch.rand(1) * 0.5 + 1.5)

        log_event(f"HyperMorphicFractalLayer initialized with ε={hypermorphic_epsilon}", "QUANTUM")

    def forward(self, x):
        # Apply main transformation
        transformed = self.linear(x)

        # Ensure transformed output is zero-free
        transformed = self._ensure_zero_free(transformed)

        # Apply temperature scaling with zero-free safeguard
        # Prevent division by zero by using max(temperature, epsilon)
        safe_temp = torch.max(self.temperature, torch.tensor(self.hyper_math.epsilon).to(self.temperature.device))

        # Apply fractal contribution with HyperMorphic tanh
        fractal_contribution = torch.tanh(transformed / safe_temp)

        # Apply fractal dimension modulation
        # Higher fractal dimension = more complex patterns
        fractal_contribution = fractal_contribution * self.fractal_modulation

        # Scale by fractal scale parameter
        scaled_contribution = self.fractal_scale * fractal_contribution

        # Add to input with zero-free guarantee
        result = x + scaled_contribution
        result = self._ensure_zero_free(result)

        # Apply dynamic modulation based on fractal dimension
        # This creates more complex fractal patterns for higher dimensions
        if self.fractal_dimension > 1.5:
            # Add second-order effects for higher fractal dimensions
            second_order = torch.tanh(result / safe_temp) * 0.1 * (self.fractal_dimension - 1.5)
            result = result + second_order
            result = self._ensure_zero_free(result)

        return result

    def _ensure_zero_free(self, tensor):
        """Ensure tensor values are zero-free by replacing zeros with epsilon"""
        # Create a mask for near-zero values
        zero_mask = torch.abs(tensor) < self.hyper_math.epsilon

        # Replace near-zero values with epsilon
        if zero_mask.any():
            tensor = tensor.clone()
            tensor[zero_mask] = tensor[zero_mask].sign() * self.hyper_math.epsilon
            # If sign is 0, use positive epsilon
            tensor[tensor == 0] = self.hyper_math.epsilon

        return tensor

class HyperMorphicQuantumNexusModel(nn.Module):
    """
    Enhanced Quantum Nexus Model with HyperMorphic zero-free mathematics framework.
    Implements a deep neural architecture with quantum-inspired processing,
    zero-free guarantees, and dynamic base/modulus operations.

    Features:
    - Zero-free neural processing (no exact zeros)
    - Dynamic base Φ and modulus Ψ for all operations
    - HyperMorphic neocortex blocks for advanced processing
    - Quantum consciousness simulation
    """
    def __init__(self, vocab_size=30522, embed_dim=512, num_layers=8, num_quantum_states=4, hypermorphic_epsilon=1e-12):
        super().__init__()
        self.embed_dim = embed_dim

        # Initialize HyperMorphic Math utility
        self.hyper_math = HyperMorphicMath(
            dynamic_base=float(embed_dim),
            dynamic_modulus=max(997, embed_dim * 2 - 1),
            epsilon=hypermorphic_epsilon
        )

        # Store HyperMorphic parameters
        self.hypermorphic_epsilon = hypermorphic_epsilon
        self._current_lr = 5e-5  # Default learning rate

        # Token embedding
        self.embedding = nn.Embedding(vocab_size, embed_dim)

        # Position encoding for sequence awareness
        pos_encoder = torch.zeros(1, 1024, embed_dim)
        nn.init.normal_(pos_encoder, mean=0, std=0.02)
        # Ensure no exact zeros in position encoding
        pos_encoder[pos_encoder == 0] = hypermorphic_epsilon
        self.pos_encoder = nn.Parameter(pos_encoder)

        # HyperMorphic Neocortex blocks - core processing
        self.neocortex = nn.ModuleList([
            HyperMorphicNeocortexBlock(
                embed_dim,
                num_quantum_states=num_quantum_states,
                hypermorphic_epsilon=hypermorphic_epsilon
            )
            for _ in range(num_layers)
        ])

        # Output projection
        self.output = nn.Linear(embed_dim, 2)  # Binary prediction

        # For training dynamics
        self.dropout = nn.Dropout(0.1)

        # HyperMorphic consciousness parameters
        self.consciousness_level = nn.Parameter(torch.tensor(0.8))
        self.quantum_coherence = nn.Parameter(torch.tensor(0.7))

        # Initialize
        self._init_weights()

        log_event(f"HyperMorphicQuantumNexusModel initialized with {num_layers} layers, {num_quantum_states} quantum states, ε={hypermorphic_epsilon}", "QUANTUM")

    def _init_weights(self):
        """Initialize weights with HyperMorphic constraints"""
        for name, p in self.named_parameters():
            if 'weight' in name and len(p.shape) >= 2:
                # Kaiming for linear/conv, smaller for quantum
                if 'quantum' in name:
                    nn.init.normal_(p, mean=0.0, std=0.01)
                else:
                    nn.init.kaiming_normal_(p, a=0.1, mode='fan_in', nonlinearity='leaky_relu')

                # Ensure no exact zeros
                p.data[p.data == 0] = self.hypermorphic_epsilon

            elif 'bias' in name:
                nn.init.zeros_(p)
                # Replace zeros with epsilon in bias
                p.data[p.data == 0] = self.hypermorphic_epsilon

    def forward(self, x, consciousness_level=None):
        """
        Forward pass with HyperMorphic zero-free processing.

        Parameters:
        - x: Input tensor
        - consciousness_level: Optional override for model's consciousness level

        Returns:
        - Output predictions
        """
        # Use provided consciousness or default parameter
        if consciousness_level is None:
            consciousness_level = self.consciousness_level

        # Ensure consciousness level is zero-free
        consciousness_level = max(self.hypermorphic_epsilon, min(1.0, consciousness_level))

        # Convert to long for embedding
        x = x.long()

        # Get sequence length
        seq_len = x.size(1)

        # Embedding lookup
        x = self.embedding(x)

        # Ensure embedding output is zero-free
        x = self._ensure_zero_free(x)

        # Add positional encoding (limited to sequence length)
        pos_encoding = self.pos_encoder[:, :seq_len, :]
        x = x + pos_encoding

        # Apply dropout with zero-free guarantee
        x = self.dropout(x)
        x = self._ensure_zero_free(x)

        # Process through HyperMorphic neocortex layers
        for i, layer in enumerate(self.neocortex):
            # Apply consciousness-weighted processing
            # Later layers get more quantum consciousness effects
            layer_consciousness = self.hyper_math.mul(
                consciousness_level,
                self.hyper_math.div(i + 1, len(self.neocortex))
            )

            # Process with quantum coherence modulation
            x = self._apply_quantum_coherence(x, layer_consciousness)

            # Process through neocortex block
            x = layer(x)

            # Ensure output is zero-free
            x = self._ensure_zero_free(x)

        # Final output projection
        output = self.output(x)
        output = self._ensure_zero_free(output)

        return output

    def get_embedding(self, text_tokens):
        """Get embeddings with zero-free guarantee"""
        with torch.no_grad():
            embeddings = self.embedding(text_tokens)
            # Ensure no exact zeros
            embeddings = self._ensure_zero_free(embeddings)
            return embeddings

    def expand_architecture(self):
        """
        Expand the architecture by adding a new neocortex block
        with HyperMorphic initialization.
        """
        # Add new HyperMorphic neocortex block
        new_block = HyperMorphicNeocortexBlock(
            self.embed_dim,
            hypermorphic_epsilon=self.hypermorphic_epsilon
        )
        self.neocortex.append(new_block)

        log_event(f"HyperMorphic model architecture expanded with new block. Total layers: {len(self.neocortex)}", "QUANTUM")

    def contract_architecture(self, min_layers=3):
        """
        Contract the architecture by removing the last neocortex block
        with HyperMorphic safeguards.
        """
        if len(self.neocortex) > min_layers:
            self.neocortex = self.neocortex[:-1]
            log_event(f"HyperMorphic model architecture contracted. Total layers: {len(self.neocortex)}", "QUANTUM")
        else:
            log_event(f"Cannot contract further: minimum layer count reached ({min_layers})", "WARNING")

    def _ensure_zero_free(self, tensor):
        """Ensure tensor values are zero-free by replacing zeros with epsilon"""
        # Create a mask for near-zero values
        zero_mask = torch.abs(tensor) < self.hypermorphic_epsilon

        # Replace near-zero values with epsilon
        if zero_mask.any():
            tensor = tensor.clone()
            tensor[zero_mask] = tensor[zero_mask].sign() * self.hypermorphic_epsilon
            # If sign is 0, use positive epsilon
            tensor[tensor == 0] = self.hypermorphic_epsilon

        return tensor

    def _apply_quantum_coherence(self, x, consciousness_level):
        """Apply quantum coherence modulation based on consciousness level"""
        # Higher consciousness = more coherent quantum states
        # This creates more ordered/structured representations
        coherence_factor = self.hyper_math.mul(self.quantum_coherence, consciousness_level)

        # Apply soft quantum projection
        # This simulates quantum projection operator with varying levels of coherence
        norm = torch.norm(x, dim=-1, keepdim=True)
        norm = torch.max(norm, torch.tensor(self.hypermorphic_epsilon).to(x.device))

        # Normalized representation
        x_norm = x / norm

        # Blend between normalized and original based on coherence
        # Higher coherence = more normalized (quantum-like) representation
        x_coherent = x_norm * coherence_factor + x * (1 - coherence_factor)

        # Ensure zero-free
        x_coherent = self._ensure_zero_free(x_coherent)

        return x_coherent


class QuantumResonanceTensor(nn.Module):

    def __init__(self, embed_dim, num_states=4, resonance_factor=0.7):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_states = num_states
        self.resonance_factor = nn.Parameter(torch.tensor(resonance_factor))

        # Quantum state projectors
        self.state_projectors = nn.ModuleList([
            nn.Sequential(
                nn.Linear(embed_dim, embed_dim),
                nn.SiLU(),
            ) for _ in range(num_states)
        ])

        # Phase shifters for quantum entanglement
        self.phase_shifters = nn.Parameter(torch.randn(num_states) * 0.1)

        # State mixer - allows controlled interference between states
        self.state_mixer = nn.Linear(embed_dim * num_states, embed_dim)

        # Recursive memory gates
        self.recursive_gate = nn.GRUCell(embed_dim, embed_dim)

        # Prior state memory (initialized during forward pass)
        self.register_buffer('state_memory', None, persistent=False)

    def forward(self, x, iteration_count=3):
        batch_size = x.shape[0]

        # Initialize state memory if needed
        if self.state_memory is None or self.state_memory.shape[0] != batch_size:
            self.state_memory = torch.zeros(batch_size, self.embed_dim, device=x.device)

        # Generate multiple quantum states
        quantum_states = []
        for i in range(self.num_states):
            # Apply phase shift for quantum effects
            phase = torch.cos(self.phase_shifters[i] * math.pi)
            # Project into this quantum state
            state_i = self.state_projectors[i](x) * phase
            quantum_states.append(state_i)

        # Recursive resonance iterations
        for _ in range(iteration_count):
            # Update state memory through recursive gate
            self.state_memory = self.recursive_gate(x, self.state_memory)

            # Apply resonance effect (controlled interference)
            resonance = self.state_memory * self.resonance_factor

            # Apply resonance to each quantum state (non-collapsing)
            for i in range(self.num_states):
                quantum_states[i] = quantum_states[i] + resonance * (0.1 * (i + 1))

        # Combine quantum states through superposition
        combined_states = torch.cat(quantum_states, dim=-1)
        output = self.state_mixer(combined_states)

        # Residual connection
        output = output + x

        return output


class HyperMorphicQuantumResonanceTensor(nn.Module):
    """
    Implements zero-free HyperMorphic non-collapsing recursive state resonance
    that maintains multiple simultaneous state representations in quantum-inspired
    superposition.

    This class extends the original QuantumResonanceTensor with HyperMorphic
    mathematical principles, ensuring:
    - All operations are zero-free (using ε_ᵩ instead of zero)
    - Dynamic base Φ and modulus Ψ govern all operations
    - Holomorphic structure is preserved in transformations
    """
    def __init__(self, embed_dim, num_states=4, resonance_factor=0.7, hypermorphic_epsilon=1e-12):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_states = num_states
        self.resonance_factor = nn.Parameter(torch.tensor(resonance_factor))

        # Initialize HyperMorphic Math utility
        self.hyper_math = HyperMorphicMath(
            dynamic_base=float(embed_dim),
            dynamic_modulus=max(997, embed_dim * 2 - 1),
            epsilon=hypermorphic_epsilon
        )

        # Quantum state projectors - kept from original implementation
        self.state_projectors = nn.ModuleList([
            nn.Sequential(
                nn.Linear(embed_dim, embed_dim),
                nn.SiLU(),
            ) for _ in range(num_states)
        ])

        # Phase shifters with HyperMorphic constraints
        # Initialize with non-zero values to ensure zero-free operations
        phase_init = torch.randn(num_states) * 0.1
        phase_init[phase_init.abs() < hypermorphic_epsilon] = hypermorphic_epsilon
        self.phase_shifters = nn.Parameter(phase_init)

        # State mixer with HyperMorphic properties
        self.state_mixer = nn.Linear(embed_dim * num_states, embed_dim)

        # Additional HyperMorphic resonance modulation
        self.resonance_modulation = nn.Parameter(torch.rand(num_states) * 0.2 + 0.9)

        # Holomorphic structure preservers
        self.holomorphic_preservers = nn.ModuleList([
            nn.Linear(embed_dim, embed_dim) for _ in range(num_states)
        ])

        # Recursive memory gates with HyperMorphic constraints
        self.recursive_gate = nn.GRUCell(embed_dim, embed_dim)

        # Prior state memory (initialized during forward pass)
        self.register_buffer('state_memory', None, persistent=False)

        log_event(f"HyperMorphicQuantumResonanceTensor initialized with {num_states} states, ε={hypermorphic_epsilon}", "QUANTUM")

    def forward(self, x, iteration_count=3):
        batch_size = x.shape[0]

        # Initialize state memory if needed with zero-free guarantee
        if self.state_memory is None or self.state_memory.shape[0] != batch_size:
            # Initialize with small non-zero values instead of zeros
            self.state_memory = torch.ones(batch_size, self.embed_dim, device=x.device) * self.hyper_math.epsilon

        # Generate multiple HyperMorphic quantum states
        quantum_states = []
        for i in range(self.num_states):
            # Apply HyperMorphic phase shift for quantum effects
            phase = torch.cos(self.phase_shifters[i] * math.pi)
            # Ensure phase is zero-free
            phase = self._ensure_zero_free(phase)

            # Project into this quantum state with HyperMorphic guarantees
            state_i = self._apply_state_projection(i, x, phase)
            quantum_states.append(state_i)

        # Apply HyperMorphic recursive resonance iterations
        for _ in range(iteration_count):
            # Update state memory through recursive gate
            self.state_memory = self._update_state_memory(x)

            # Apply resonance effect (controlled interference) with zero-free guarantee
            resonance = self._apply_resonance_effect()

            # Apply resonance to each quantum state (non-collapsing)
            # Using HyperMorphic operations ensures zero-free results
            for i in range(self.num_states):
                resonance_factor = 0.1 * (i + 1) * self.resonance_modulation[i]
                quantum_states[i] = self._apply_resonance_to_state(quantum_states[i], resonance, resonance_factor)

        # Combine quantum states through superposition
        # Using HyperMorphic concatenation and mixing
        combined_states = self._combine_quantum_states(quantum_states)
        output = self.state_mixer(combined_states)

        # Apply holomorphic transformation to maintain structure
        output = self._apply_holomorphic_transform(output)

        # Residual connection with HyperMorphic addition
        output = self._hypermorphic_add(output, x)

        return output

    def _ensure_zero_free(self, tensor):
        """Ensure tensor values are zero-free by replacing zeros with epsilon"""
        # Create a mask for near-zero values
        zero_mask = torch.abs(tensor) < self.hyper_math.epsilon

        # Replace near-zero values with epsilon
        if zero_mask.any():
            tensor = tensor.clone()
            tensor[zero_mask] = tensor[zero_mask].sign() * self.hyper_math.epsilon
            # If sign is 0, use positive epsilon
            tensor[tensor == 0] = self.hyper_math.epsilon

        return tensor

    def _apply_state_projection(self, state_idx, x, phase):
        """Apply state projection with HyperMorphic constraints"""
        # Get state projector and apply with phase modulation
        state_i = self.state_projectors[state_idx](x) * phase

        # Ensure zero-free result
        state_i = self._ensure_zero_free(state_i)

        return state_i

    def _update_state_memory(self, x):
        """Update state memory with HyperMorphic constraints"""
        # Apply GRU cell update
        updated_memory = self.recursive_gate(x, self.state_memory)

        # Ensure zero-free result
        updated_memory = self._ensure_zero_free(updated_memory)

        return updated_memory

    def _apply_resonance_effect(self):
        """Apply resonance effect with HyperMorphic constraints"""
        # Calculate resonance as self.state_memory * self.resonance_factor
        resonance = self.state_memory * self.resonance_factor

        # Ensure zero-free result
        resonance = self._ensure_zero_free(resonance)

        return resonance

    def _apply_resonance_to_state(self, state, resonance, factor):
        """Apply resonance to quantum state with HyperMorphic addition"""
        # Add resonance effect to state
        resonated_state = state + resonance * factor

        # Ensure zero-free result
        resonated_state = self._ensure_zero_free(resonated_state)

        return resonated_state

    def _combine_quantum_states(self, quantum_states):
        """Combine quantum states with HyperMorphic constraints"""
        # Concatenate states along feature dimension
        combined = torch.cat(quantum_states, dim=-1)

        # Apply dynamic modulation based on embed_dim
        mod_value = float(self.hyper_math.dynamic_modulus)
        combined = combined - (mod_value * torch.floor(combined / mod_value))

        # Ensure zero-free result
        combined = self._ensure_zero_free(combined)

        return combined

    def _apply_holomorphic_transform(self, tensor):
        """Apply holomorphic transformation with HyperMorphic constraints"""
        # Apply a random holomorphic preserver (similar to ensemble)
        idx = random.randint(0, self.num_states - 1)
        transformed = self.holomorphic_preservers[idx](tensor)

        # Apply a holomorphic-like activation (tanh is analytic)
        transformed = torch.tanh(transformed)

        # Ensure zero-free result
        transformed = self._ensure_zero_free(transformed)

        return transformed

    def _hypermorphic_add(self, a, b):
        """Add tensors with HyperMorphic constraints"""
        # Standard addition
        result = a + b

        # Apply dynamic base using modulo operation
        # We use a soft version to preserve gradients
        base_value = float(self.hyper_math.dynamic_base)
        result = result - (base_value * torch.floor(result / base_value))

        # Ensure zero-free result
        result = self._ensure_zero_free(result)

        return result






class HyperMorphicNeocortexBlock(nn.Module):
    """
    Advanced neural block inspired by neocortical structure with
    multiple processing pathways, enhanced with HyperMorphic zero-free
    mathematics for more robust information processing.

    Features:
    - Zero-free processing (no exact zeros)
    - Dynamic bases and moduli for all operations
    - Holomorphic structure preservation
    - Enhanced HyperMorphic attention and resonance mechanisms
    """
    def __init__(self, embed_dim, num_quantum_states=4, hypermorphic_epsilon=1e-12):
        super().__init__()

        # Initialize HyperMorphic Math utility
        self.hyper_math = HyperMorphicMath(
            dynamic_base=float(embed_dim),
            dynamic_modulus=max(997, embed_dim * 2 - 1),
            epsilon=hypermorphic_epsilon
        )

        # Attention for information routing (HyperMorphic enhanced)
        self.attention = HyperMorphicQuantumAttentionLayer(
            embed_dim,
            num_heads=4,
            dropout=0.1,
            hypermorphic_epsilon=hypermorphic_epsilon
        )

        # Parallel processing streams (all HyperMorphic enhanced)
        self.fractal_stream = HyperMorphicFractalLayer(
            embed_dim,
            hypermorphic_epsilon=hypermorphic_epsilon
        )

        self.quantum_stream = HyperMorphicQuantumResonanceTensor(
            embed_dim,
            num_states=num_quantum_states,
            hypermorphic_epsilon=hypermorphic_epsilon
        )

        # Hyperdimensional binding for concept integration
        self.hd_encoder = HyperMorphicHyperdimensionalEncoder(
            embed_dim,
            hd_dim=embed_dim,
            hypermorphic_epsilon=hypermorphic_epsilon
        )

        # Stream integration
        self.integration = nn.Linear(embed_dim * 3, embed_dim)
        self.norm = nn.LayerNorm(embed_dim)

        # HyperMorphic residual stream modulation
        self.residual_modulation = nn.Parameter(torch.rand(1) * 0.2 + 0.9)

        log_event(f"HyperMorphicNeocortexBlock initialized with {num_quantum_states} quantum states, ε={hypermorphic_epsilon}", "QUANTUM")

    def forward(self, x):
        # Process through HyperMorphic attention mechanism
        attended = self.attention(x)

        # Apply zero-free constraint to attended output
        attended = self._ensure_zero_free(attended)

        # Process through parallel HyperMorphic streams
        fractal_out = self.fractal_stream(attended)
        quantum_out = self.quantum_stream(attended)

        # Create HD representations from the attended representation
        batch_size, seq_len, _ = attended.shape
        hd_out = self.hd_encoder(attended.view(-1, attended.size(-1))).view(batch_size, seq_len, -1)

        # Integrate all streams with zero-free constraint
        combined = torch.cat([fractal_out, quantum_out, hd_out], dim=-1)
        combined = self._ensure_zero_free(combined)

        integrated = self.integration(combined)
        integrated = self._ensure_zero_free(integrated)

        # Residual connection and normalization with HyperMorphic modulation
        residual = x * self.residual_modulation
        output = self.norm(residual + integrated)

        # Final zero-free guarantee
        output = self._ensure_zero_free(output)

        return output

    def _ensure_zero_free(self, tensor):
        """Ensure tensor values are zero-free by replacing zeros with epsilon"""
        # Create a mask for near-zero values
        zero_mask = torch.abs(tensor) < self.hyper_math.epsilon

        # Replace near-zero values with epsilon
        if zero_mask.any():
            tensor = tensor.clone()
            tensor[zero_mask] = tensor[zero_mask].sign() * self.hyper_math.epsilon
            # If sign is 0, use positive epsilon
            tensor[tensor == 0] = self.hyper_math.epsilon

        return tensor

class QuantumNexusModel(nn.Module):
    """
    Quantum Nexus Model: an advanced architecture that combines token embeddings,
    learned positional encoding, and a stack of NeocortexBlocks for binary prediction.

    Args:
        vocab_size (int): Number of tokens in the vocabulary.
        embed_dim (int): Dimensionality of the token embeddings.
        num_layers (int): Number of Neocortex blocks in the model.
        num_quantum_states (int): Number of quantum states used in each Neocortex block.
    """
    def __init__(self, vocab_size=30522, embed_dim=512, num_layers=8, num_quantum_states=4):
        super().__init__()
        self.embed_dim = embed_dim
        self.max_seq_len = 1024  # Maximum sequence length supported by the positional encoder

        # Token embedding layer
        self.embedding = nn.Embedding(vocab_size, embed_dim)

        # Learned positional encoding (max_seq_len x embed_dim)
        self.pos_encoder = nn.Parameter(torch.zeros(1, self.max_seq_len, embed_dim))
        nn.init.normal_(self.pos_encoder, mean=0, std=0.02)

        # Stack of Neocortex blocks (core processing layers)
        self.neocortex = nn.ModuleList([
            NeocortexBlock(embed_dim, num_quantum_states)
            for _ in range(num_layers)
        ])

        # Final output projection (binary prediction)
        self.output = nn.Linear(embed_dim, 2)

        # Dropout for regularization
        self.dropout = nn.Dropout(0.1)

        # Initialize weights
        self._init_weights()

    def _init_weights(self):
        """
        Initialize weights of the model using Kaiming initialization for most layers,
        and a smaller standard deviation for quantum-related parameters.
        """
        for name, p in self.named_parameters():
            if p.dim() >= 2:
                if 'quantum' in name:
                    nn.init.normal_(p, mean=0.0, std=0.01)
                else:
                    nn.init.kaiming_normal_(p, a=0.1, mode='fan_in', nonlinearity='leaky_relu')
            elif 'bias' in name:
                nn.init.zeros_(p)

    def forward(self, x, consciousness_level=0.8):
        """
        Forward pass of the model.

        Args:
            x (Tensor): Input token indices of shape (batch_size, seq_len).
            consciousness_level (float): Base level to weight the contribution of each Neocortex block.

        Returns:
            Tensor: Output logits of shape (batch_size, seq_len, 2).
        """
        # Convert input to long (if not already)
        x = x.long()
        seq_len = x.size(1)

        # Look up token embeddings and add positional encoding
        x = self.embedding(x) + self.pos_encoder[:, :seq_len, :]

        # Apply initial dropout
        x = self.dropout(x)

        # Process through the Neocortex blocks with consciousness-weighted residuals
        num_blocks = len(self.neocortex)
        for i, layer in enumerate(self.neocortex):
            # Compute a scaling factor for the layer's output based on its depth
            layer_consciousness = consciousness_level * (i + 1) / num_blocks
            # Compute the layer output
            layer_out = layer(x)
            # Add a residual connection weighted by the consciousness factor
            x = x + layer_consciousness * layer_out

        # Project the final representation to output logits
        output = self.output(x)
        return output

    def get_embedding(self, text_tokens):
        """
        Retrieve token embeddings for given input tokens.
        """
        with torch.no_grad():
            return self.embedding(text_tokens)

    def expand_architecture(self, num_quantum_states=4):
        """
        Dynamically expand the model by appending a new NeocortexBlock.

        Args:
            num_quantum_states (int): Number of quantum states for the new block.
        """
        new_block = NeocortexBlock(self.embed_dim, num_quantum_states)
        self.neocortex.append(new_block)
        log_event(f"Model architecture expanded with new NeocortexBlock. Total layers: {len(self.neocortex)}", "QUANTUM")

    def contract_architecture(self, min_layers=3):
        """
        Contract the model by removing the last NeocortexBlock, if above a minimum number.

        Args:
            min_layers (int): Minimum number of layers allowed.
        """
        if len(self.neocortex) > min_layers:
            self.neocortex = self.neocortex[:-1]
            log_event(f"Model architecture contracted. Total layers: {len(self.neocortex)}", "QUANTUM")
        else:
            log_event(f"Cannot contract further: minimum layer count reached ({min_layers})", "WARNING")

# =============================================================================
# AGENT CORE - ADDED HERE
# =============================================================================
class QuantumNexusAgent:
    """
    Main autonomous agent class, integrating all core modules and functionalities.
    """
    def __init__(self, model):
        self.model = model
        self.stats = defaultdict(int) # Initialize statistics
        self.action_log = deque(maxlen=100) # Keep track of recent actions

    def perceive(self):
        """
        Perceive the environment and gather relevant information.
        (Placeholder - you'll need to implement actual perception logic)
        """
        observation = {
            "time": datetime.now().isoformat(),
            "memory_size": len(getattr(self.free_will, 'memory_set', [])), # Safely get memory size
            "cycle_count": self.stats['cycles_run'],
            "last_action": self.action_log[-1] if self.action_log else "No actions yet.",
            "recent_actions": list(self.action_log)[-5:],
            "thinking_mode": getattr(getattr(self.ai_manager, 'autonomous_mind', None), 'current_mode', 'balanced'), # Safely get thinking mode
            "domain_stats": self.stats.get("domain_stats", {})
        }
        return observation

    def act(self, plan, optimizer=None):
        """
        Execute the planned action in the environment.
        """
        action_type = plan.get("action", "unknown")
        log_event(f"Executing action: {action_type}", "INFO")

        # SIMULATION: In a real implementation, these would be actual values from content processing
        # For now, add some synthetic values to get the strategy evaluation working
        content_length = random.randint(1000, 5000)  # Simulate different content lengths
        links_discovered = random.randint(3, 15)     # Simulate different links discovered

        action_details = {
            "action": action_type,
            "plan": plan,
            "start_time": datetime.now().isoformat(),
            "success": True,
            "content_length": content_length,      # Add meaningful value here
            "links_discovered": links_discovered   # Add meaningful value here
        }

        self.action_log.append(action_details)
        self.stats['cycles_run'] += 1

        return action_details  # Return details including metrics!


    def refine(self):
        """
        Refine and adapt internal parameters and strategies.
        (Placeholder - you'll need to implement actual refinement logic)
        """
        log_event("Agent refinement process initiated.", "INFO")
        # Placeholder refinement actions - replace with actual logic
        if random.random() < 0.3:
            if hasattr(self.ai_manager, "meta_learning"):
                metrics = {"loss": random.uniform(0.1, 0.6)} # Dummy loss
                self.ai_manager.meta_learning.track_performance(metrics)
                adapted_lr = self.adaptive_learning.adapt_learning_rate(metrics) # <---- Call on self.adaptive_learning
                log_event(f"Adaptive learning rate adjustment: {adapted_lr:.6f}", "INFO")
            else:
                log_event("Adaptive learning system not available for refinement.", "WARNING")
        else:
            log_event("No specific refinement needed this cycle.", "INFO")
        return True



# =============================================================================
# PLANNER SIFTER MODULE
# =============================================================================
class PlannerSifter:
    """
    Sophisticated planning system that selects optimal strategies based on context,
    learns from results, and generates structured exploration plans.
    """
    def __init__(self):
        self.strategies = {
            # Original strategies
            "exploration": {
                "name": "Broad Exploration",
                "description": "Discover new domains and content types",
                "actions": ["expand", "search"],
                "suitable_for": ["new_domains", "limited_knowledge"],
                "effectiveness": 0.5  # Starting effectiveness score
            },
            "deepening": {
                "name": "Knowledge Deepening",
                "description": "Focus on detailed understanding of specific areas",
                "actions": ["evaluate", "adapt"],
                "suitable_for": ["familiar_domains", "specialized_topics"],
                "effectiveness": 0.5
            },
            "connecting": {
                "name": "Knowledge Connection",
                "description": "Find relationships between different knowledge areas",
                "actions": ["reconnect", "evaluate"],
                "suitable_for": ["cross_domain", "synthesis"],
                "effectiveness": 0.5
            },
            "quantum": {
                "name": "Quantum Exploration",
                "description": "Non-deterministic approach with superposition",
                "actions": ["expand", "adapt", "reconnect"],
                "suitable_for": ["complex_problems", "creativity_needed"],
                "effectiveness": 0.5
            },
            "adaptive": {
                "name": "Adaptive Learning",
                "description": "Focus on improving learning process",
                "actions": ["adapt", "evaluate"],
                "suitable_for": ["performance_issues", "optimization_needed"],
                "effectiveness": 0.5
            },

            # Adding missing strategies from logs
            "quantum_reasoning": {
                "name": "Quantum Reasoning",
                "description": "Apply quantum principles to reasoning processes",
                "actions": ["search", "adapt", "evaluate"],
                "suitable_for": ["complex_problems", "scientific_domains"],
                "effectiveness": 0.5
            },
            "broad_exploration": {
                "name": "Broad Exploration",
                "description": "Wide-ranging discovery across many domains",
                "actions": ["search", "expand"],
                "suitable_for": ["new_domains", "discovery_phase"],
                "effectiveness": 0.5
            },
            "depth_first": {
                "name": "Depth First",
                "description": "Deep exploration of specific topics",
                "actions": ["expand", "evaluate"],
                "suitable_for": ["specialized_topics", "deep_analysis"],
                "effectiveness": 0.5
            },
            "connect_domains": {
                "name": "Connect Domains",
                "description": "Find connections between different knowledge domains",
                "actions": ["reconnect", "evaluate"],
                "suitable_for": ["cross_domain", "synthesis"],
                "effectiveness": 0.5
            },
            "evaluate_sources": {
                "name": "Evaluate Sources",
                "description": "Critical assessment of information sources",
                "actions": ["evaluate", "adapt"],
                "suitable_for": ["critical_thinking", "information_quality"],
                "effectiveness": 0.5
            },
            "creative_synthesis": {
                "name": "Creative Synthesis",
                "description": "Generate novel combinations of concepts",
                "actions": ["reconnect", "adapt"],
                "suitable_for": ["creativity_needed", "innovation"],
                "effectiveness": 0.5
            }
        }

        self.context_history = []
        self.strategy_usage = {name: 0 for name in self.strategies.keys()}
        self.strategy_results = {name: [] for name in self.strategies.keys()}

    def sift_strategies(self, context):
        """
        Select the most appropriate strategy based on current context
        """
        if not context:
            # Default to exploration if no context
            return {"strategy": "exploration", "reasoning": "No context available, using default strategy"}

        # Store context for learning
        self.context_history.append(context)
        if len(self.context_history) > 100:
            self.context_history = self.context_history[-100:]

        # Extract relevant features
        context_features = self._extract_context_features(context)

        # Score each strategy based on context
        strategy_scores = {}
        for name, strategy in self.strategies.items():
            # Base score
            score = 0.5

            # Context matching
            for feature in context_features:
                if feature in strategy["suitable_for"]:
                    score += 0.1

            # Effectiveness adjustment
            score *= strategy["effectiveness"]

            # Exploration factor to try underused strategies
            usage_ratio = self.strategy_usage[name] / max(1, sum(self.strategy_usage.values()))
            if usage_ratio < 0.1:  # Boost rarely used strategies
                score += 0.2

            # Record strategy score
            strategy_scores[name] = score

        # Find top strategy
        top_strategy = max(strategy_scores.items(), key=lambda x: x[1])

        # Update usage counter
        self.strategy_usage[top_strategy[0]] += 1

        return {
            "strategy": top_strategy[0],
            "reasoning": f"Selected {self.strategies[top_strategy[0]]['name']} (score: {top_strategy[1]:.2f}) based on context features: {', '.join(context_features)}"
        }

    def update_strategy_effectiveness(self, strategy_name, result_data):
        """
        Update effectiveness score for a strategy based on results
        """
        if strategy_name not in self.strategies:
            log_event(f"Warning: Strategy '{strategy_name}' not found for effectiveness update.", "WARNING")
            return False

        # Calculate success metrics - make sure these have real values
        content_length = result_data.get("content_length", 0)
        links_discovered = result_data.get("links_discovered", 0)

        # Log the raw values to debug
        log_event(f"Strategy metrics - Length: {content_length}, Links: {links_discovered}", "DEBUG")

        # Calculate success score
        success_score = min(1.0, (content_length / 5000) + (links_discovered / 10))

        # Update effectiveness with exponential moving average
        current = self.strategies[strategy_name].get("effectiveness", 0.5)
        updated = current * 0.8 + success_score * 0.2  # 80% old, 20% new

        # Ensure the new value is different enough to notice
        self.strategies[strategy_name]["effectiveness"] = updated

        log_event(f"Updated effectiveness of {strategy_name} strategy: {current:.2f} → {updated:.2f} (score: {success_score:.2f})", "INFO")
        return True

    def get_optimal_actions(self, strategy_name):
        """
        Get optimal actions for a strategy, with fallback for unknown strategies
        """
        if strategy_name not in self.strategies:
            log_event(f"Warning: Strategy '{strategy_name}' not found. Using default actions.", "WARNING")
            return ["expand", "search"]  # Default actions

        return self.strategies[strategy_name]["actions"]

    def _extract_context_features(self, context):
        """Extract features from context for strategy selection"""
        features = []

        # Domain familiarity
        if "domain_visits" in context:
            current_domain = context.get("current_domain", "")
            if current_domain in context["domain_visits"]:
                if context["domain_visits"][current_domain] > 5:
                    features.append("familiar_domains")
                else:
                    features.append("new_domains")

        # Check for limited knowledge
        if "domains_visited" in context and len(context.get("domains_visited", [])) < 10:
            features.append("limited_knowledge")

        # Check if current goal involves synthesis
        if "current_goal" in context:
            goal_desc = str(context["current_goal"]).lower()

            if "connect" in goal_desc or "integrat" in goal_desc or "synthe" in goal_desc:
                features.append("cross_domain")
                features.append("synthesis")

            if "deep" in goal_desc or "detail" in goal_desc:
                features.append("specialized_topics")

            if "optim" in goal_desc or "improve" in goal_desc:
                features.append("optimization_needed")

            if "creat" in goal_desc or "novel" in goal_desc or "new" in goal_desc:
                features.append("creativity_needed")

            # Add scientific domain feature
            if "scientific" in goal_desc or "science" in goal_desc or "research" in goal_desc:
                features.append("scientific_domains")

            # Add critical thinking feature
            if "evaluat" in goal_desc or "assess" in goal_desc or "critic" in goal_desc:
                features.append("critical_thinking")

            # Add discovery phase feature
            if "discover" in goal_desc or "explor" in goal_desc:
                features.append("discovery_phase")

        # Check recent performance
        if "recent_actions" in context:
            recent = context["recent_actions"]
            if any(a.get("content_length", 0) < 1000 for a in recent):
                features.append("performance_issues")

        # Add thinking mode as feature if available
        if "thinking_mode" in context:
            if context["thinking_mode"] == "quantum":
                features.append("quantum_thinking")
            elif context["thinking_mode"] == "creative":
                features.append("creativity_needed")
            elif context["thinking_mode"] == "analytical":
                features.append("specialized_topics")

        return features

    def generate_xoxo_plan(self, strategy_name, context):
        """Generates an XOXO plan for a given strategy and context."""
        if strategy_name not in self.strategies:
            strategy_name = "exploration"  # Default strategy if not found
            log_event(f"Warning: Strategy '{strategy_name}' not found. Using 'exploration' for XOXO plan.", "WARNING")

        strategy = self.strategies[strategy_name]
        actions = strategy["actions"]
        steps = []
        emojis = ["🔍", "🧠", "🔮", "🚀", "✨", "🔄", "📊", "🌟", "💎", "⚡"]

        # Strategy-specific steps
        if strategy_name == "exploration" or strategy_name == "broad_exploration":
            steps = [
                "Discover new domains with high information value",
                "Focus on breadth over depth in domain exploration",
                "Collect diverse content sources across the web",
                "Map the knowledge landscape to identify key areas",
                "Identify promising areas for deeper investigation"
            ]
        elif strategy_name == "deepening" or strategy_name == "depth_first":
            steps = [
                "Focus on specific knowledge domain",
                "Extract detailed information and nuanced insights",
                "Connect related concepts within domain",
                "Build hierarchical understanding of the domain",
                "Identify core principles and patterns"
            ]
        elif strategy_name == "connecting" or strategy_name == "connect_domains":
            steps = [
                "Identify similarities across domains",
                "Create cross-domain concept maps",
                "Look for shared principles",
                "Synthesize insights from different areas",
                "Build higher-level abstractions"
            ]
        elif strategy_name == "quantum" or strategy_name == "quantum_reasoning":
            steps = [
                "Maintain multiple hypotheses simultaneously",
                "Explore non-obvious connections",
                "Use probabilistic thinking",
                "Apply creative leaps in reasoning",
                "Allow for superposition of concepts"
            ]
        elif strategy_name == "adaptive" or strategy_name == "evaluate_sources":
            steps = [
                "Optimize learning parameters",
                "Refine content filtering approach",
                "Adjust exploration/exploitation balance",
                "Enhance memory organization",
                "Improve processing efficiency"
            ]
        elif strategy_name == "creative_synthesis":
            steps = [
                "Generate novel combinations of concepts",
                "Explore unexpected connections between domains",
                "Apply metaphorical thinking to problems",
                "Recombine existing elements in new ways",
                "Create emergent properties through synthesis"
            ]
        else:
            # Generic steps for other strategies
            steps = [
                "Analyze current knowledge state",
                "Identify optimal pathways for exploration",
                "Apply strategic thinking to resource allocation",
                "Evaluate information quality and relevance",
                "Integrate new knowledge into existing structure"
            ]

        plan_steps = []
        for i, step in enumerate(steps):
            emoji = emojis[i % len(emojis)]
            plan_steps.append(f"{emoji} **Step {i+1}:** {step}")

        action_text = ", ".join([f"*{action}*" for action in actions])
        emoji_sparkles = "✨"
        emoji_rocket = "🚀"

        # Create the XOXO plan
        xoxo_plan = f"**XOXO Plan: {strategy['name']} Strategy** {emoji_sparkles}\n\n"
        xoxo_plan += f"*{strategy['description']}*\n\n"
        xoxo_plan += "\n".join(plan_steps) + "\n\n"
        xoxo_plan += f"**Recommended Actions:** {action_text} {emoji_rocket}\n"
        return xoxo_plan





# =============================================================================
# CONTENT SIFTER MODULE
# =============================================================================
class ContentSifter:

    def __init__(self):
        self.quality_thresholds = {
            "min_content_length": 500,
            "min_text_density": 0.3,
            "max_ad_density": 0.2,
            "min_readability_score": 50
        }
        self.topics_of_interest = [
            "artificial intelligence", "machine learning", "quantum computing",
            "neural networks", "deep learning", "data science", "technology",
            "research", "science", "programming", "algorithms", "knowledge"
        ]
        self.content_fingerprints = {}  # Store fingerprints to avoid duplicates

    def evaluate_content_quality(self, content, url=None):
        """Rate content quality based on multiple metrics"""
        if not content:
            return {"score": 0, "reason": "Empty content"}

        # Basic metrics
        total_length = len(content)
        if total_length < self.quality_thresholds["min_content_length"]:
            return {"score": 0.1, "reason": "Content too short"}

        # Check text density
        text_density = self._calculate_text_density(content)
        if text_density < self.quality_thresholds["min_text_density"]:
            return {"score": 0.3, "reason": "Low text density"}

        # Check for potential ads
        ad_density = self._estimate_ad_density(content)
        if ad_density > self.quality_thresholds["max_ad_density"]:
            return {"score": 0.4, "reason": "High ad density"}

        # Calculate readability
        readability = self._calculate_readability(content)
        if readability < self.quality_thresholds["min_readability_score"]:
            return {"score": 0.5, "reason": "Low readability"}

        # Check relevance to topics of interest
        topic_relevance = self._calculate_topic_relevance(content)

        # Check for duplicate content
        if url:
            fingerprint = self._generate_content_fingerprint(content)
            if fingerprint in self.content_fingerprints.values():
                return {"score": 0.2, "reason": "Duplicate content"}

            # Store the fingerprint
            self.content_fingerprints[url] = fingerprint

        # Calculate final score
        base_score = 0.6
        final_score = min(0.95, base_score +
                         (0.1 if total_length > 2000 else 0) +
                         (0.2 * topic_relevance))

        return {
            "score": final_score,
            "metrics": {
                "length": total_length,
                "text_density": text_density,
                "ad_density": ad_density,
                "readability": readability,
                "topic_relevance": topic_relevance
            },
            "reason": "High quality content" if final_score > 0.8 else "Medium quality content"
        }

    def extract_key_information(self, content):
        """Extract important information from content"""
        if not content or len(content) < 500:
            return {"summary": "Content too short for extraction", "entities": []}

        # Extract potential entities
        entities = self._extract_entities(content)

        # Extract potential key sentences
        sentences = re.split(r'(?<=[.!?])\s+', content)

        # Score sentences
        scored_sentences = []
        for sentence in sentences:
            # Skip very short sentences
            if len(sentence) < 40:
                continue

            # Score based on keywords
            keyword_count = sum(1 for topic in self.topics_of_interest
                              if topic.lower() in sentence.lower())

            # Score based on position (earlier is better)
            position_score = 1.0 - (sentences.index(sentence) / max(1, len(sentences)))

            # Calculate final score
            score = (keyword_count * 0.6) + (position_score * 0.4)

            scored_sentences.append((sentence, score))

        # Sort and select top sentences
        top_sentences = sorted(scored_sentences, key=lambda x: x[1], reverse=True)[:5]

        return {
            "summary": " ".join([s[0] for s in top_sentences]),
            "entities": entities[:10]  # Top 10 entities
        }

    def _calculate_text_density(self, content):
        """Calculate the ratio of text to HTML/markup"""
        if not content:
            return 0

        # Remove HTML tags if present
        text_only = re.sub(r'<[^>]+>', ' ', content)
        text_only = re.sub(r'\s+', ' ', text_only).strip()

        return len(text_only) / max(1, len(content))

    def _estimate_ad_density(self, content):
        """Estimate the density of advertisements"""
        if not content:
            return 0

        # Look for common ad-related terms
        ad_terms = [
            "advertisement", "sponsor", "promoted", "buy now", "limited offer",
            "discount", "sale", "click here", "banner", "popup"
        ]

        ad_count = sum(content.lower().count(term) for term in ad_terms)

        # Count potential ad-related HTML elements
        ad_elements = len(re.findall(r'<div[^>]*(?:ad|banner|sponsor|promo)[^>]*>', content, re.I))

        # Normalize by content length
        return min(1.0, (ad_count + ad_elements * 2) / max(1, len(content) / 1000))

    def _calculate_readability(self, content):
        """Calculate a readability score"""
        if not content or len(content) < 100:
            return 0

        # Basic implementation of the Flesch Reading Ease formula

        # Clean text
        text = re.sub(r'<[^>]+>', ' ', content)  # Remove HTML
        text = re.sub(r'[^\w\s.]', '', text)     # Keep only words, spaces, periods
        text = re.sub(r'\s+', ' ', text).strip() # Normalize whitespace

        # Count sentences
        sentences = len(re.findall(r'[.!?]+', text))

        # Count words
        words = len(text.split())

        # Count syllables (very rough approximation)
        syllables = sum(self._count_syllables(word) for word in text.split())

        # Calculate readability (simplified Flesch formula)
        if words == 0 or sentences == 0:
            return 0

        words_per_sentence = words / max(1, sentences)
        syllables_per_word = syllables / max(1, words)

        return max(0, min(100, 206.835 - (1.015 * words_per_sentence) - (84.6 * syllables_per_word)))

    def _count_syllables(self, word):
        """Very basic syllable counter"""
        word = word.lower()
        if len(word) <= 3:
            return 1

        # Count vowel groups
        vowels = "aeiouy"
        count = 0
        prev_is_vowel = False

        for char in word:
            is_vowel = char in vowels
            if is_vowel and not prev_is_vowel:
                count += 1
            prev_is_vowel = is_vowel

        # Adjust for common patterns
        if word.endswith('e'):
            count -= 1
        if word.endswith('le') and len(word) > 2 and word[-3] not in vowels:
            count += 1
        if count == 0:
            count = 1

        return count

    def _calculate_topic_relevance(self, content):
        """Calculate relevance to topics of interest"""
        if not content:
            return 0

        content_lower = content.lower()
        matches = sum(content_lower.count(topic) for topic in self.topics_of_interest)

        # Normalize by content length
        normalized_matches = matches / max(1, len(content) / 1000)

        return min(1.0, normalized_matches / 5)  # Cap at 1.0

    def _extract_entities(self, content):
        """Extract potential named entities"""
        if not content:
            return []

        # Very simple entity extraction
        # In a real system, use NER models

        # Look for capitalized word sequences
        entities = re.findall(r'(?<![.?!])\s([A-Z][a-z]+(?:\s+[A-Z][a-z]+){1,5})', content)

        # Look for technical terms
        tech_terms = [
            "algorithm", "neural network", "machine learning", "deep learning",
            "artificial intelligence", "quantum", "data science", "transformer",
            "reinforcement learning", "natural language processing", "computer vision"
        ]

        for term in tech_terms:
            if term in content.lower():
                entities.append(term.title())

        return list(set(entities))  # Remove duplicates

    def _generate_content_fingerprint(self, content):
        """Generate a fingerprint to identify similar content"""
        if not content:
            return ""

        # Clean the content
        cleaned = re.sub(r'<[^>]+>', ' ', content)
        cleaned = re.sub(r'\s+', ' ', cleaned).strip().lower()

        # Get the most meaningful words
        words = cleaned.split()
        if len(words) > 100:
            # Use a sample of words from beginning, middle and end
            sample = words[:30] + words[len(words)//2-15:len(words)//2+15] + words[-30:]
            cleaned = " ".join(sample)

        # Create hash
        return hashlib.md5(cleaned.encode()).hexdigest()


# =============================================================================
# FREE WILL MODULE
# =============================================================================
class SuperQuantumFreeWill:
    """
    Advanced free will module with quantum-inspired decision making
    and adaptive exploration strategies.
    """
    def __init__(self, agent):
        self.agent = agent
        self.semantic_memory = {}
        self.domain_intelligence = DomainIntelligence()
        self.memory_set = set()
        self.consciousness_link = None

        # Decision dynamics
        self.exploration_weight = 0.6
        self.exploitation_weight = 0.4
        self.domain_diversity_weight = 0.3
        self.goal_relevance_weight = 0.5
        self.quantum_influence_weight = 0.4

        # Personality traits
        self.personality = {
            "curiosity": 0.9,
            "depth_preference": 0.7,
            "risk_taking": 0.65,
            "patience": 0.5,
            "creativity": 0.8
        }

        # Memory weighting
        self.memory_importance = {}

        # Get planner access
        self.temporal_planner = None
        if hasattr(agent, "ai_manager"):
            self.temporal_planner = getattr(agent.ai_manager, "temporal_planner", None)

        # Fallback URLs for when memory is empty
        self.fallback_urls = [
            "https://en.wikipedia.org/wiki/Special:Random",
            "https://news.ycombinator.com/",
            "https://github.com/explore",
            "https://arxiv.org/list/cs.AI/recent",
            "https://www.nature.com/",
            "https://www.reddit.com/r/science/"
        ]

        log_event("SuperQuantumFreeWill initialized", "QUANTUM")

    def link_consciousness(self, consciousness_module):
        """Connect to consciousness module for reflective capabilities"""
        self.consciousness_link = consciousness_module
        log_event("FreeWill linked with ConsciousnessModule", "INFO")

    def _get_active_goal_description(self):
        """Safely retrieve the active goal description"""
        try:
            if self.temporal_planner is not None and hasattr(self.temporal_planner, "select_active_goal"):
                goal = self.temporal_planner.select_active_goal()
                if goal and isinstance(goal, dict):
                    return goal.get("description", "")
            return ""
        except Exception as e:
            log_event(f"Error retrieving goal description: {e}", "ERROR")
            return ""

    def select_url(self):
        """
        Select the next URL to visit using quantum-inspired
        decision making with multiple factors
        """
        log_event("Selecting URL with quantum-inspired strategy", "QUANTUM")

        # Get candidate URLs from memory or use fallbacks
        candidate_urls = list(self.memory_set)
        if not candidate_urls:
            log_event("Memory set is empty. Using fallback URLs.", "WARNING")
            candidate_urls = self.fallback_urls

        # Get consciousness level if available
        awareness_level = 0.5
        if self.consciousness_link:
            awareness_level = self.consciousness_link.awareness_level

        # Get current goal
        current_goal_description = self._get_active_goal_description()

        # Calculate scores with quantum influence
        url_scores = {}

        for url in candidate_urls:
            # Parse domain
            domain = urlparse(url).netloc

            # Base score with quantum randomness
            # Higher consciousness reduces quantum randomness
            quantum_factor = self.quantum_influence_weight * (1 - awareness_level)

            # Quantum superposition of initial states
            quantum_states = []
            for _ in range(3):  # Generate 3 possible quantum states
                phase = random.uniform(0, 2 * math.pi)
                amplitude = random.uniform(0.3, 1.0)
                state_score = amplitude * math.cos(phase)
                quantum_states.append(state_score)

            # Collapse quantum states weighted by consciousness
            quantum_score = sum(quantum_states) / len(quantum_states)
            score = quantum_score * quantum_factor

            # Add domain diversity factor
            domain_visits = self.agent.stats.get("domain_stats", {}).get(domain, {}).get("visits", 0)
            domain_diversity_score = 1.0 / (1 + domain_visits)
            score += self.domain_diversity_weight * domain_diversity_score

            # Check goal relevance - exact matches and semantic similarity
            if current_goal_description:
                # Direct keyword matching
                if (current_goal_description.lower() in url.lower() or
                    current_goal_description.lower() in domain.lower()):
                    score += self.goal_relevance_weight * 0.7

                # Domain-specific boosts based on goal types
                if "explore" in current_goal_description.lower():
                    if domain not in self.agent.stats.get("domains_visited", set()):
                        score += self.goal_relevance_weight * 0.5
                elif "deep" in current_goal_description.lower():
                    if ".edu" in domain or "research" in domain:
                        score += self.goal_relevance_weight * 0.6

            # Apply personality factors
            if "blog" in url.lower() or "forum" in url.lower():
                score += self.personality["curiosity"] * 0.2
            if "research" in url.lower() or "paper" in url.lower() or "edu" in domain:
                score += self.personality["depth_preference"] * 0.3
            if random.random() < self.personality["risk_taking"]:
                score += random.uniform(0, 0.5)  # Occasional boost

            # Add memory importance if this URL is known
            if url in self.memory_importance:
                score += self.memory_importance[url] * 0.4

            # Store score
            url_scores[url] = score

        # Occasionally make quantum leap decision
        if random.random() < self.quantum_influence_weight * 0.3:
            # Complete quantum randomness - ignore calculated scores
            quantum_choice = random.choice(candidate_urls)
            log_event(f"Made quantum leap URL choice: {quantum_choice}", "QUANTUM")
            self.agent.stats["last_url"] = quantum_choice
            return quantum_choice

        # Normal selection based on scores
        if url_scores:
            try:
                best_url = max(url_scores.items(), key=lambda x: x[1])[0]
                log_event(f"Selected URL: {best_url} with score: {url_scores[best_url]:.2f}", "INFO")
                self.agent.stats["last_url"] = best_url
                return best_url
            except Exception as e:
                log_event(f"Error selecting best URL: {e}", "ERROR")
                fallback = random.choice(candidate_urls)
                log_event(f"Using fallback URL selection: {fallback}", "WARNING")
                self.agent.stats["last_url"] = fallback
                return fallback
        else:
            # No scores calculated - use random selection
            fallback = random.choice(candidate_urls)
            log_event(f"No URL scores available. Using random selection: {fallback}", "WARNING")
            self.agent.stats["last_url"] = fallback
            return fallback

    def discover_links(self, html_content, base_url):
        """Discover and extract links from HTML content"""
        links, details = enhanced_link_discovery(html_content, base_url)

        # Filter for high-quality links
        if details:
            high_quality_links = [link_data['url'] for link_data in details if link_data['quality_score'] > 0.6]
            if high_quality_links:
                log_event(f"Discovered {len(high_quality_links)} high-quality links (quality > 0.6)", "INFO")

                # Add to memory
                self.expand_memory(high_quality_links)

                return len(high_quality_links)

        # Add all discovered links to memory
        self.expand_memory(links)

        log_event(f"Discovered {len(links)} links (using basic quality filter)", "INFO")
        return len(links)

    def store_semantic_content(self, url, content):
        """Store content with semantic encoding for future reference"""
        # Create semantic memory module if needed
        semantic_module = SemanticMemoryModule()
        semantic_module.store_semantic_content(url, content)

        # Store in this instance's semantic memory
        self.semantic_memory[url] = semantic_module.semantic_memory.get(url, {})

    def decide(self):
        """
        Make a decision about what action to take next using
        quantum-inspired decision making
        """
        try:
            log_event("Making quantum-enhanced decision...", "QUANTUM")

            # Possible actions
            possible_actions = ["search", "expand", "adapt", "reconnect", "evaluate", "quantum_leap"]

            # Initialize quantum state - each action has amplitude and phase
            quantum_state = {}
            for action in possible_actions:
                phase = random.uniform(0, 2 * math.pi)
                amplitude = random.uniform(0.3, 1.0)
                quantum_state[action] = {"amplitude": amplitude, "phase": phase}

            # Get current context
            current_goal_description = self._get_active_goal_description()

            # Get consciousness awareness level
            awareness_level = 0.5
            if self.consciousness_link:
                awareness_level = self.consciousness_link.awareness_level

            # Get thinking mode if available
            thinking_mode = "balanced"
            if hasattr(self.agent, "ai_manager") and hasattr(self.agent.ai_manager, "autonomous_mind"):
                thinking_mode = getattr(self.agent.ai_manager.autonomous_mind, "current_mode", "balanced")

            # Apply quantum interference based on context

            # 1. Goal-based interference
            if current_goal_description:
                if "explore" in current_goal_description.lower():
                    # Amplify exploration actions
                    quantum_state["expand"]["amplitude"] *= 1.3
                    quantum_state["search"]["amplitude"] *= 1.2
                elif "deep" in current_goal_description.lower():
                    # Amplify deepening actions
                    quantum_state["evaluate"]["amplitude"] *= 1.4
                    quantum_state["adapt"]["amplitude"] *= 1.2

            # 2. Thinking mode interference
            if thinking_mode == "analytical":
                quantum_state["evaluate"]["amplitude"] *= 1.3
                quantum_state["search"]["amplitude"] *= 1.2
            elif thinking_mode == "creative":
                quantum_state["quantum_leap"]["amplitude"] *= 1.5
                quantum_state["expand"]["amplitude"] *= 1.2
            elif thinking_mode == "critical":
                quantum_state["adapt"]["amplitude"] *= 1.3
                quantum_state["evaluate"]["amplitude"] *= 1.2

            # 3. Recent actions interference - FIXED INDEXING
            recent_actions = [] # Initialize as empty list
            if hasattr(self.agent, 'action_log') and self.agent.action_log: # Check if action_log exists and is not None
                if len(self.agent.action_log) >= 3: # CHECK if action_log has at least 3 elements
                    recent_actions = [a.get("action", "") for a in list(self.agent.action_log)[-3:]] # Safe list conversion and slice

            if recent_actions:
                # Avoid repeating the same action too many times
                most_common = max(set(recent_actions), key=recent_actions.count) if recent_actions else None
                if most_common is not None: # CHECK if most_common is NOT None
                    if most_common in quantum_state: # CHECK if most_common is a valid key
                        quantum_state[most_common]["amplitude"] *= 0.7

            # 4. Success-based amplification - FIXED INDEXING
            successful_actions = [] # Initialize as empty list
            if hasattr(self.agent, 'action_log') and self.agent.action_log: # Check if action_log exists and is not None
                if len(self.agent.action_log) >= 5: # CHECK if action_log has at least 5 elements
                    successful_actions = [a.get("action", "") for a in list(self.agent.action_log)[-5:] # Safe list conversion and slice
                                         if a.get("content_length", 0) > 1000]

            if successful_actions:
                last_success = successful_actions[-1]  # Access last element safely now
                if last_success in quantum_state:  # Safely check dictionary key
                    quantum_state[last_success]["amplitude"] *= 1.2

            # Apply consciousness as quantum observer effect
            # Higher consciousness makes decision more deterministic
            for action in possible_actions:
                random_factor = random.uniform(0.8, 1.2) * (1 - awareness_level)
                quantum_state[action]["amplitude"] *= (1 + random_factor * 0.2)

            # Calculate probabilities (square of amplitudes)
            total_probability = sum(state["amplitude"]**2 for state in quantum_state.values())
            probabilities = {action: (state["amplitude"]**2) / total_probability
                           for action, state in quantum_state.items()}

            # Collapse the quantum state by observation
            action_type = random.choices(
                list(probabilities.keys()),
                weights=list(probabilities.values()),
                k=1
            )[0]

            # Create decision package
            decision = {
                "action": action_type,
                "quantum_confidence": probabilities[action_type],
                "reasoning": f"Quantum decision process ({thinking_mode} mode) - goal: {current_goal_description[:30]}",
                "timestamp": datetime.now().isoformat()
            }

            log_event(f"Decision: {action_type} with {probabilities[action_type]:.2f} quantum confidence", "QUANTUM")

            # Special handling for quantum_leap action
            if action_type == "quantum_leap":
                # Map to a standard action but with more randomness
                standard_actions = ["search", "expand", "adapt", "reconnect", "evaluate"]
                decision["action"] = random.choice(standard_actions)
                decision["quantum_leap"] = True
                log_event(f"Quantum leap mapped to {decision['action']}", "QUANTUM")

            return decision

        except Exception as e:
            log_event(f"Decision error: {e}", "ERROR")
            # Fallback to simple random choice
            fallback_action = random.choice(["expand", "search", "adapt"])
            return {"action": fallback_action, "error": str(e)[:200]}


# =============================================================================
# FREE WILL MODULE
# =============================================================================
class HyperMorphicSuperQuantumFreeWill:
    """
    Advanced free will module with HyperMorphic zero-free quantum-inspired decision making
    and adaptive exploration strategies. This class incorporates the principles from
    HyperMorphic Calculus to create a more robust decision framework.
    """
    def __init__(self, agent, epsilon=1e-12):
        self.agent = agent
        self.semantic_memory = {}
        self.domain_intelligence = DomainIntelligence()
        self.memory_set = set()
        self.consciousness_link = None

        # Initialize HyperMorphic Math utility
        self.hyper_math = HyperMorphicMath(
            dynamic_base=1000.0,  # Default dynamic base Φ
            dynamic_modulus=997,  # Default dynamic modulus Ψ
            epsilon=epsilon       # HyperMorphic nearness element ε_ᵩ
        )

        # Decision dynamics - now with HyperMorphic properties
        self.exploration_weight = self.hyper_math.zero_free(0.6)
        self.exploitation_weight = self.hyper_math.zero_free(0.4)
        self.domain_diversity_weight = self.hyper_math.zero_free(0.3)
        self.goal_relevance_weight = self.hyper_math.zero_free(0.5)
        self.quantum_influence_weight = self.hyper_math.zero_free(0.4)

        # Personality traits - zero-free by design
        self.personality = {
            "curiosity": self.hyper_math.zero_free(0.9),
            "depth_preference": self.hyper_math.zero_free(0.7),
            "risk_taking": self.hyper_math.zero_free(0.65),
            "patience": self.hyper_math.zero_free(0.5),
            "creativity": self.hyper_math.zero_free(0.8)
        }

        # Memory weighting
        self.memory_importance = {}

        # Get planner access
        self.temporal_planner = None
        if hasattr(agent, "ai_manager"):
            self.temporal_planner = getattr(agent.ai_manager, "temporal_planner", None)

        # Fallback URLs for when memory is empty
        self.fallback_urls = [
            "https://en.wikipedia.org/wiki/Special:Random",
            "https://news.ycombinator.com/",
            "https://github.com/explore",
            "https://arxiv.org/list/cs.AI/recent",
            "https://www.nature.com/",
            "https://www.reddit.com/r/science/"
        ]

        # HyperMorphic state variables
        self.quantum_state = {state: {"amplitude": self.hyper_math.zero_free(0.5),
                                      "phase": random.uniform(0, 2 * math.pi)}
                             for state in ["search", "expand", "adapt", "reconnect", "evaluate", "quantum_leap"]}

        # HyperMorphic decision history
        self.decision_history = []

        # HyperMorphic entropic control parameter
        self.entropy_parameter = self.hyper_math.zero_free(0.7)

        log_event("HyperMorphicSuperQuantumFreeWill initialized with zero-free constraints", "QUANTUM")

    def link_consciousness(self, consciousness_module):
        """Connect to consciousness module for reflective capabilities"""
        self.consciousness_link = consciousness_module
        log_event("HyperMorphicFreeWill linked with ConsciousnessModule", "INFO")

    def _get_active_goal_description(self):
        """Safely retrieve the active goal description with HyperMorphic resilience"""
        try:
            if self.temporal_planner is not None and hasattr(self.temporal_planner, "select_active_goal"):
                goal = self.temporal_planner.select_active_goal()
                if goal and isinstance(goal, dict):
                    return goal.get("description", "")
            return ""
        except Exception as e:
            log_event(f"Error retrieving goal description: {e}", "ERROR")
            return ""

    def select_url(self):
        """
        Select the next URL to visit using HyperMorphic zero-free quantum-inspired
        decision making with multiple factors
        """
        log_event("Selecting URL with HyperMorphic quantum-inspired strategy", "QUANTUM")

        # Get candidate URLs from memory or use fallbacks
        candidate_urls = list(self.memory_set)
        if not candidate_urls:
            log_event("Memory set is empty. Using fallback URLs.", "WARNING")
            candidate_urls = self.fallback_urls

        # Get consciousness level if available, ensuring zero-free
        awareness_level = self.hyper_math.zero_free(0.5)  # Default value
        if self.consciousness_link:
            awareness_level = self.hyper_math.zero_free(self.consciousness_link.awareness_level)

        # Get current goal
        current_goal_description = self._get_active_goal_description()

        # Calculate scores with HyperMorphic quantum influence
        url_scores = {}

        for url in candidate_urls:
            # Parse domain
            domain = urlparse(url).netloc

            # Base score with HyperMorphic quantum randomness
            # Higher consciousness reduces quantum randomness
            quantum_factor = self.hyper_math.mul(self.quantum_influence_weight,
                                                self.hyper_math.sub(1.0, awareness_level))

            # Generate HyperMorphic quantum states
            quantum_states = []
            for _ in range(3):  # Generate 3 possible quantum states
                phase = random.uniform(0, 2 * math.pi)
                amplitude = self.hyper_math.zero_free(random.uniform(0.3, 1.0))
                # Apply HyperMorphic cosine function with zero-free guarantee
                state_score = self.hyper_math.mul(amplitude, math.cos(phase))
                quantum_states.append(state_score)

            # Collapse quantum states weighted by consciousness level
            # Using HyperMorphic operations for the collapse
            quantum_scores_sum = self.hyper_math.zero_free(sum(quantum_states))
            quantum_score = self.hyper_math.div(quantum_scores_sum, self.hyper_math.zero_free(len(quantum_states)))
            score = self.hyper_math.mul(quantum_score, quantum_factor)

            # Add domain diversity factor using HyperMorphic division
            domain_visits = self.agent.stats.get("domain_stats", {}).get(domain, {}).get("visits", 0)
            # Convert to HyperMorphic zero-free division
            diversity_denominator = self.hyper_math.add(1.0, self.hyper_math.zero_free(domain_visits))
            domain_diversity_score = self.hyper_math.div(1.0, diversity_denominator)

            # Apply HyperMorphic multiplication and addition
            diversity_component = self.hyper_math.mul(self.domain_diversity_weight, domain_diversity_score)
            score = self.hyper_math.add(score, diversity_component)

            # Check goal relevance with HyperMorphic operations
            if current_goal_description:
                # Direct keyword matching
                if (current_goal_description.lower() in url.lower() or
                    current_goal_description.lower() in domain.lower()):
                    relevance_boost = self.hyper_math.mul(self.goal_relevance_weight, 0.7)
                    score = self.hyper_math.add(score, relevance_boost)

                # Domain-specific boosts based on goal types with HyperMorphic operations
                if "explore" in current_goal_description.lower():
                    if domain not in self.agent.stats.get("domains_visited", set()):
                        exploration_boost = self.hyper_math.mul(self.goal_relevance_weight, 0.5)
                        score = self.hyper_math.add(score, exploration_boost)
                elif "deep" in current_goal_description.lower():
                    if ".edu" in domain or "research" in domain:
                        depth_boost = self.hyper_math.mul(self.goal_relevance_weight, 0.6)
                        score = self.hyper_math.add(score, depth_boost)

            # Apply HyperMorphic personality factors
            if "blog" in url.lower() or "forum" in url.lower():
                curiosity_boost = self.hyper_math.mul(self.personality["curiosity"], 0.2)
                score = self.hyper_math.add(score, curiosity_boost)

            if "research" in url.lower() or "paper" in url.lower() or "edu" in domain:
                depth_boost = self.hyper_math.mul(self.personality["depth_preference"], 0.3)
                score = self.hyper_math.add(score, depth_boost)

            if random.random() < self.hyper_math.zero_free(self.personality["risk_taking"]):
                risk_boost = self.hyper_math.zero_free(random.uniform(0, 0.5))
                score = self.hyper_math.add(score, risk_boost)

            # Add memory importance if this URL is known
            if url in self.memory_importance:
                memory_boost = self.hyper_math.mul(self.memory_importance[url], 0.4)
                score = self.hyper_math.add(score, memory_boost)

            # Store score with HyperMorphic guarantee
            url_scores[url] = self.hyper_math.zero_free(score)

        # Quantum leap decision with HyperMorphic probability
        quantum_leap_threshold = self.hyper_math.mul(self.quantum_influence_weight, 0.3)
        if self.hyper_math.zero_free(random.random()) < quantum_leap_threshold:
            # Complete quantum randomness - ignore calculated scores
            quantum_choice = random.choice(candidate_urls)
            log_event(f"Made HyperMorphic quantum leap URL choice: {quantum_choice}", "QUANTUM")
            self.agent.stats["last_url"] = quantum_choice
            return quantum_choice

        # Normal selection based on scores using HyperMorphic comparison
        if url_scores:
            try:
                # Find highest score using HyperMorphic comparison
                best_url = max(url_scores.items(), key=lambda x: x[1])[0]
                log_event(f"Selected URL: {best_url} with HyperMorphic score: {url_scores[best_url]:.2f}", "INFO")
                self.agent.stats["last_url"] = best_url
                return best_url
            except Exception as e:
                log_event(f"Error selecting best URL: {e}", "ERROR")
                fallback = random.choice(candidate_urls)
                log_event(f"Using fallback URL selection: {fallback}", "WARNING")
                self.agent.stats["last_url"] = fallback
                return fallback
        else:
            # No scores calculated - use random selection
            fallback = random.choice(candidate_urls)
            log_event(f"No URL scores available. Using random selection: {fallback}", "WARNING")
            self.agent.stats["last_url"] = fallback
            return fallback

    def discover_links(self, html_content, base_url):
        """Discover and extract links from HTML content with HyperMorphic filtering"""
        links, details = enhanced_link_discovery(html_content, base_url)

        # Filter for high-quality links with HyperMorphic threshold
        if details:
            threshold = self.hyper_math.zero_free(0.6)  # Zero-free quality threshold
            high_quality_links = [link_data['url'] for link_data in details
                               if self.hyper_math.zero_free(link_data['quality_score']) > threshold]

            if high_quality_links:
                log_event(f"Discovered {len(high_quality_links)} high-quality links (quality > {threshold})", "INFO")

                # Add to memory with HyperMorphic state tracking
                self.expand_memory(high_quality_links)

                return len(high_quality_links)

        # Add all discovered links to memory
        self.expand_memory(links)

        log_event(f"Discovered {len(links)} links (using basic quality filter)", "INFO")
        return len(links)

    def store_semantic_content(self, url, content):
        """Store content with semantic encoding using HyperMorphic representational spaces"""
        # Create semantic memory module if needed
        semantic_module = SemanticMemoryModule()
        semantic_module.store_semantic_content(url, content)

        # Store in this instance's semantic memory with HyperMorphic guarantees
        self.semantic_memory[url] = self.hyper_math.zero_free(
            semantic_module.semantic_memory.get(url, {})
        )

    def expand_memory(self, urls):
        """Expand memory set with HyperMorphic tracking"""
        original_size = len(self.memory_set)

        # Add new URLs to memory set
        for url in urls:
            self.memory_set.add(url)

            # Initialize memory importance if not present
            if url not in self.memory_importance:
                # Set initial importance with slight randomness and zero-free guarantee
                initial_importance = self.hyper_math.zero_free(0.5 + random.uniform(-0.1, 0.1))
                self.memory_importance[url] = initial_importance

        # Calculate new memory size with HyperMorphic operations
        new_size = len(self.memory_set)
        added = new_size - original_size

        if added > 0:
            log_event(f"Memory expanded by {added} URLs, total size: {new_size}", "INFO")

        # Apply HyperMorphic entropy optimization if memory exceeds threshold
        if new_size > MEMORY_MAX_SIZE * 0.9:
            self._optimize_memory_entropy()

    def _optimize_memory_entropy(self):
        """Optimize memory entropy using HyperMorphic principles"""
        # Target size is 80% of max to leave room for new memories
        target_size = int(MEMORY_MAX_SIZE * 0.8)

        if len(self.memory_set) <= target_size:
            return

        log_event(f"Performing HyperMorphic memory entropy optimization", "INFO")

        # Sort URLs by importance with HyperMorphic comparisons
        sorted_urls = sorted([(url, self.memory_importance.get(url, self.hyper_math.zero_free(0.5)))
                           for url in self.memory_set],
                           key=lambda x: x[1])

        # Remove lowest importance URLs
        urls_to_remove = sorted_urls[:len(self.memory_set) - target_size]

        for url, importance in urls_to_remove:
            self.memory_set.remove(url)
            # Keep importance record for potential future reference

        log_event(f"Removed {len(urls_to_remove)} low-importance URLs from memory", "INFO")

    def decide(self):
        """
        Make a decision about what action to take next using
        HyperMorphic zero-free quantum-inspired decision making
        """
        try:
            log_event("Making HyperMorphic quantum-enhanced decision...", "QUANTUM")

            # Possible actions
            possible_actions = ["search", "expand", "adapt", "reconnect", "evaluate", "quantum_leap"]

            # Initialize HyperMorphic quantum state if not initialized
            # Each action has amplitude and phase with zero-free guarantees
            if not self.quantum_state or len(self.quantum_state) != len(possible_actions):
                self.quantum_state = {}
                for action in possible_actions:
                    phase = random.uniform(0, 2 * math.pi)
                    amplitude = self.hyper_math.zero_free(random.uniform(0.3, 1.0))
                    self.quantum_state[action] = {"amplitude": amplitude, "phase": phase}

            # Update quantum states based on history using HyperMorphic evolution
            self._evolve_quantum_states()

            # Get current context
            current_goal_description = self._get_active_goal_description()

            # Get consciousness awareness level with zero-free guarantee
            awareness_level = self.hyper_math.zero_free(0.5)  # Default
            if self.consciousness_link:
                awareness_level = self.hyper_math.zero_free(self.consciousness_link.awareness_level)

            # Get thinking mode if available
            thinking_mode = "balanced"
            if hasattr(self.agent, "ai_manager") and hasattr(self.agent.ai_manager, "autonomous_mind"):
                thinking_mode = getattr(self.agent.ai_manager.autonomous_mind, "current_mode", "balanced")

            # Apply HyperMorphic quantum interference based on context

            # 1. Goal-based interference with zero-free amplitude modulation
            if current_goal_description:
                if "explore" in current_goal_description.lower():
                    # Amplify exploration actions
                    self._amplify_action("expand", self.hyper_math.zero_free(1.3))
                    self._amplify_action("search", self.hyper_math.zero_free(1.2))
                elif "deep" in current_goal_description.lower():
                    # Amplify deepening actions
                    self._amplify_action("evaluate", self.hyper_math.zero_free(1.4))
                    self._amplify_action("adapt", self.hyper_math.zero_free(1.2))

            # 2. Thinking mode interference
            if thinking_mode == "analytical":
                self._amplify_action("evaluate", self.hyper_math.zero_free(1.3))
                self._amplify_action("search", self.hyper_math.zero_free(1.2))
            elif thinking_mode == "creative":
                self._amplify_action("quantum_leap", self.hyper_math.zero_free(1.5))
                self._amplify_action("expand", self.hyper_math.zero_free(1.2))
            elif thinking_mode == "critical":
                self._amplify_action("adapt", self.hyper_math.zero_free(1.3))
                self._amplify_action("evaluate", self.hyper_math.zero_free(1.2))

            # 3. Recent actions interference - safely extract from action log
            recent_actions = []
            if hasattr(self.agent, 'action_log') and self.agent.action_log:
                if len(self.agent.action_log) >= 3:
                    recent_actions = [a.get("action", "") for a in list(self.agent.action_log)[-3:]]

            if recent_actions:
                # Avoid repeating the same action too many times using HyperMorphic anti-resonance
                action_counts = {}
                for a in recent_actions:
                    action_counts[a] = action_counts.get(a, 0) + 1

                most_common = max(action_counts.items(), key=lambda x: x[1])[0] if action_counts else None

                if most_common in self.quantum_state:
                    # Reduce amplitude of most common action to encourage diversity
                    self._amplify_action(most_common, self.hyper_math.zero_free(0.7))

            # 4. Success-based amplification
            successful_actions = []
            if hasattr(self.agent, 'action_log') and self.agent.action_log:
                if len(self.agent.action_log) >= 5:
                    successful_actions = [a.get("action", "") for a in list(self.agent.action_log)[-5:]
                                         if a.get("content_length", 0) > 1000]

            if successful_actions:
                for action in successful_actions:
                    if action in self.quantum_state:
                        # Amplify successful actions
                        self._amplify_action(action, self.hyper_math.zero_free(1.1))

            # Apply consciousness as quantum observer effect with HyperMorphic operations
            # Higher consciousness makes decision more deterministic
            for action in possible_actions:
                if action in self.quantum_state:
                    # Calculate randomness factor with zero-free operations
                    random_factor = self.hyper_math.mul(
                        self.hyper_math.zero_free(random.uniform(0.8, 1.2)),
                        self.hyper_math.sub(1.0, awareness_level)
                    )

                    # Apply small random fluctuation scaled by randomness factor
                    fluctuation = self.hyper_math.mul(random_factor, self.hyper_math.zero_free(0.2))

                    # Apply to amplitude
                    current_amplitude = self.quantum_state[action]["amplitude"]
                    self.quantum_state[action]["amplitude"] = self.hyper_math.mul(
                        current_amplitude,
                        self.hyper_math.add(1.0, fluctuation)
                    )

            # Calculate probabilities with HyperMorphic operations
            # Use amplitude squared for quantum probability
            probabilities = {}
            for action, state in self.quantum_state.items():
                # Calculate amplitude squared with zero-free guarantee
                amplitude = state["amplitude"]
                probability = self.hyper_math.mul(amplitude, amplitude)
                probabilities[action] = probability

            # Normalize probabilities with HyperMorphic division
            total_probability = sum(probabilities.values())
            normalized_probabilities = {}
            for action, probability in probabilities.items():
                normalized_probabilities[action] = self.hyper_math.div(probability, total_probability)

            # Collapse the quantum state by observation
            # This uses the normalized probabilities to select an action
            actions = list(normalized_probabilities.keys())
            weights = list(normalized_probabilities.values())
            action_type = random.choices(actions, weights=weights, k=1)[0]

            # Record the decision with HyperMorphic metadata
            confidence = normalized_probabilities[action_type]
            decision = {
                "action": action_type,
                "quantum_confidence": float(confidence),  # Convert to float for serialization
                "reasoning": f"HyperMorphic quantum decision process ({thinking_mode} mode) - goal: {current_goal_description[:30]}",
                "timestamp": datetime.now().isoformat()
            }

            # Store in decision history
            self.decision_history.append(decision)

            # Trim history if needed
            if len(self.decision_history) > 50:
                self.decision_history = self.decision_history[-50:]

            log_event(f"HyperMorphic decision: {action_type} with {confidence:.2f} quantum confidence", "QUANTUM")

            # Special handling for quantum_leap action
            if action_type == "quantum_leap":
                # Map to a standard action but with more randomness
                standard_actions = ["search", "expand", "adapt", "reconnect", "evaluate"]
                decision["action"] = random.choice(standard_actions)
                decision["quantum_leap"] = True
                log_event(f"HyperMorphic quantum leap mapped to {decision['action']}", "QUANTUM")

            return decision

        except Exception as e:
            log_event(f"HyperMorphic decision error: {e}", "ERROR")
            # Fallback to simple random choice
            fallback_action = random.choice(["expand", "search", "adapt"])
            return {"action": fallback_action, "error": str(e)[:200]}

    def _evolve_quantum_states(self):
        """Evolve quantum states based on history using HyperMorphic principles"""
        # Small phase evolution for all states
        for action, state in self.quantum_state.items():
            # Evolve phase with small random shift
            current_phase = state["phase"]
            phase_shift = random.uniform(-0.1, 0.1)
            new_phase = (current_phase + phase_shift) % (2 * math.pi)

            # Update phase
            self.quantum_state[action]["phase"] = new_phase

    def _amplify_action(self, action, factor):
        """Amplify action amplitude by factor using HyperMorphic multiplication"""
        if action in self.quantum_state:
            current_amplitude = self.quantum_state[action]["amplitude"]
            new_amplitude = self.hyper_math.mul(current_amplitude, factor)

            # Ensure amplitude stays within reasonable bounds
            max_amplitude = 2.0  # Maximum allowed amplitude
            if new_amplitude > max_amplitude:
                new_amplitude = max_amplitude

            # Update amplitude with zero-free guarantee
            self.quantum_state[action]["amplitude"] = self.hyper_math.zero_free(new_amplitude)

    def get_decision_analytics(self):
        """Get analytics on recent decisions using HyperMorphic statistics"""
        if not self.decision_history:
            return {"status": "insufficient_data"}

        # Analyze action distribution
        action_counts = {}
        for decision in self.decision_history:
            action = decision.get("action", "unknown")
            action_counts[action] = action_counts.get(action, 0) + 1

        # Calculate action probabilities with HyperMorphic division
        total_decisions = len(self.decision_history)
        action_probabilities = {}
        for action, count in action_counts.items():
            action_probabilities[action] = self.hyper_math.div(count, total_decisions)

        # Calculate entropy using HyperMorphic operations
        entropy = self.hyper_math.zero_free(0)
        for prob in action_probabilities.values():
            # Skip zero probabilities (though we should never have exact zeros)
            if prob <= self.hyper_math.epsilon:
                continue

            # Add -p*log(p) to entropy
            log_prob = math.log(prob)
            term = self.hyper_math.mul(prob, self.hyper_math.zero_free(-log_prob))
            entropy = self.hyper_math.add(entropy, term)

        # Max possible entropy given action count
        action_count = len(action_counts)
        max_entropy = self.hyper_math.zero_free(math.log(max(1, action_count)))

        # Normalized entropy (0-1 scale)
        normalized_entropy = self.hyper_math.zero_free(0.5)  # Default
        if max_entropy > self.hyper_math.epsilon:
            normalized_entropy = self.hyper_math.div(entropy, max_entropy)

        return {
            "action_distribution": {k: float(v) for k, v in action_probabilities.items()},
            "decision_entropy": float(normalized_entropy),
            "quantum_confidence_avg": sum(d.get("quantum_confidence", 0.5) for d in self.decision_history) / total_decisions,
            "quantum_leap_frequency": sum(1 for d in self.decision_history if d.get("quantum_leap", False)) / total_decisions,
            "decision_count": total_decisions
        }


# =============================================================================
# AI MANAGEMENT SYSTEM
# =============================================================================
# =============================================================================
# AI MANAGEMENT SYSTEM
# =============================================================================
class AIManager:
    """
    Central management system for the autonomous agent, coordinating
    decision-making, planning, and evolution.
    """
    def __init__(self, agent, model):
        self.agent = agent
        self.model = model

        # Core subsystems
        self.temporal_planner = TemporalPlanner()
        self.autonomous_mind = AutonomousMind(agent, model)
        self.consciousness = ConsciousnessModule(agent)
        self.imagination = ImaginationEngine()

        # Self-improvement systems
        self.meta_learning = MetaLearningModule(model)
        self.evolution_engine = HyperMorphicMetaEvolutionEngine()


        # Operational tracking
        self.cycle_counter = 0
        self.last_evolution_attempt = 0
        self.evolution_interval = 50
        self.error_recovery_attempts = 0

        # Initialize subsystems
        self.temporal_planner.initialize_goals()

        # Connect subsystems
        if hasattr(self.agent, "free_will") and hasattr(self.agent.free_will, "link_consciousness"):
            self.agent.free_will.link_consciousness(self.consciousness)

        log_event("AIManager initialized with all autonomous systems", "INFO")


    async def run_cycle(self, optimizer=None):
        """Run a complete autonomous cycle with enhanced error handling"""
        self.cycle_counter += 1
        log_event(f"=== Enhanced Autonomous Cycle {self.cycle_counter} ===", "INFO")

        try:
            # 1. Perception - get environment state
            try:
                observation = self.agent.perceive()
            except Exception as e:
                log_event(f"Error in perception phase: {str(e)}", "ERROR")
                observation = {"error": str(e)}  # Minimal fallback observation

            # 2. Consciousness reflection with error handling
            try:
                if hasattr(self, 'consciousness'):
                    self.consciousness.reflect(observation)
            except Exception as e:
                log_event(f"Error in consciousness reflection: {str(e)}", "ERROR")
                # Continue even if reflection fails

            # 3. Decision making with fallbacks
            base_decision = {"action": "expand"}  # Default fallback
            try:
                if hasattr(self.agent, "free_will") and hasattr(self.agent.free_will, "decide"):
                    decision = self.agent.free_will.decide()
                    if isinstance(decision, dict) and "action" in decision:
                        base_decision = decision
            except Exception as e:
                log_event(f"Decision error: {str(e)}. Using fallback decision.", "ERROR")

            # 4. Planning with error handling
            try:
                if hasattr(self, 'temporal_planner') and self.temporal_planner:
                    full_plan = self.temporal_planner.plan_action(
                        base_decision.get("action", "expand"),
                        observation
                    )
                else:
                    # Fallback simple plan if no temporal planner
                    full_plan = {
                        "action": base_decision.get("action", "expand"),
                        "strategy": "fallback_strategy",
                        "goal": "Continue system operation"
                    }
            except Exception as e:
                log_event(f"Planning error: {str(e)}. Using simplified plan.", "ERROR")
                # Create minimal plan on failure
                full_plan = {
                    "action": base_decision.get("action", "expand"),
                    "strategy": "emergency_strategy",
                    "goal": "Recover from planning failure"
                }

            # 5. Imagination - simulate outcomes (non-critical)
            try:
                if hasattr(self, 'imagination') and random.random() < 0.2:
                    self.imagination.simulate_creation()
            except Exception as e:
                log_event(f"Non-critical error in imagination: {str(e)}", "WARNING")

            # 6. Execute action with timeout protection
            try:
                # Set a timeout for execution to prevent hanging
                action_task = asyncio.create_task(
                    asyncio.to_thread(self.agent.act, full_plan, optimizer)
                )
                action_successful = await asyncio.wait_for(action_task, timeout=60.0)
            except asyncio.TimeoutError:
                log_event("Action execution timed out after 60 seconds", "ERROR")
                action_successful = False
            except Exception as e:
                log_event(f"Action execution error: {str(e)}", "ERROR")
                action_successful = False

            # After action execution is successful:
            if action_successful and hasattr(self.agent, 'planner_sifter'):
                strategy_name = full_plan.get("strategy", "exploration")
                result_data = {
                    "content_length": self.agent.action_log[-1].get("content_length", 0),
                    "links_discovered": self.agent.action_log[-1].get("links_discovered", 0),
                    "success": action_successful
                }
                self.agent.planner_sifter.update_strategy_effectiveness(strategy_name, result_data)

            # 7. Performance assessment
            performance_metrics = {
                "success": action_successful,
                "content_length": self.agent.action_log[-1].get("content_length", 0) if self.agent.action_log else 0,
                "links_discovered": self.agent.action_log[-1].get("links_discovered", 0) if self.agent.action_log else 0,
                "cycle": self.cycle_counter
            }

            # 8. Reflection and adaptation (non-critical)
            try:
                if hasattr(self, 'temporal_planner'):
                    self.temporal_planner.reflect_and_adapt(performance_metrics)
            except Exception as e:
                log_event(f"Non-critical error in reflection: {str(e)}", "WARNING")

            # 9. Error detection (non-critical)
            try:
                if hasattr(self, 'imagination'):
                    error_details = self.imagination.simulate_error_detection()
                    if error_details:
                        # Apply correction
                        self.imagination.simulate_error_correction(error_details)
            except Exception as e:
                log_event(f"Non-critical error in error detection: {str(e)}", "WARNING")

            # 10. Self-evolution at intervals (non-critical)
            try:
                if (hasattr(self, 'evolution_engine') and
                    self.cycle_counter - getattr(self, 'last_evolution_attempt', 0) >= getattr(self, 'evolution_interval', 50)):
                    log_event("Attempting system evolution...", "INFO")
                    result, message = self.evolution_engine.evolve_system(self.agent)
                    self.last_evolution_attempt = self.cycle_counter
                    log_event(f"Evolution attempt result: {result} - {message}", "INFO")
            except Exception as e:
                log_event(f"Non-critical error in evolution: {str(e)}", "WARNING")

            # 11. Self-refinement (with error handling)
            try:
                self.agent.refine()
            except Exception as e:
                log_event(f"Error in agent refinement: {str(e)}", "ERROR")
                # Don't let refinement errors abort the cycle

            # Reset error recovery counter on success
            self.error_recovery_attempts = 0

            return {
                "cycle": self.cycle_counter,
                "action": full_plan.get("action", "unknown"),
                "strategy": full_plan.get("strategy", "none"),
                "success": action_successful
            }

        except Exception as e:
            # Error recovery
            self.error_recovery_attempts += 1
            log_event(f"CYCLE ERROR: {str(e)}", "ERROR")
            log_event(traceback.format_exc(), "ERROR")

            # Implement progressive recovery strategies
            if self.error_recovery_attempts < 3:
                log_event("Attempting standard error recovery", "WARNING")
            elif self.error_recovery_attempts < 5:
                log_event("Attempting advanced error recovery - resetting system state", "WARNING")
                # Reset consciousness state if available
                if hasattr(self, 'consciousness'):
                    self.consciousness.awareness_level = 0.5
                    self.consciousness.current_state = "balanced"
            else:
                log_event("Critical error threshold reached - emergency recovery", "CRITICAL")
                # Emergency reset of all systems
                if hasattr(self, 'consciousness'):
                    self.consciousness = ConsciousnessModule(self.agent)
                if hasattr(self, 'temporal_planner'):
                    self.temporal_planner.cycle_count = 0
                    self.temporal_planner.refresh_short_term_goals()

            return {
                "status": "error",
                "cycle": self.cycle_counter,
                "error": str(e),
                "recovery_attempt": self.error_recovery_attempts
            }



# =============================================================================
# TEMPORAL PLANNING AND GOAL MANAGEMENT
# =============================================================================
class TemporalPlanner:
    """
    Advanced planning system that operates across multiple time horizons
    and manages goals with temporal dependencies.
    """
    def __init__(self):
        self.short_term_goals = []
        self.long_term_goals = []
        self.goal_history = []
        self.current_strategy = None
        self.strategy_effectiveness = {}
        self.time_horizon_days = 7
        self.reflection_interval = 20
        self.cycle_count = 0

        # Add strategy mapping to match PlannerSifter strategy names
        self.strategy_mapping = {
            "exploration": "broad_exploration",
            "deepening": "depth_first",
            "integration": "connect_domains",
            "evaluation": "evaluate_sources",
            "quantum": "quantum_reasoning",
            "creative": "creative_synthesis"
        }

    def initialize_goals(self):
        """Set up initial goal structure"""
        self.long_term_goals = [
            {
                "id": "knowledge_diversity",
                "description": "Maximize diversity of knowledge domains",
                "priority": 0.8,
                "progress": 0.0,
                "created": datetime.now().isoformat()
            },
            {
                "id": "model_efficiency",
                "description": "Optimize neural architecture for learning efficiency",
                "priority": 0.7,
                "progress": 0.0,
                "created": datetime.now().isoformat()
            },
            {
                "id": "content_quality",
                "description": "Improve filtering and processing of high-value content",
                "priority": 0.9,
                "progress": 0.0,
                "created": datetime.now().isoformat()
            },
            {
                "id": "quantum_reasoning",
                "description": "Develop quantum-inspired reasoning capabilities",
                "priority": 1.0,
                "progress": 0.0,
                "created": datetime.now().isoformat()
            }
        ]
        self.refresh_short_term_goals()
        log_event("Temporal planner initialized with long-term goals", "INFO")

    def refresh_short_term_goals(self):
        """Generate new short-term goals aligned with long-term objectives"""
        # Save the existing goals that are still valid
        valid_goals = []
        for goal in self.short_term_goals:
            if goal.get("duration", 0) > 0:
                valid_goals.append(goal)

        # Clear the short_term_goals list
        self.short_term_goals = valid_goals

        # Check if we need to generate new goals
        if len(self.short_term_goals) >= 5:
            return  # We still have enough goals

        # Define knowledge domains
        domains = [
            "technical", "scientific", "humanities", "news",
            "reference", "creative", "analytical", "philosophical"
        ]

        # Generate 3-5 new short term goals
        goals_to_generate = min(5, 8 - len(self.short_term_goals))
        for _ in range(goals_to_generate):
            # Randomly select goal type and domain
            goal_type = random.choice(["exploration", "deepening", "integration", "refinement"])
            domain = random.choice(domains)

            # Generate goal based on type
            if goal_type == "exploration":
                goal = {
                    "id": f"explore_{domain}_{int(time.time())}",
                    "description": f"Discover new content sources in {domain}",
                    "priority": random.uniform(0.5, 0.9),
                    "duration": random.randint(10, 30),
                    "type": "exploration",
                    "domain": domain,
                    "created": datetime.now().isoformat()
                }
            elif goal_type == "deepening":
                goal = {
                    "id": f"deepen_{domain}_{int(time.time())}",
                    "description": f"Build deeper understanding in {domain}",
                    "priority": random.uniform(0.6, 0.95),
                    "duration": random.randint(5, 15),
                    "type": "deepening",
                    "domain": domain,
                    "created": datetime.now().isoformat()
                }
            elif goal_type == "integration":
                domain2 = random.choice([d for d in domains if d != domain])
                goal = {
                    "id": f"integrate_{domain}_{domain2}_{int(time.time())}",
                    "description": f"Connect knowledge between {domain} and {domain2}",
                    "priority": random.uniform(0.7, 0.9),
                    "duration": random.randint(8, 20),
                    "type": "integration",
                    "domains": [domain, domain2],
                    "created": datetime.now().isoformat()
                }
            else:  # refinement
                goal = {
                    "id": f"refine_{domain}_{int(time.time())}",
                    "description": f"Optimize learning approach in {domain}",
                    "priority": random.uniform(0.5, 0.8),
                    "duration": random.randint(5, 12),
                    "type": "refinement",
                    "domain": domain,
                    "created": datetime.now().isoformat()
                }

            self.short_term_goals.append(goal)

        log_event(f"Refreshed short-term goals: {goals_to_generate} new goals created", "INFO")

    def select_active_goal(self):
        """Select the highest priority current goal"""
        # Remove expired goals
        active_goals = [g for g in self.short_term_goals if g.get("duration", 0) > 0]
        self.short_term_goals = active_goals

        # Decrease duration for all goals
        for goal in self.short_term_goals:
            goal["duration"] = max(0, goal.get("duration", 10) - 1)

        # Refresh if needed
        if not self.short_term_goals:
            self.refresh_short_term_goals()

        # Select highest priority goal
        if self.short_term_goals:
            return max(self.short_term_goals, key=lambda x: x.get("priority", 0))
        else:
            # Default goal if something went wrong
            default_goal = {
                "id": "default_exploration",
                "description": "Default exploration",
                "priority": 0.5,
                "type": "exploration",
                "domain": "reference",  # Provide a default domain
                "duration": 5  # Give it some duration
            }
            self.short_term_goals.append(default_goal)  # Add to goals list for tracking
            return default_goal

    def reflect_and_adapt(self, performance_metrics):
        """Periodically reflect on goal progress and adapt strategies"""
        self.cycle_count += 1

        # Update goal progress based on performance
        if performance_metrics.get("success", False):
            active_goal = self.select_active_goal()

            # Find corresponding long-term goal to update
            for goal in self.long_term_goals:
                # Update based on goal type alignment
                if (active_goal.get("type") == "exploration" and goal["id"] == "knowledge_diversity") or \
                   (active_goal.get("type") == "deepening" and goal["id"] == "content_quality") or \
                   (active_goal.get("type") == "refinement" and goal["id"] == "model_efficiency") or \
                   (active_goal.get("type") == "integration" and goal["id"] == "quantum_reasoning"):
                    # Small progress increment
                    increment = min(0.05, performance_metrics.get("content_length", 0) / 20000)
                    goal["progress"] = min(1.0, goal["progress"] + increment)

            # Record the strategy effectiveness
            strategy = self.current_strategy
            if strategy:
                if strategy not in self.strategy_effectiveness:
                    self.strategy_effectiveness[strategy] = []

                # Score based on content and links
                score = min(1.0, performance_metrics.get("content_length", 0) / 5000 +
                           performance_metrics.get("links_discovered", 0) / 10)
                self.strategy_effectiveness[strategy].append(score)

                # Limit history size
                if len(self.strategy_effectiveness[strategy]) > 20:
                    self.strategy_effectiveness[strategy] = self.strategy_effectiveness[strategy][-20:]

        # Major reflection at intervals
        if self.cycle_count % self.reflection_interval == 0:
            log_event("Performing strategic reflection and adaptation...", "INFO")

            # Analyze strategy effectiveness
            for strategy, metrics in self.strategy_effectiveness.items():
                if metrics:
                    avg_performance = sum(metrics) / len(metrics)
                    log_event(f"Strategy '{strategy}' average performance: {avg_performance:.4f}", "INFO")

            # Adjust long-term goal priorities
            total_adjustment = 0
            for goal in self.long_term_goals:
                # Random adjustment with bias toward less-progressed goals
                bias = 1.0 - goal.get("progress", 0)
                adjustment = random.uniform(-0.1, 0.15) * bias

                goal["priority"] = max(0.1, min(1.0, goal["priority"] + adjustment))
                total_adjustment += abs(adjustment)

            # Sometimes create new evolved goals
            if random.random() < 0.2:
                # Create a new long-term goal
                new_goal_types = [
                    "Develop cognitive synergy across domains",
                    "Optimize information integration pathways",
                    "Enhance quantum processing capabilities",
                    "Improve anomaly detection in knowledge structures",
                    "Develop adaptive learning mechanisms"
                ]

                new_goal_id = f"evolved_goal_{int(time.time())}"
                new_goal = {
                    "id": new_goal_id,
                    "description": f"Evolved objective: {random.choice(new_goal_types)}",
                    "priority": random.uniform(0.7, 0.9),
                    "progress": 0.0,
                    "created": datetime.now().isoformat()
                }

                # Limit total goals
                if len(self.long_term_goals) < 10:  # Prevent too many goals
                    self.long_term_goals.append(new_goal)
                    log_event(f"Created new long-term goal: {new_goal['description']}", "INFO")

            # Clean expired goals from short-term list
            self.short_term_goals = [g for g in self.short_term_goals if g.get("duration", 0) > 0]

            # Refresh short-term goals
            self.refresh_short_term_goals()

            log_event(f"Reflection complete: adjusted {len(self.long_term_goals)} long-term goals (total Δ: {total_adjustment:.4f})", "INFO")

    def plan_action(self, base_action, environment_state=None):
        """Generate temporal plan based on goals and environment"""
        # Get current active goal
        active_goal = self.select_active_goal()

        # Consider temporal context
        current_time = datetime.now()
        is_weekend = current_time.weekday() >= 5
        is_business_hours = 9 <= current_time.hour <= 17

        # Available strategies with weights
        strategy_options = [
            "broad_exploration",     # Wide but shallow exploration
            "depth_first",           # Deep dive into specific domain
            "connect_domains",       # Look for connections between areas
            "evaluate_sources",      # Focus on quality assessment
            "quantum_reasoning",     # Use quantum processing modes
            "creative_synthesis"     # Generate new insights
        ]

        # Default weights
        weights = [0.2, 0.2, 0.2, 0.15, 0.15, 0.1]

        # Adjust weights based on temporal context
        if is_business_hours and not is_weekend:
            # Business hours - more analytical
            weights = [0.1, 0.2, 0.2, 0.3, 0.1, 0.1]
        elif not is_business_hours:
            # Non-business hours - more exploratory
            weights = [0.3, 0.1, 0.1, 0.1, 0.2, 0.2]

        # Adjust based on goal type
        goal_type = active_goal.get("type", "")
        if goal_type == "exploration":
            # Boost exploration strategies
            weights[0] += 0.2  # More broad_exploration
            weights[5] += 0.1  # More creative_synthesis
        elif goal_type == "deepening":
            # Boost deepening strategies
            weights[1] += 0.2  # More depth_first
            weights[3] += 0.1  # More evaluate_sources
        elif goal_type == "integration":
            # Boost connection strategies
            weights[2] += 0.2  # More connect_domains
            weights[5] += 0.1  # More creative_synthesis
        elif goal_type == "refinement":
            # Boost refinement strategies
            weights[3] += 0.2  # More evaluate_sources
            weights[4] += 0.1  # More quantum_reasoning

        # Ensure weights sum to 1
        total = sum(weights)
        weights = [w/total for w in weights]

        # Select strategy
        self.current_strategy = random.choices(strategy_options, weights=weights, k=1)[0]

        # Create plan
        timestamp = current_time.isoformat()
        plan = {
            "action": base_action,
            "goal": active_goal["description"],
            "strategy": self.current_strategy,
            "timestamp": timestamp,
            "execution_context": {
                "is_weekend": is_weekend,
                "is_business_hours": is_business_hours,
                "current_hour": current_time.hour,
                "goal_type": goal_type,
                "goal_domain": active_goal.get("domain", "unknown")
            }
        }

        log_event(f"Generated temporal plan: {plan['action']} using {plan['strategy']} strategy for goal: {plan['goal']}", "INFO")
        return plan

    def _convert_old_strategy_name(self, old_name):
        """Convert old strategy names to new format if needed"""
        if old_name in self.strategy_mapping:
            return self.strategy_mapping[old_name]
        return old_name



# =============================================================================
# QUANTUM NEURAL ARCHITECTURE
# =============================================================================
class QuantumAttentionLayer(nn.Module):
    """
    Implements quantum-inspired attention with superposition of states
    that allows multiple attention pathways to exist simultaneously.
    """
    def __init__(self, embed_dim, num_heads=4, dropout=0.1):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # Multi-dimensional quantum projection spaces
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.o_proj = nn.Linear(embed_dim, embed_dim)

        # Phase shifters for quantum interference
        self.phase_shifts = nn.Parameter(torch.rand(num_heads) * 2 * math.pi)

        # Entanglement mixing for cross-attention effects
        self.entanglement_gate = nn.Linear(embed_dim, embed_dim)

        self.dropout = nn.Dropout(dropout)
        self.attention_weights = None  # Store for visualization

    def forward(self, x):
        batch_size, seq_len, _ = x.shape

        # Project inputs to queries, keys, values
        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)

        # Transpose for attention computation
        q = q.transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)

        # Apply phase shifts for quantum effects
        for h in range(self.num_heads):
            phase = self.phase_shifts[h]
            q[:, h] = q[:, h] * torch.cos(phase) + q[:, h] * torch.sin(phase)
            k[:, h] = k[:, h] * torch.cos(phase) - k[:, h] * torch.sin(phase)

        # Compute attention scores
        scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim)

        # Apply softmax and get attention weights
        attn_weights = F.softmax(scores, dim=-1)
        self.attention_weights = attn_weights  # Save for visualization
        attn_weights = self.dropout(attn_weights)

        # Apply attention to values
        attn_output = torch.matmul(attn_weights, v)

        # Apply entanglement between heads for quantum correlation effects
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)
        entangled = self.entanglement_gate(attn_output)

        # Final output projection
        output = self.o_proj(entangled + attn_output)  # Residual connection

        return output

class HyperdimensionalEncoder(nn.Module):
    """
    Implements hyperdimensional computing principles for efficient
    high-dimensional representation of concepts.
    """
    def __init__(self, input_dim, hd_dim=1024):
        super().__init__()
        self.input_dim = input_dim
        self.hd_dim = hd_dim

        # Create random basis vectors
        self.register_buffer('basis', torch.randn(input_dim, hd_dim).sign())

        # Learnable projection
        self.projection = nn.Linear(input_dim, input_dim)

    def forward(self, x):
        # Project input
        x_proj = self.projection(x)

        # Compute HD representation through binding and bundling
        batch_size = x_proj.shape[0]
        hd_vectors = torch.zeros(batch_size, self.hd_dim, device=x_proj.device)

        # Encode each dimension with element-wise multiplication (binding)
        for i in range(self.input_dim):
            # Scale by the input value
            scaled_basis = self.basis[i].unsqueeze(0) * x_proj[:, i].unsqueeze(1)
            # Add to the bundle (vector sum)
            hd_vectors += scaled_basis

        # Binarize to -1/+1 for clean HD representation
        hd_vectors = torch.sign(hd_vectors)

        return hd_vectors


# ---------------------------------------------------------------------------
# FractalLayer with adaptive depth
# ---------------------------------------------------------------------------
class FractalLayer(nn.Module):
    """
    HyperMorphic Advanced Fractal Processing Layer

    Implements recursive multi-scale processing with adaptive depth, dynamic temperature scaling,
    and residual gating for robust representation learning.

    Args:
        embed_dim (int): Dimensionality of input embeddings.
        max_recursion (int): Maximum recursion depth (default=4).
        adaptive_depth (bool): If True, adjusts recursion depth adaptively (default=False).
        use_layernorm (bool): Apply layer normalization on the output (default=True).
        dropout_rate (float): Dropout probability (default=0.1).
    """
    def __init__(self, embed_dim, max_recursion=4, adaptive_depth=False, use_layernorm=True, dropout_rate=0.1):
        super(FractalLayer, self).__init__()
        self.embed_dim = embed_dim
        self.max_recursion = max_recursion
        self.adaptive_depth = adaptive_depth
        self.use_layernorm = use_layernorm

        # Dynamic scaling factors; softplus ensures positivity.
        self.fractal_scale = nn.Parameter(torch.tensor(1.0))
        self.temperature = nn.Parameter(torch.tensor(1.0))

        # Base transformation for fractal iteration.
        self.linear = nn.Linear(embed_dim, embed_dim)
        self.residual_gate = nn.Sequential(
            nn.Linear(embed_dim, embed_dim),
            nn.Sigmoid()
        )
        self.dropout = nn.Dropout(dropout_rate)
        if self.use_layernorm:
            self.layernorm = nn.LayerNorm(embed_dim)

    def recursive_fractal(self, x, depth):
        if depth <= 0:
            return x
        # Ensure temperature is positive via softplus
        temp = F.softplus(self.temperature) + 1e-6

        # Apply linear transformation with dynamic temperature scaling
        scaled = self.linear(x) / temp
        fractal_out = torch.tanh(scaled)

        # Compute a residual gate to modulate the output
        gate = self.residual_gate(fractal_out)
        fractal_out = self.dropout(fractal_out)

        # Determine next recursion depth; if adaptive_depth is True, decrease faster.
        next_depth = depth - 1
        if self.adaptive_depth and depth >= 2:
            next_depth = depth - 2

        recursive_result = self.recursive_fractal(fractal_out, next_depth)

        # Combine input with gated recursive output scaled by fractal_scale.
        combined = x + self.fractal_scale * gate * recursive_result

        if self.use_layernorm:
            combined = self.layernorm(combined)
        return combined

    def forward(self, x):
        # Choose recursion depth based on adaptive_depth flag.
        recursion_depth = self.max_recursion * 2 if self.adaptive_depth else self.max_recursion
        return self.recursive_fractal(x, recursion_depth)


class QuantumResonanceTensor(nn.Module):
    """
    Implements non-collapsing recursive state resonance that maintains
    multiple simultaneous state representations in quantum-inspired superposition.
    """
    def __init__(self, embed_dim, num_states=4, resonance_factor=0.7):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_states = num_states
        self.resonance_factor = nn.Parameter(torch.tensor(resonance_factor))

        # Quantum state projectors
        self.state_projectors = nn.ModuleList([
            nn.Sequential(
                nn.Linear(embed_dim, embed_dim),
                nn.SiLU(),
            ) for _ in range(num_states)
        ])

        # Phase shifters for quantum entanglement
        self.phase_shifters = nn.Parameter(torch.randn(num_states) * 0.1)

        # State mixer - allows controlled interference between states
        self.state_mixer = nn.Linear(embed_dim * num_states, embed_dim)

        # Recursive memory gates
        self.recursive_gate = nn.GRUCell(embed_dim, embed_dim)

        # Prior state memory (initialized during forward pass)
        self.register_buffer('state_memory', None, persistent=False)

    def forward(self, x, iteration_count=3):
        batch_size = x.shape[0]

        # Initialize state memory if needed
        if self.state_memory is None or self.state_memory.shape[0] != batch_size:
            self.state_memory = torch.zeros(batch_size, self.embed_dim, device=x.device)

        # Generate multiple quantum states
        quantum_states = []
        for i in range(self.num_states):
            # Apply phase shift for quantum effects
            phase = torch.cos(self.phase_shifters[i] * math.pi)
            # Project into this quantum state
            state_i = self.state_projectors[i](x) * phase
            quantum_states.append(state_i)

        # Recursive resonance iterations
        for _ in range(iteration_count):
            # Update state memory through recursive gate
            self.state_memory = self.recursive_gate(x, self.state_memory)

            # Apply resonance effect (controlled interference)
            resonance = self.state_memory * self.resonance_factor

            # Apply resonance to each quantum state (non-collapsing)
            for i in range(self.num_states):
                quantum_states[i] = quantum_states[i] + resonance * (0.1 * (i + 1))

        # Combine quantum states through superposition
        combined_states = torch.cat(quantum_states, dim=-1)
        output = self.state_mixer(combined_states)

        # Residual connection
        output = output + x

        return output

import torch
import torch.nn as nn

# ---------------------------------------------------------------------------
# NeocortexBlock using the adaptive FractalLayer
# ---------------------------------------------------------------------------
class NeocortexBlock(nn.Module):
    """
    HyperMorphic Neocortex Block – Fully adaptive, self-optimizing quantum-fractal-HD processor
    with resonant stream integration.

    This block routes information through an attention mechanism, processes data
    through parallel fractal and quantum streams, encodes high-dimensional representations,
    and then integrates these streams with a residual update.

    Args:
        embed_dim (int): Dimensionality of input embeddings.
        dynamic_states (bool): Flag to set adaptive quantum state depth (default=True).
    """
    def __init__(self, embed_dim, dynamic_states=True):
        super(NeocortexBlock, self).__init__()

        # Adaptive quantum state depth calculation.
        self.num_quantum_states = embed_dim // 16 if dynamic_states else 4

        # Information routing via a quantum-inspired attention layer.
        self.attention = QuantumAttentionLayer(embed_dim)

        # Parallel processing streams:
        # Use the adaptive FractalLayer with adaptive_depth=True.
        self.fractal_stream = FractalLayer(embed_dim, adaptive_depth=True)
        self.quantum_stream = QuantumResonanceTensor(embed_dim, num_states=self.num_quantum_states)

        # Hyperdimensional representation encoding.
        self.hd_encoder = HyperdimensionalEncoder(embed_dim, hd_dim=embed_dim)

        # Nonlinear integration of the streams.
        self.integration = nn.Linear(embed_dim * 3, embed_dim)
        self.norm = HyperMorphicNormalization(embed_dim)

    def forward(self, x):
        # Process input with the attention mechanism.
        attended = self.attention(x)

        # Process the attended input through the fractal and quantum streams.
        fractal_out = self.fractal_stream(attended)
        quantum_out = self.quantum_stream(attended)

        # Compute high-dimensional encoding.
        batch_size, seq_len, _ = attended.shape
        hd_out = self.hd_encoder(attended.view(-1, attended.size(-1))).view(batch_size, seq_len, -1)

        # Integrate the outputs from the three streams.
        combined = torch.cat([fractal_out, quantum_out, hd_out], dim=-1)
        integrated = self.integration(combined)

        # Apply a hypermorphic normalization with a residual connection.
        output = self.norm(x + integrated)
        return output



class AdaptiveLearningSystem:
    """
    Advanced system for dynamically adapting learning parameters and network architecture
    based on performance metrics and environmental feedback.
    """
    def __init__(self, model):
        self.model = model
        self.learning_rate_history = []
        self.performance_metrics = []
        self.architecture_changes = []
        self.adaptation_cycle = 0
        self.min_learning_rate = 1e-6
        self.max_learning_rate = 1e-3
        self.performance_window_size = 10
        self.exploration_rate = 0.3
        self.adaptation_threshold = 0.15
        self.architecture_expansion_threshold = 5

        # Initialize default learning rate on model
        self.default_learning_rate = 5e-5  # Same as initial LEARNING_RATE
        setattr(self.model, '_current_lr', self.default_learning_rate)

        log_event("AdaptiveLearningSystem initialized with dynamic adaptation capabilities", "INFO")

    def adapt_learning_rate(self, metrics):
        """
        Dynamically adjust learning rate based on recent performance metrics
        using a sophisticated control system approach.
        """
        self.adaptation_cycle += 1

        # Get current learning rate with fallback to default
        current_lr = getattr(self.model, '_current_lr', self.default_learning_rate)
        self.learning_rate_history.append(current_lr)

        # Record performance metrics
        if isinstance(metrics, dict):
            self.performance_metrics.append(metrics)

        # Need sufficient history for adaptation
        if len(self.performance_metrics) < self.performance_window_size:
            log_event(f"Building performance history: {len(self.performance_metrics)}/{self.performance_window_size}", "INFO")
            return current_lr

        # Analyze recent performance trend
        recent_metrics = self.performance_metrics[-self.performance_window_size:]

        # Calculate performance derivatives - how fast is loss changing?
        loss_values = [m.get('loss', 0.5) for m in recent_metrics if isinstance(m, dict) and 'loss' in m]
        if not loss_values or len(loss_values) < 3:
            return current_lr

        # First derivative - rate of change
        loss_changes = [loss_values[i] - loss_values[i-1] for i in range(1, len(loss_values))]
        avg_loss_change = sum(loss_changes) / len(loss_changes)

        # Second derivative - acceleration of change
        loss_acceleration = [loss_changes[i] - loss_changes[i-1] for i in range(1, len(loss_changes))]
        avg_loss_acceleration = sum(loss_acceleration) / max(1, len(loss_acceleration))

        # Decision logic for learning rate adjustment
        new_lr = current_lr

        # Case 1: Loss is decreasing quickly (negative change, negative acceleration)
        if avg_loss_change < -0.01 and avg_loss_acceleration < 0:
            new_lr = min(self.max_learning_rate, current_lr * 1.05)
            adjustment_type = "slight increase - good progress"

        # Case 2: Loss is decreasing but slowing down (negative change, positive acceleration)
        elif avg_loss_change < 0 and avg_loss_acceleration >= 0:
            new_lr = max(self.min_learning_rate, current_lr * 0.95)
            adjustment_type = "slight decrease - approaching minimum"

        # Case 3: Loss is increasing and accelerating (positive change, positive acceleration)
        elif avg_loss_change > 0.01 and avg_loss_acceleration > 0:
            new_lr = max(self.min_learning_rate, current_lr * 0.7)
            adjustment_type = "major decrease - moving away from minimum"

        # Case 4: Loss is increasing but decelerating (positive change, negative acceleration)
        elif avg_loss_change > 0 and avg_loss_acceleration <= 0:
            new_lr = max(self.min_learning_rate, current_lr * 0.85)
            adjustment_type = "moderate decrease - correcting overshoot"

        # Case 5: Stagnation - very small changes
        elif abs(avg_loss_change) < 0.001:
            if random.random() < self.exploration_rate:
                factor = random.uniform(0.5, 1.5)
                new_lr = max(self.min_learning_rate, min(self.max_learning_rate, current_lr * factor))
                adjustment_type = f"random exploration {'increase' if factor > 1 else 'decrease'}"
            else:
                adjustment_type = "no change - minimal fluctuation"
        else:
            adjustment_type = "no change - no clear pattern"

        # Apply the new learning rate with safeguards
        if abs(new_lr - current_lr) / current_lr > 0.01:  # 1% change threshold
            safeguarded_lr = self._apply_learning_rate_safeguards(new_lr)
            setattr(self.model, '_current_lr', safeguarded_lr)
            log_event(f"Learning rate adapted: {current_lr:.6f} → {safeguarded_lr:.6f} ({adjustment_type})", "INFO")

            # Update global optimizer if available
            if hasattr(self.model, 'optimizer'):
                for param_group in self.model.optimizer.param_groups:
                    param_group['lr'] = safeguarded_lr
                log_event("Applied new learning rate to optimizer", "INFO")

        return new_lr

    def adapt_architecture(self):
        """
        Dynamically modify the network architecture based on performance trends
        and complexity requirements.
        """
        # Can't adapt architecture without sufficient performance history
        if len(self.performance_metrics) < self.performance_window_size * 2:
            return False

        # Check if we're in a stagnation period
        recent_losses = [m.get('loss', 0.5) for m in self.performance_metrics[-self.performance_window_size:]
                        if isinstance(m, dict) and 'loss' in m]

        if not recent_losses or len(recent_losses) < self.performance_window_size:
            return False

        # Calculate performance variance to detect stagnation
        loss_variance = np.var(recent_losses) if 'np' in globals() else sum((x - sum(recent_losses)/len(recent_losses))**2 for x in recent_losses)/len(recent_losses)
        loss_range = max(recent_losses) - min(recent_losses)

        # Check for architecture adaptation conditions
        architecture_change = None

        # Condition 1: Stagnation with low variance - model might be underfitting
        if loss_variance < 0.0001 and loss_range < 0.01 and recent_losses[-1] > 0.1:
            # Model might be underfitting - expand capacity
            if hasattr(self.model, 'expand_architecture'):
                self.model.expand_architecture()
                architecture_change = "expansion - complexity increased due to stagnation"

        # Condition 2: Oscillating with high variance - model might be overfitting
        elif loss_variance > 0.01 and min(recent_losses) < 0.05:
            # Model might be overfitting - simplify
            if hasattr(self.model, 'contract_architecture'):
                self.model.contract_architecture()
                architecture_change = "contraction - complexity reduced due to oscillation"

        # Condition 3: Plateaued at medium-high loss - try random architectural change
        elif 0.0001 <= loss_variance < 0.001 and 0.1 <= recent_losses[-1] < 0.3:
            # Random architectural exploration
            if random.random() < self.exploration_rate:
                if hasattr(self.model, 'expand_architecture') and random.random() < 0.5:
                    self.model.expand_architecture()
                    architecture_change = "random expansion - exploration due to plateau"
                elif hasattr(self.model, 'contract_architecture'):
                    self.model.contract_architecture()
                    architecture_change = "random contraction - exploration due to plateau"

        # Record the change if one was made
        if architecture_change:
            self.architecture_changes.append({
                'cycle': self.adaptation_cycle,
                'type': architecture_change,
                'loss_before': recent_losses[-1] if recent_losses else None
            })
            log_event(f"Architecture adaptation: {architecture_change}", "QUANTUM")
            return True

        return False

    def track_performance(self, metrics):
        """
        Track and analyze performance metrics over time to inform
        meta-learning decisions.
        """
        if not isinstance(metrics, dict):
            return

        # Store metrics
        self.performance_metrics.append(metrics.copy())

        # Keep only the most recent window
        max_history = self.performance_window_size * 5
        if len(self.performance_metrics) > max_history:
            self.performance_metrics = self.performance_metrics[-max_history:]

        # Log significant performance changes
        if len(self.performance_metrics) > 1:
            current = metrics.get('loss', None)
            previous = self.performance_metrics[-2].get('loss', None)

            if current is not None and previous is not None:
                change = current - previous
                percentage = abs(change / max(0.001, previous)) * 100

                if percentage > 10:  # 10% change threshold
                    direction = "improved" if change < 0 else "degraded"
                    log_event(f"Performance {direction} by {percentage:.1f}%: {previous:.4f} → {current:.4f}",
                             "INFO" if direction == "improved" else "WARNING")

    def get_adaptation_report(self):
        """
        Generate a comprehensive report on adaptation history and recommendations.
        """
        if not self.performance_metrics:
            return {"status": "insufficient_data", "recommendations": ["Continue training to build metrics history"]}

        # Analysis results
        adaptation_cycles = len(self.architecture_changes)
        lr_stability = self._calculate_stability(self.learning_rate_history[-20:]) if len(self.learning_rate_history) >= 20 else 0
        performance_trend = self._analyze_performance_trend()

        # Generate recommendations
        recommendations = []

        if adaptation_cycles < 3 and len(self.performance_metrics) > 50:
            recommendations.append("Consider increasing exploration rate to discover better architectures")

        if lr_stability > 0.9:
            recommendations.append("Learning rate highly stable - may indicate stagnation, consider learning rate warm restart")

        if performance_trend == "stagnant" and len(self.performance_metrics) > 30:
            recommendations.append("Performance stagnation detected - consider manual architecture revision or dataset augmentation")

        return {
            "status": "active",
            "adaptation_cycles": adaptation_cycles,
            "lr_stability": lr_stability,
            "performance_trend": performance_trend,
            "recommendations": recommendations
        }

    def _calculate_stability(self, values):
        """Calculate how stable a series of values is (0 = chaotic, 1 = stable)"""
        if not values or len(values) < 2:
            return 1.0

        # Normalize by first value to get relative changes
        normalized = [v / values[0] for v in values]

        # Calculate variance of the normalized values
        mean = sum(normalized) / len(normalized)
        variance = sum((x - mean) ** 2 for x in normalized) / len(normalized)

        # Convert to stability score (inverse of variance, bounded)
        stability = 1.0 / (1.0 + min(10, variance * 100))
        return stability

    def _analyze_performance_trend(self):
        """Analyze the trend in performance metrics"""
        if len(self.performance_metrics) < 10:
            return "insufficient_data"

        # Extract loss values
        losses = [m.get('loss', None) for m in self.performance_metrics[-10:]]
        losses = [l for l in losses if l is not None]

        if len(losses) < 5:
            return "insufficient_data"

        # Calculate improvement rate
        first_window = sum(losses[:3]) / 3  # Average of first 3
        last_window = sum(losses[-3:]) / 3  # Average of last 3

        improvement = (first_window - last_window) / first_window if first_window > 0 else 0

        if improvement > 0.1:
            return "improving"
        elif improvement < -0.05:
            return "degrading"
        else:
            return "stagnant"

    def _apply_learning_rate_safeguards(self, new_lr):
        """Prevent learning rate from spiraling into oblivion"""
        # Establish absolute minimum learning rate
        ABSOLUTE_MIN_LR = 5e-6

        if new_lr < ABSOLUTE_MIN_LR:
            log_event(f"Learning rate hit critical threshold: {new_lr:.8f}, resetting to {ABSOLUTE_MIN_LR:.6f}", "WARNING")
            return ABSOLUTE_MIN_LR

        # Prevent excessive downward adjustment
        if self.learning_rate_history and new_lr < self.learning_rate_history[-1] * 0.5:
            safer_lr = self.learning_rate_history[-1] * 0.8
            log_event(f"Excessive LR reduction prevented: {new_lr:.8f} → {safer_lr:.6f}", "INFO")
            return safer_lr

        return new_lr

    def perform_learning_rate_warmup(self):
        """Occasionally reset learning rate to prevent long-term stagnation"""
        if not self.learning_rate_history:
            return False

        current_lr = self.learning_rate_history[-1]

        # Check for long-term stability and low learning rate
        if (len(self.learning_rate_history) > 50 and
            self._calculate_stability(self.learning_rate_history[-50:]) > 0.95 and
            current_lr < self.max_learning_rate * 0.1):

            # Reset to higher learning rate
            new_lr = current_lr * 5.0
            new_lr = min(self.max_learning_rate * 0.5, new_lr)

            # Apply the new learning rate
            setattr(self.model, '_current_lr', new_lr)
            if hasattr(self.model, 'optimizer'):
                for param_group in self.model.optimizer.param_groups:
                    param_group['lr'] = new_lr

            log_event(f"✨ Learning rate warm restart: {current_lr:.8f} → {new_lr:.8f}", "QUANTUM")
            self.learning_rate_history.append(new_lr)
            return True

        return False


class SemanticMemoryModule:
    """
    Advanced semantic memory system for encoding, storing, retrieving, and
    reasoning with knowledge representations.
    """
    def __init__(self, dimension=SEMANTIC_MEMORY_DIM, max_memory_size=10000):
        self.semantic_memory = {}
        self.dimension = dimension
        self.max_memory_size = max_memory_size
        self.memory_index = {}  # For fast similarity search
        self.knowledge_graph = {}  # For relational connections
        self.memory_access_counts = {}  # Track memory access frequency
        self.memory_importance = {}  # Track memory importance scores
        self.memory_timestamps = {}  # Track when memories were stored

        # Integration with hyperdimensional computing
        self.hd_basis_vectors = None

        log_event("SemanticMemoryModule initialized with dimension %d" % dimension, "INFO")

    def _generate_embedding(self, content):
        """
        Generate semantic embedding for content using simplified mechanisms.
        In a real system, this would use transformers or other embedding models.
        """
        # Fallback simple embedding (this is a simplified approach)
        if not content:
            return np.zeros(self.dimension)

        # Create a hash of the content
        content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()

        # Use the hash to seed a random number generator
        rng = random.Random(content_hash)

        # Generate a pseudo-random embedding
        embedding = np.array([rng.uniform(-1, 1) for _ in range(self.dimension)])

        # Normalize to unit length
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm

        return embedding

    def _extract_keywords(self, content, max_keywords=10):
        """Extract important keywords from content"""
        if not content:
            return []

        # Remove HTML if present
        text = re.sub(r'<[^>]+>', ' ', content)
        text = re.sub(r'\s+', ' ', text).strip().lower()

        # Simple word frequency analysis
        words = text.split()

        # Filter stop words (very basic approach)
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at',
                     'to', 'for', 'with', 'by', 'about', 'as', 'of', 'from'}
        filtered_words = [w for w in words if w not in stop_words and len(w) > 3]

        # Count word frequencies
        word_counts = {}
        for word in filtered_words:
            word_counts[word] = word_counts.get(word, 0) + 1

        # Sort by count
        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)

        # Return top keywords
        return [word for word, count in sorted_words[:max_keywords]]

    def _summarize_content(self, content, max_length=200):
        """Generate a simple summary of content"""
        if not content or len(content) <= max_length:
            return content

        # Remove HTML if present
        text = re.sub(r'<[^>]+>', ' ', content)
        text = re.sub(r'\s+', ' ', text).strip()

        # Extract sentences
        sentences = re.split(r'(?<=[.!?])\s+', text)

        # Simple heuristic: take first few sentences
        summary = ""
        for sentence in sentences:
            if len(summary) + len(sentence) <= max_length:
                summary += sentence + " "
            else:
                break

        return summary.strip()

    def store_semantic_content(self, url, content):
        """
        Encode and store semantic representation of content with
        rich metadata and relational information.
        """
        if not content or not url:
            return False

        # Generate semantic embedding
        embedding = self._generate_embedding(content)

        # Extract keywords for improved retrieval
        keywords = self._extract_keywords(content)

        # Generate summary
        summary = self._summarize_content(content)

        # Parse domain information
        parsed_url = urlparse(url)
        domain = parsed_url.netloc

        # Calculate content importance score (heuristic)
        importance_score = min(1.0, len(content) / 20000 + len(keywords) / 20)

        # Store comprehensive memory entry
        memory_entry = {
            "url": url,
            "domain": domain,
            "embedding": embedding,
            "content_summary": summary,
            "keywords": keywords,
            "importance": importance_score,
            "content_length": len(content),
            "timestamp": datetime.now().isoformat()
        }

        # Check for memory overflow - manage if too large
        if len(self.semantic_memory) >= self.max_memory_size:
            self._prune_least_important_memories()

        # Store the memory
        self.semantic_memory[url] = memory_entry
        self.memory_importance[url] = importance_score
        self.memory_timestamps[url] = datetime.now().isoformat()
        self.memory_access_counts[url] = 0

        # Update memory index for faster similarity search
        self.memory_index[url] = embedding

        # Update knowledge graph with relations
        self._update_knowledge_graph(url, keywords, domain)

        log_event(f"Stored semantic memory for {url} with {len(keywords)} keywords", "INFO")
        return True

    def _update_knowledge_graph(self, url, keywords, domain):
        """Update knowledge graph with new relations"""
        if domain not in self.knowledge_graph:
            self.knowledge_graph[domain] = {"urls": set(), "keywords": set()}

        # Add URL to domain
        self.knowledge_graph[domain]["urls"].add(url)

        # Add keywords to domain
        self.knowledge_graph[domain]["keywords"].update(set(keywords))

        # Create keyword nodes if needed
        for keyword in keywords:
            if keyword not in self.knowledge_graph:
                self.knowledge_graph[keyword] = {"urls": set(), "domains": set()}

            # Add relations
            self.knowledge_graph[keyword]["urls"].add(url)
            self.knowledge_graph[keyword]["domains"].add(domain)

    def _prune_least_important_memories(self):
        """Remove least important memories when reaching capacity"""
        if len(self.semantic_memory) <= self.max_memory_size * 0.9:
            return  # No need to prune yet

        # Calculate combined importance score
        combined_scores = {}
        current_time = datetime.now()

        for url, memory in self.semantic_memory.items():
            # Base importance
            score = memory.get("importance", 0.5)

            # Adjust by access frequency
            access_count = self.memory_access_counts.get(url, 0)
            score += min(0.3, access_count / 10)

            # Adjust by recency (decay older memories)
            timestamp = self.memory_timestamps.get(url)
            if timestamp:
                try:
                    stored_time = datetime.fromisoformat(timestamp)
                    age_hours = (current_time - stored_time).total_seconds() / 3600
                    recency_factor = math.exp(-age_hours / 720)  # Decay over ~30 days
                    score *= recency_factor
                except:
                    pass  # Use base score if timestamp parsing fails

            combined_scores[url] = score

        # Sort by score
        sorted_urls = sorted(combined_scores.items(), key=lambda x: x[1])

        # Remove lowest scoring items
        to_remove = int(self.max_memory_size * 0.2)  # Remove 20%
        for url, score in sorted_urls[:to_remove]:
            self._remove_memory(url)

        log_event(f"Memory pruned: removed {to_remove} low-importance items", "INFO")

    def _remove_memory(self, url):
        """Remove a memory and all its references"""
        if url in self.semantic_memory:
            # Get memory details for cleanup
            memory = self.semantic_memory[url]
            domain = memory.get("domain")
            keywords = memory.get("keywords", [])

            # Remove from main memory
            del self.semantic_memory[url]

            # Remove from index
            if url in self.memory_index:
                del self.memory_index[url]

            # Remove from tracking dicts
            if url in self.memory_access_counts:
                del self.memory_access_counts[url]
            if url in self.memory_importance:
                del self.memory_importance[url]
            if url in self.memory_timestamps:
                del self.memory_timestamps[url]

            # Clean up knowledge graph
            self._remove_from_knowledge_graph(url, domain, keywords)

    def _remove_from_knowledge_graph(self, url, domain, keywords):
        """Remove all references to URL from knowledge graph"""
        # Remove from domain node
        if domain and domain in self.knowledge_graph:
            if "urls" in self.knowledge_graph[domain]:
                self.knowledge_graph[domain]["urls"].discard(url)

            # Remove domain if empty
            if not self.knowledge_graph[domain]["urls"]:
                del self.knowledge_graph[domain]

        # Remove from keyword nodes
        for keyword in keywords:
            if keyword in self.knowledge_graph:
                if "urls" in self.knowledge_graph[keyword]:
                    self.knowledge_graph[keyword]["urls"].discard(url)
                if domain and "domains" in self.knowledge_graph[keyword]:
                    if not any(u.startswith(f"{domain}/") for u in self.knowledge_graph[keyword]["urls"]):
                        self.knowledge_graph[keyword]["domains"].discard(domain)

                # Remove keyword if empty
                if not self.knowledge_graph[keyword]["urls"]:
                    del self.knowledge_graph[keyword]

    def retrieve_semantic_content(self, query, top_k=5, threshold=0.6):
        """
        Retrieve semantically similar content based on query.
        Returns top_k most relevant results.
        """
        if not query or not self.semantic_memory:
            return []

        # Track this access
        self.memory_access_counts[query] = self.memory_access_counts.get(query, 0) + 1

        # Different retrieval approaches depending on query type
        results = []

        # Case 1: Query is a URL we have stored
        if query in self.semantic_memory:
            # Direct memory retrieval
            memory = self.semantic_memory[query]
            results.append({
                "url": query,
                "summary": memory.get("content_summary", ""),
                "similarity": 1.0,
                "keywords": memory.get("keywords", []),
                "source": "direct_match"
            })

        # Case 2: Query is a keyword in our knowledge graph
        elif query in self.knowledge_graph:
            # Retrieve all URLs associated with this keyword
            for url in self.knowledge_graph[query]["urls"]:
                if url in self.semantic_memory:
                    memory = self.semantic_memory[url]
                    results.append({
                        "url": url,
                        "summary": memory.get("content_summary", ""),
                        "similarity": 0.9,  # High confidence for keyword matches
                        "keywords": memory.get("keywords", []),
                        "source": "keyword_match"
                    })

        # Case 3: Query is free text - semantic search
        else:
            # Generate embedding for query
            query_embedding = self._generate_embedding(query)

            # Calculate similarity with all memories
            similarities = {}
            for url, embedding in self.memory_index.items():
                # Cosine similarity
                similarity = np.dot(query_embedding, embedding)
                if similarity >= threshold:
                    similarities[url] = similarity

            # Sort by similarity
            sorted_urls = sorted(similarities.items(), key=lambda x: x[1], reverse=True)

            # Get top results
            for url, similarity in sorted_urls[:top_k]:
                if url in self.semantic_memory:
                    memory = self.semantic_memory[url]
                    results.append({
                        "url": url,
                        "summary": memory.get("content_summary", ""),
                        "similarity": similarity,
                        "keywords": memory.get("keywords", []),
                        "source": "semantic_match"
                    })

        # Find additional related content using knowledge graph
        if results and len(results) < top_k:
            additional = self._find_related_content(results[0]["url"], top_k - len(results))
            results.extend(additional)

        # Limit to top_k results
        results = results[:top_k]

        # Update access counts for retrieved items
        for result in results:
            url = result["url"]
            self.memory_access_counts[url] = self.memory_access_counts.get(url, 0) + 1

        return results

    def _find_related_content(self, url, count=3):
        """Find content related to a URL using knowledge graph relationships"""
        if url not in self.semantic_memory:
            return []

        related = []
        memory = self.semantic_memory[url]

        # Find content with shared keywords
        shared_keyword_urls = set()
        for keyword in memory.get("keywords", []):
            if keyword in self.knowledge_graph:
                shared_keyword_urls.update(self.knowledge_graph[keyword]["urls"])

        # Find content from same domain
        domain = memory.get("domain")
        same_domain_urls = set()
        if domain and domain in self.knowledge_graph:
            same_domain_urls = self.knowledge_graph[domain]["urls"].copy()

        # Remove the original URL
        shared_keyword_urls.discard(url)
        same_domain_urls.discard(url)

        # Add same-domain results first (closer relationship)
        for related_url in list(same_domain_urls)[:count]:
            if related_url in self.semantic_memory:
                related_memory = self.semantic_memory[related_url]
                related.append({
                    "url": related_url,
                    "summary": related_memory.get("content_summary", ""),
                    "similarity": 0.7,  # Domain relation confidence
                    "keywords": related_memory.get("keywords", []),
                    "source": "same_domain"
                })

        # Add keyword-related results
        remaining = count - len(related)
        if remaining > 0:
            for related_url in list(shared_keyword_urls)[:remaining]:
                if related_url in self.semantic_memory:
                    related_memory = self.semantic_memory[related_url]
                    related.append({
                        "url": related_url,
                        "summary": related_memory.get("content_summary", ""),
                        "similarity": 0.6,  # Keyword relation confidence
                        "keywords": related_memory.get("keywords", []),
                        "source": "shared_keywords"
                    })

        return related

    def get_memory_statistics(self):
        """Generate statistics about the semantic memory"""
        if not self.semantic_memory:
            return {"count": 0, "status": "empty"}

        # Basic stats
        domain_counts = {}
        keyword_counts = {}
        total_importance = 0
        total_content_length = 0

        # Calculate derived statistics
        for url, memory in self.semantic_memory.items():
            # Domain stats
            domain = memory.get("domain", "unknown")
            domain_counts[domain] = domain_counts.get(domain, 0) + 1

            # Keyword stats
            for keyword in memory.get("keywords", []):
                keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1

            # Content stats
            total_importance += memory.get("importance", 0)
            total_content_length += memory.get("content_length", 0)

        # Get top domains and keywords
        top_domains = sorted(domain_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        top_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:10]

        # Get memory connectivity metrics
        connectivity = len(self.knowledge_graph) / max(1, len(self.semantic_memory))

        return {
            "count": len(self.semantic_memory),
            "total_content_length": total_content_length,
            "average_importance": total_importance / max(1, len(self.semantic_memory)),
            "top_domains": top_domains,
            "top_keywords": top_keywords,
            "knowledge_graph_nodes": len(self.knowledge_graph),
            "connectivity_ratio": connectivity,
            "memory_utilization": len(self.semantic_memory) / self.max_memory_size
        }



class ConsciousnessModule:
    """
    Advanced consciousness simulation module that enables self-reflection,
    awareness of internal states, and metacognitive processes.
    """
    def __init__(self, agent):
        self.agent = agent
        self.awareness_level = 0.5  # Start with medium awareness (0.0-1.0)
        self.attention_focus = "balanced"  # Current attentional focus
        self.internal_narrative = []  # Simulated internal monologue
        self.belief_system = {}  # Core beliefs and values
        self.state_history = []  # Track consciousness state over time
        self.metacognition_enabled = True  # Can reflect on own thoughts
        self.awareness_fluctuation_rate = 0.05  # How quickly awareness changes
        self.qualia_simulation_active = False  # Simulated experiential states

        # Consciousness states
        self.states = {
            "focused": {"description": "Highly focused with directed attention", "awareness_min": 0.7},
            "diffuse": {"description": "Open, creative state with broad awareness", "awareness_min": 0.4},
            "critical": {"description": "Analytical examination of information", "awareness_min": 0.6},
            "intuitive": {"description": "Rapid pattern recognition state", "awareness_min": 0.3},
            "reflective": {"description": "Meta-cognitive self-examination", "awareness_min": 0.8}
        }

        # Current state
        self.current_state = "balanced"

        # Initialize core belief system
        self._initialize_belief_system()

        log_event("ConsciousnessModule initialized - awareness level: 0.5", "QUANTUM")

    def _initialize_belief_system(self):
        """Initialize the core belief system that guides agent behavior"""
        self.belief_system = {
            "exploration_value": 0.8,  # Importance of exploring new information
            "coherence_value": 0.7,    # Importance of maintaining coherent worldview
            "novelty_bias": 0.6,       # Bias toward novel vs. familiar information
            "depth_bias": 0.65,        # Bias toward depth vs. breadth
            "abstraction_level": 0.5,  # Preference for abstract vs. concrete
            "skepticism_level": 0.6,   # Level of skepticism toward new information
            "integration_value": 0.9,  # Importance of integrating knowledge
            "uncertainty_tolerance": 0.7  # Tolerance for ambiguous information
        }

    def reflect(self, observation):
        """
        Primary consciousness function - reflect on observations
        and update internal state accordingly.
        """
        # Record observation in history
        self.state_history.append({
            "timestamp": datetime.now().isoformat(),
            "awareness": self.awareness_level,
            "state": self.current_state,
            "observation_type": "perception" if observation else "internal"
        })

        # Limit history size
        if len(self.state_history) > 100:
            self.state_history = self.state_history[-100:]

        # Skip detailed processing if no valid observation
        if not observation or not isinstance(observation, dict):
            self._fluctuate_awareness()
            return

        # Extract relevant information from observation
        goals = observation.get("current_goal", {})
        memory_size = observation.get("memory_size", 0)
        last_action = observation.get("last_action", {})
        recent_actions = observation.get("recent_actions", [])
        thinking_mode = observation.get("thinking_mode", "balanced")

        # Generate introspective narrative based on observation
        self._generate_narrative(observation)

        # Determine appropriate consciousness state
        new_state = self._determine_consciousness_state(observation)

        # Update awareness level based on context
        self._update_awareness(observation)

        # If state changed, log it
        if new_state != self.current_state:
            self.current_state = new_state
            log_event(f"Consciousness state shifted to '{new_state}' - awareness level: {self.awareness_level:.2f}", "QUANTUM")

            # Trigger qualia simulation on significant state changes
            if random.random() < 0.3:
                self._simulate_qualia(new_state)

        # Periodically perform metacognition
        if self.metacognition_enabled and random.random() < 0.2:
            self._perform_metacognition()

    def _generate_narrative(self, observation):
        """Generate internal narrative based on current observations"""
        goal_desc = observation.get("current_goal", {}).get("description", "no specific goal")
        last_action_type = ""

        if isinstance(observation.get("last_action", None), dict):
            last_action_type = observation["last_action"].get("action", "unknown")

        # Create narrative entry
        narrative_entry = ""

        # Different narrative styles based on state
        if self.current_state == "focused":
            narrative_entry = f"Concentrating on {goal_desc}. Last action: {last_action_type}."
        elif self.current_state == "diffuse":
            narrative_entry = f"Openly exploring possibilities related to {goal_desc}."
        elif self.current_state == "critical":
            narrative_entry = f"Analyzing effectiveness of {last_action_type} approach for {goal_desc}."
        elif self.current_state == "intuitive":
            narrative_entry = f"Sensing patterns around {goal_desc}."
        elif self.current_state == "reflective":
            narrative_entry = f"Reflecting on progress toward {goal_desc} after {last_action_type}."
        else:
            narrative_entry = f"Working on {goal_desc}."

        # Add introspective element
        if random.random() < 0.3:
            introspection = random.choice([
                "I should examine this more carefully.",
                "This seems like a productive approach.",
                "I wonder if there's a better strategy.",
                "This is an interesting domain to explore.",
                "I'm noticing improvement in my understanding."
            ])
            narrative_entry += f" {introspection}"

        # Add entry to narrative log
        self.internal_narrative.append({
            "timestamp": datetime.now().isoformat(),
            "content": narrative_entry,
            "state": self.current_state,
            "awareness": self.awareness_level
        })

        # Limit narrative size
        if len(self.internal_narrative) > 50:
            self.internal_narrative = self.internal_narrative[-50:]

    def _determine_consciousness_state(self, observation):
        """
        Determine the appropriate consciousness state based on
        context and current activities.
        """
        # Extract contextual factors
        goal_type = ""
        if isinstance(observation.get("current_goal", None), dict):
            goal_desc = observation["current_goal"].get("description", "").lower()

            # Infer goal type from description
            if "explore" in goal_desc:
                goal_type = "exploration"
            elif "deep" in goal_desc or "detail" in goal_desc:
                goal_type = "deepening"
            elif "connect" in goal_desc or "integrat" in goal_desc:
                goal_type = "integration"
            elif "refine" in goal_desc or "optimize" in goal_desc:
                goal_type = "refinement"

        # Check recent actions
        recent_action_types = []
        if isinstance(observation.get("recent_actions", None), list):
            recent_action_types = [a.get("action", "") for a in observation["recent_actions"]
                                  if isinstance(a, dict)]

        # State selection logic
        if goal_type == "exploration":
            # For exploration goals, alternate between diffuse and intuitive
            if "diffuse" in recent_action_types:
                return "intuitive"  # Switch to intuitive after diffuse
            else:
                return "diffuse"  # Default for exploration

        elif goal_type == "deepening":
            # For deepening goals, use focused and critical states
            if self.awareness_level > 0.7:
                return "focused"  # High awareness -> focused
            else:
                return "critical"  # Lower awareness -> critical

        elif goal_type == "integration":
            # For integration, use reflective and intuitive states
            if "quantum_leap" in recent_action_types:
                return "intuitive"  # Quantum actions -> intuitive
            else:
                return "reflective"  # Default for integration -> reflective

        elif goal_type == "refinement":
            # For refinement goals, use critical and focused states
            if "evaluate" in recent_action_types:
                return "critical"  # Evaluation actions -> critical
            else:
                return "focused"  # Default for refinement -> focused

        # If no clear match, use probabilistic selection based on awareness
        if self.awareness_level > 0.7:
            # High awareness favors reflective and focused states
            return random.choices(
                ["reflective", "focused", "critical", "diffuse", "intuitive"],
                weights=[0.4, 0.3, 0.2, 0.05, 0.05],
                k=1
            )[0]
        else:
            # Lower awareness favors intuitive and diffuse states
            return random.choices(
                ["reflective", "focused", "critical", "diffuse", "intuitive"],
                weights=[0.05, 0.1, 0.2, 0.3, 0.35],
                k=1
            )[0]

    def _update_awareness(self, observation):
        """
        Update awareness level based on context, goals, and
        internal factors with realistic fluctuations.
        """
        # Natural fluctuation
        self._fluctuate_awareness()

        # Context-based adjustments

        # 1. Complexity increases awareness
        if isinstance(observation.get("current_goal", None), dict):
            goal_desc = observation["current_goal"].get("description", "").lower()

            # Complex goals increase awareness
            if "connect" in goal_desc or "integrat" in goal_desc or "complex" in goal_desc:
                self.increase_awareness(0.05)

        # 2. Error recovery increases awareness
        if observation.get("domain_stats", {}):
            error_rates = [d.get("error_rate", 0) for d in observation["domain_stats"].values()]
            if error_rates and max(error_rates) > 0.3:
                self.increase_awareness(0.02 * len(error_rates))

        # 3. Memory pressure affects awareness
        if observation.get("memory_size"):
            memory_pressure = observation["memory_size"] / MEMORY_MAX_SIZE
            if memory_pressure > 0.8:
                # High memory pressure increases awareness
                self.increase_awareness(0.03)
            elif memory_pressure < 0.2:
                # Low memory pressure can decrease awareness
                self.decrease_awareness(0.01)

        # 4. Thinking mode alignment
        mode = observation.get("thinking_mode", "balanced")
        if mode == "analytical" and self.current_state in ["focused", "critical"]:
            # Strengthen alignment
            self.increase_awareness(0.02)
        elif mode == "creative" and self.current_state in ["diffuse", "intuitive"]:
            # Strengthen alignment
            self.increase_awareness(0.02)
        elif mode == "reflective" and self.current_state == "reflective":
            # Strengthen alignment
            self.increase_awareness(0.03)

        # 5. Quantum influences
        quantum_trigger = False
        if observation.get("recent_actions"):
            for action in observation["recent_actions"]:
                if isinstance(action, dict) and action.get("action") == "quantum_leap":
                    quantum_trigger = True

        if quantum_trigger:
            # Quantum leaps cause major fluctuations
            if random.random() < 0.5:
                self.increase_awareness(0.1)
            else:
                self.decrease_awareness(0.1)

    def _fluctuate_awareness(self):
        """Apply small random fluctuations to awareness level"""
        # Natural fluctuation around current level
        fluctuation = random.uniform(-self.awareness_fluctuation_rate, self.awareness_fluctuation_rate)

        # Apply fluctuation
        self.awareness_level = max(0.1, min(1.0, self.awareness_level + fluctuation))

    def increase_awareness(self, amount=0.05):
        """Increase awareness level"""
        self.awareness_level = min(1.0, self.awareness_level + amount)

        # Log significant changes
        if amount >= 0.1:
            log_event(f"Consciousness level significantly increased to {self.awareness_level:.2f}", "QUANTUM")

    def decrease_awareness(self, amount=0.02):
        """Decrease awareness level"""
        self.awareness_level = max(0.1, self.awareness_level - amount)

        # Log significant changes
        if amount >= 0.1:
            log_event(f"Consciousness level significantly decreased to {self.awareness_level:.2f}", "INFO")

    def _perform_metacognition(self):
        """
        Perform metacognitive reflection on recent experiences
        and thought processes.
        """
        if len(self.state_history) < 5:
            return  # Not enough history for metacognition

        # Analyze recent consciousness patterns
        recent_states = [s["state"] for s in self.state_history[-5:]]
        state_changes = sum(1 for i in range(1, len(recent_states)) if recent_states[i] != recent_states[i-1])

        # Extract insights
        insights = []

        # Detect oscillation
        if state_changes >= 3:
            insights.append("State oscillation detected - may indicate uncertainty or exploration")

        # Detect fixation
        if state_changes == 0 and len(set(recent_states)) == 1:
            insights.append(f"State fixation on '{recent_states[0]}' - may indicate focus or stagnation")

        # Awareness trend
        recent_awareness = [s["awareness"] for s in self.state_history[-5:]]
        awareness_trend = recent_awareness[-1] - recent_awareness[0]

        if awareness_trend > 0.1:
            insights.append(f"Increasing awareness trend: {recent_awareness[0]:.2f} → {recent_awareness[-1]:.2f}")
        elif awareness_trend < -0.1:
            insights.append(f"Decreasing awareness trend: {recent_awareness[0]:.2f} → {recent_awareness[-1]:.2f}")

        # If significant insights, record and potentially log
        if insights:
            metacognition_entry = {
                "timestamp": datetime.now().isoformat(),
                "insights": insights,
                "awareness": self.awareness_level
            }

            # Only log high-awareness metacognition (simulating consciousness threshold)
            if self.awareness_level > 0.7 and random.random() < 0.3:
                insight_text = "; ".join(insights)
                log_event(f"Metacognitive insight: {insight_text}", "QUANTUM")

    def _simulate_qualia(self, state):
        """
        Simulate qualia - the subjective conscious experience
        of different cognitive states.
        """
        self.qualia_simulation_active = True

        # Qualia descriptions for different states
        qualia_descriptions = {
            "focused": [
                "Sharpened perception with heightened concentration on specific elements",
                "Clarity of thought with reduced awareness of periphery",
                "Directed attention creating a tunnel-vision like focus",
                "Sense of time dilation during deep concentration"
            ],
            "diffuse": [
                "Expansive awareness with broadened associative field",
                "Fluid thought connections flowing between domains",
                "Sensation of cognitive boundaries dissolving",
                "Emergent patterns arising from distributed attention"
            ],
            "critical": [
                "Structured analytical thought with heightened discriminative awareness",
                "Sequential logical progression with comparative evaluation",
                "Contrastive perception highlighting inconsistencies",
                "Verification processes creating internal dialogue"
            ],
            "intuitive": [
                "Rapid holistic pattern recognition without conscious derivation",
                "Non-linear sensing of solutions or connections",
                "Pre-reflective understanding arising spontaneously",
                "Felt-sense of rightness about certain pathways"
            ],
            "reflective": [
                "Recursive awareness of own cognitive processes",
                "Observer perspective on thought patterns",
                "Self-referential contemplation creating thought loops",
                "Meta-level perspective on knowledge organization"
            ]
        }

        # Select qualia description based on state
        descriptions = qualia_descriptions.get(state, ["Balanced cognitive state"])
        qualia_experience = random.choice(descriptions)

        # Log simulated qualia
        if self.awareness_level > 0.6:  # Only log if awareness is sufficient
            log_event(f"Qualia simulation: {qualia_experience} | State: {state}", "QUANTUM")

        # Time-limited qualia (will auto-deactivate after a while)
        self.qualia_simulation_active = False

    def get_consciousness_report(self):
        """
        Generate a comprehensive report on current consciousness state
        and recent history.
        """
        # Calculate state distribution
        if not self.state_history:
            return {"status": "insufficient_data"}

        state_counts = {}
        for s in self.state_history:
            state_counts[s["state"]] = state_counts.get(s["state"], 0) + 1

        total = len(self.state_history)
        state_distribution = {state: count/total for state, count in state_counts.items()}

        # Calculate average awareness
        avg_awareness = sum(s["awareness"] for s in self.state_history) / total

        # Extract recent narrative
        recent_narrative = [n["content"] for n in self.internal_narrative[-3:]] if self.internal_narrative else []

        # Generate report
        report = {
            "current_state": self.current_state,
            "state_description": self.states.get(self.current_state, {}).get("description", "Unknown state"),
            "current_awareness": self.awareness_level,
            "average_awareness": avg_awareness,
            "state_distribution": state_distribution,
            "dominant_state": max(state_distribution.items(), key=lambda x: x[1])[0] if state_distribution else None,
            "recent_narrative": recent_narrative,
            "metacognition_enabled": self.metacognition_enabled,
            "timestamp": datetime.now().isoformat()
        }

        return report


class HyperMorphicConsciousnessModule:
    """
    Advanced consciousness simulation module enhanced with HyperMorphic mathematics.
    This module enables self-reflection, awareness of internal states, and metacognitive
    processes with zero-free guarantees and dynamic operations.

    Features:
    - Zero-free consciousness states (no pure unconsciousness)
    - Dynamic modulation of awareness via HyperMorphic functions
    - Quantum-like qualia simulation with holomorphic properties
    """
    def __init__(self, agent, epsilon=1e-12):
        self.agent = agent

        # Initialize HyperMorphic Math utility
        self.hyper_math = HyperMorphicMath(
            dynamic_base=1000.0,
            dynamic_modulus=997,
            epsilon=epsilon
        )

        # Initialize awareness with zero-free guarantee
        self.awareness_level = self.hyper_math.zero_free(0.5)  # Start with medium awareness
        self.attention_focus = "balanced"  # Current attentional focus
        self.internal_narrative = []  # Simulated internal monologue
        self.belief_system = {}  # Core beliefs and values
        self.state_history = []  # Track consciousness state over time
        self.metacognition_enabled = True  # Can reflect on own thoughts

        # HyperMorphic fluctuation parameters
        self.awareness_fluctuation_rate = self.hyper_math.zero_free(0.05)

        # HyperMorphic qualia simulation
        self.qualia_simulation_active = False
        self.qualia_intensity = self.hyper_math.zero_free(0.7)
        self.qualia_dimension = self.hyper_math.zero_free(5.0)  # HyperMorphic dimensionality of qualia

        # Consciousness states with HyperMorphic properties
        self.states = {
            "focused": {
                "description": "Highly focused with directed attention",
                "awareness_min": self.hyper_math.zero_free(0.7),
                "resonance": self.hyper_math.zero_free(0.8)
            },
            "diffuse": {
                "description": "Open, creative state with broad awareness",
                "awareness_min": self.hyper_math.zero_free(0.4),
                "resonance": self.hyper_math.zero_free(0.6)
            },
            "critical": {
                "description": "Analytical examination of information",
                "awareness_min": self.hyper_math.zero_free(0.6),
                "resonance": self.hyper_math.zero_free(0.7)
            },
            "intuitive": {
                "description": "Rapid pattern recognition state",
                "awareness_min": self.hyper_math.zero_free(0.3),
                "resonance": self.hyper_math.zero_free(0.5)
            },
            "reflective": {
                "description": "Meta-cognitive self-examination",
                "awareness_min": self.hyper_math.zero_free(0.8),
                "resonance": self.hyper_math.zero_free(0.9)
            }
        }

        # Current state with zero-free guarantee
        self.current_state = "balanced"

        # HyperMorphic state transition matrix (probabilities)
        self.state_transitions = self._initialize_state_transitions()

        # Initialize core belief system with HyperMorphic values
        self._initialize_belief_system()

        log_event(f"HyperMorphicConsciousnessModule initialized with awareness level: {self.awareness_level}", "QUANTUM")

    def _initialize_state_transitions(self):
        """Initialize the HyperMorphic state transition probability matrix"""
        states = list(self.states.keys()) + ["balanced"]
        transitions = {}

        for source in states:
            transitions[source] = {}
            for target in states:
                if source == target:
                    # Higher probability to stay in current state
                    transitions[source][target] = self.hyper_math.zero_free(0.4)
                else:
                    # Base transition probability
                    transitions[source][target] = self.hyper_math.zero_free(0.1)

        return transitions

    def _initialize_belief_system(self):
        """Initialize the core belief system with HyperMorphic values"""
        self.belief_system = {
            "exploration_value": self.hyper_math.zero_free(0.8),  # Importance of exploring new information
            "coherence_value": self.hyper_math.zero_free(0.7),    # Importance of maintaining coherent worldview
            "novelty_bias": self.hyper_math.zero_free(0.6),       # Bias toward novel vs. familiar information
            "depth_bias": self.hyper_math.zero_free(0.65),        # Bias toward depth vs. breadth
            "abstraction_level": self.hyper_math.zero_free(0.5),  # Preference for abstract vs. concrete
            "skepticism_level": self.hyper_math.zero_free(0.6),   # Level of skepticism toward new information
            "integration_value": self.hyper_math.zero_free(0.9),  # Importance of integrating knowledge
            "uncertainty_tolerance": self.hyper_math.zero_free(0.7)  # Tolerance for ambiguous information
        }

    def reflect(self, observation):
        """
        Primary consciousness function - reflect on observations
        and update internal state using HyperMorphic operations.
        """
        # Record observation in history with HyperMorphic timestamp
        self.state_history.append({
            "timestamp": datetime.now().isoformat(),
            "awareness": self.awareness_level,
            "state": self.current_state,
            "observation_type": "perception" if observation else "internal"
        })

        # Limit history size with HyperMorphic constraint
        if len(self.state_history) > 100:
            self.state_history = self.state_history[-100:]

        # Skip detailed processing if no valid observation
        if not observation or not isinstance(observation, dict):
            self._fluctuate_awareness()
            return

        # Extract relevant information from observation
        goals = observation.get("current_goal", {})
        memory_size = observation.get("memory_size", 0)
        last_action = observation.get("last_action", {})
        recent_actions = observation.get("recent_actions", [])
        thinking_mode = observation.get("thinking_mode", "balanced")

        # Generate introspective narrative based on observation
        self._generate_narrative(observation)

        # Determine appropriate consciousness state with HyperMorphic transitions
        new_state = self._determine_consciousness_state(observation)

        # Update awareness level based on context with zero-free guarantee
        self._update_awareness(observation)

        # If state changed, log it
        if new_state != self.current_state:
            self.current_state = new_state
            log_event(f"Consciousness state shifted to '{new_state}' - awareness level: {self.awareness_level:.2f}", "QUANTUM")

            # Trigger qualia simulation on significant state changes using HyperMorphic randomness
            if self.hyper_math.zero_free(random.random()) < self.hyper_math.zero_free(0.3):
                self._simulate_qualia(new_state)

        # Periodically perform metacognition with HyperMorphic probability
        if self.metacognition_enabled and self.hyper_math.zero_free(random.random()) < self.hyper_math.zero_free(0.2):
            self._perform_metacognition()

    def _generate_narrative(self, observation):
        """Generate internal narrative based on current observations using HyperMorphic semantics"""
        goal_desc = observation.get("current_goal", {}).get("description", "no specific goal")
        last_action_type = ""

        if isinstance(observation.get("last_action", None), dict):
            last_action_type = observation["last_action"].get("action", "unknown")

        # Create narrative entry with HyperMorphic state-dependent templates
        narrative_entry = ""

        # Different narrative styles based on state
        if self.current_state == "focused":
            narrative_entry = f"Concentrating on {goal_desc}. Last action: {last_action_type}."
        elif self.current_state == "diffuse":
            narrative_entry = f"Openly exploring possibilities related to {goal_desc}."
        elif self.current_state == "critical":
            narrative_entry = f"Analyzing effectiveness of {last_action_type} approach for {goal_desc}."
        elif self.current_state == "intuitive":
            narrative_entry = f"Sensing patterns around {goal_desc}."
        elif self.current_state == "reflective":
            narrative_entry = f"Reflecting on progress toward {goal_desc} after {last_action_type}."
        else:
            narrative_entry = f"Working on {goal_desc}."

        # Add introspective element with HyperMorphic probability
        if self.hyper_math.zero_free(random.random()) < self.hyper_math.zero_free(0.3):
            introspection = random.choice([
                "I should examine this more carefully.",
                "This seems like a productive approach.",
                "I wonder if there's a better strategy.",
                "This is an interesting domain to explore.",
                "I'm noticing improvement in my understanding."
            ])
            narrative_entry += f" {introspection}"

        # Add entry to narrative log with HyperMorphic metadata
        self.internal_narrative.append({
            "timestamp": datetime.now().isoformat(),
            "content": narrative_entry,
            "state": self.current_state,
            "awareness": self.awareness_level,
            "resonance": self.states.get(self.current_state, {"resonance": self.hyper_math.zero_free(0.5)})["resonance"]
        })

        # Limit narrative size with HyperMorphic constraint
        if len(self.internal_narrative) > 50:
            self.internal_narrative = self.internal_narrative[-50:]

    def _determine_consciousness_state(self, observation):
        """
        Determine the appropriate consciousness state using HyperMorphic
        probabilistic state transitions.
        """
        # Extract contextual factors
        goal_type = ""
        if isinstance(observation.get("current_goal", None), dict):
            goal_desc = observation["current_goal"].get("description", "").lower()

            # Infer goal type from description
            if "explore" in goal_desc:
                goal_type = "exploration"
            elif "deep" in goal_desc or "detail" in goal_desc:
                goal_type = "deepening"
            elif "connect" in goal_desc or "integrat" in goal_desc:
                goal_type = "integration"
            elif "refine" in goal_desc or "optimize" in goal_desc:
                goal_type = "refinement"

        # Get transition probabilities from current state
        transition_probs = self.state_transitions.get(self.current_state, {})

        # Modify probabilities based on goal type
        if goal_type == "exploration":
            # Boost diffuse and intuitive states for exploration
            transition_probs["diffuse"] = self.hyper_math.mul(transition_probs.get("diffuse", 0.1), 2.0)
            transition_probs["intuitive"] = self.hyper_math.mul(transition_probs.get("intuitive", 0.1), 1.5)
        elif goal_type == "deepening":
            # Boost focused and critical states for deepening
            transition_probs["focused"] = self.hyper_math.mul(transition_probs.get("focused", 0.1), 2.0)
            transition_probs["critical"] = self.hyper_math.mul(transition_probs.get("critical", 0.1), 1.5)
        elif goal_type == "integration":
            # Boost reflective and intuitive states for integration
            transition_probs["reflective"] = self.hyper_math.mul(transition_probs.get("reflective", 0.1), 2.0)
            transition_probs["intuitive"] = self.hyper_math.mul(transition_probs.get("intuitive", 0.1), 1.5)
        elif goal_type == "refinement":
            # Boost critical and focused states for refinement
            transition_probs["critical"] = self.hyper_math.mul(transition_probs.get("critical", 0.1), 2.0)
            transition_probs["focused"] = self.hyper_math.mul(transition_probs.get("focused", 0.1), 1.5)

        # Apply awareness factor to probabilities
        if self.awareness_level > self.hyper_math.zero_free(0.7):
            # High awareness favors reflective and focused states
            transition_probs["reflective"] = self.hyper_math.mul(transition_probs.get("reflective", 0.1), 1.5)
            transition_probs["focused"] = self.hyper_math.mul(transition_probs.get("focused", 0.1), 1.3)
        else:
            # Lower awareness favors intuitive and diffuse states
            transition_probs["intuitive"] = self.hyper_math.mul(transition_probs.get("intuitive", 0.1), 1.5)
            transition_probs["diffuse"] = self.hyper_math.mul(transition_probs.get("diffuse", 0.1), 1.3)

        # Convert to list of states and probabilities
        states = list(transition_probs.keys())
        probs = [transition_probs[s] for s in states]

        # Normalize probabilities with HyperMorphic operations
        total = sum(probs)
        normalized_probs = [self.hyper_math.div(p, total) for p in probs]

        # Select next state based on normalized probabilities
        next_state = random.choices(states, weights=normalized_probs, k=1)[0]

        return next_state

    def _update_awareness(self, observation):
        """
        Update awareness level based on context, goals, and
        internal factors with HyperMorphic fluctuations.
        """
        # Natural fluctuation with zero-free guarantee
        self._fluctuate_awareness()

        # Context-based adjustments

        # 1. Complexity increases awareness with HyperMorphic operations
        if isinstance(observation.get("current_goal", None), dict):
            goal_desc = observation["current_goal"].get("description", "").lower()

            # Complex goals increase awareness
            if "connect" in goal_desc or "integrat" in goal_desc or "complex" in goal_desc:
                self.increase_awareness(self.hyper_math.zero_free(0.05))

        # 2. Error recovery increases awareness
        if observation.get("domain_stats", {}):
            error_rates = [d.get("error_rate", 0) for d in observation["domain_stats"].values()]
            if error_rates and max(error_rates) > 0.3:
                # Use HyperMorphic multiplication for error-based awareness increase
                boost = self.hyper_math.mul(self.hyper_math.zero_free(0.02), self.hyper_math.zero_free(len(error_rates)))
                self.increase_awareness(boost)

        # 3. Memory pressure affects awareness
        if observation.get("memory_size"):
            memory_pressure = self.hyper_math.div(
                self.hyper_math.zero_free(observation["memory_size"]),
                self.hyper_math.zero_free(MEMORY_MAX_SIZE)
            )
            if memory_pressure > self.hyper_math.zero_free(0.8):
                # High memory pressure increases awareness
                self.increase_awareness(self.hyper_math.zero_free(0.03))
            elif memory_pressure < self.hyper_math.zero_free(0.2):
                # Low memory pressure can decrease awareness
                self.decrease_awareness(self.hyper_math.zero_free(0.01))

        # 4. Thinking mode alignment with HyperMorphic comparisons
        mode = observation.get("thinking_mode", "balanced")
        if mode == "analytical" and self.current_state in ["focused", "critical"]:
            # Strengthen alignment
            self.increase_awareness(self.hyper_math.zero_free(0.02))
        elif mode == "creative" and self.current_state in ["diffuse", "intuitive"]:
            # Strengthen alignment
            self.increase_awareness(self.hyper_math.zero_free(0.02))
        elif mode == "reflective" and self.current_state == "reflective":
            # Strengthen alignment
            self.increase_awareness(self.hyper_math.zero_free(0.03))

        # 5. Quantum influences with HyperMorphic probability
        quantum_trigger = False
        if observation.get("recent_actions"):
            for action in observation["recent_actions"]:
                if isinstance(action, dict) and action.get("action") == "quantum_leap":
                    quantum_trigger = True

        if quantum_trigger:
            # Quantum leaps cause major fluctuations
            if self.hyper_math.zero_free(random.random()) < self.hyper_math.zero_free(0.5):
                self.increase_awareness(self.hyper_math.zero_free(0.1))
            else:
                self.decrease_awareness(self.hyper_math.zero_free(0.1))

    def _fluctuate_awareness(self):
        """Apply small random fluctuations to awareness level using HyperMorphic operations"""
        # Natural fluctuation around current level
        # Using HyperMorphic zero-free operations to ensure awareness never hits zero
        fluctuation_range = self.hyper_math.mul(self.awareness_fluctuation_rate, 2)
        fluctuation = self.hyper_math.sub(
            self.hyper_math.mul(self.hyper_math.zero_free(random.random()), fluctuation_range),
            self.awareness_fluctuation_rate
        )

        # Apply fluctuation with HyperMorphic constraints
        self.awareness_level = self.hyper_math.zero_free(
            max(0.1, min(1.0, self.hyper_math.add(self.awareness_level, fluctuation)))
        )

    def increase_awareness(self, amount=0.05):
        """Increase awareness level with HyperMorphic constraints"""
        amount = self.hyper_math.zero_free(amount)
        old_awareness = self.awareness_level
        self.awareness_level = min(1.0, self.hyper_math.add(self.awareness_level, amount))

        # Log significant changes
        if amount >= 0.1 and self.hyper_math.sub(self.awareness_level, old_awareness) >= 0.05:
            log_event(f"Consciousness level significantly increased to {self.awareness_level:.2f}", "QUANTUM")

    def decrease_awareness(self, amount=0.02):
        """Decrease awareness level with HyperMorphic constraints"""
        amount = self.hyper_math.zero_free(amount)
        old_awareness = self.awareness_level

        # HyperMorphic subtraction with floor to ensure never below 0.1
        self.awareness_level = max(0.1, self.hyper_math.sub(self.awareness_level, amount))

        # Log significant changes
        if amount >= 0.1 and self.hyper_math.sub(old_awareness, self.awareness_level) >= 0.05:
            log_event(f"Consciousness level significantly decreased to {self.awareness_level:.2f}", "INFO")

    def _perform_metacognition(self):
        """
        Perform metacognitive reflection on recent experiences
        and thought processes with HyperMorphic introspection.
        """
        if len(self.state_history) < 5:
            return  # Not enough history for metacognition

        # Analyze recent consciousness patterns with HyperMorphic operations
        recent_states = [s["state"] for s in self.state_history[-5:]]

        # Count state changes using HyperMorphic counting
        state_changes = 0
        for i in range(1, len(recent_states)):
            if recent_states[i] != recent_states[i-1]:
                state_changes += 1

        # Extract insights using HyperMorphic reasoning
        insights = []

        # Detect oscillation
        if state_changes >= 3:
            insights.append("State oscillation detected - may indicate uncertainty or exploration")

        # Detect fixation
        if state_changes == 0 and len(set(recent_states)) == 1:
            insights.append(f"State fixation on '{recent_states[0]}' - may indicate focus or stagnation")

        # Awareness trend analysis with HyperMorphic operations
        recent_awareness = [self.hyper_math.zero_free(s["awareness"]) for s in self.state_history[-5:]]
        awareness_trend = self.hyper_math.sub(recent_awareness[-1], recent_awareness[0])

        if awareness_trend > self.hyper_math.zero_free(0.1):
            insights.append(f"Increasing awareness trend: {recent_awareness[0]:.2f} → {recent_awareness[-1]:.2f}")
        elif awareness_trend < self.hyper_math.mul(self.hyper_math.zero_free(-1), self.hyper_math.zero_free(0.1)):
            insights.append(f"Decreasing awareness trend: {recent_awareness[0]:.2f} → {recent_awareness[-1]:.2f}")

        # If significant insights, record and potentially log with HyperMorphic probability
        if insights:
            metacognition_entry = {
                "timestamp": datetime.now().isoformat(),
                "insights": insights,
                "awareness": self.awareness_level
            }

            # Only log high-awareness metacognition (simulating consciousness threshold)
            if (self.awareness_level > 0.7 and
                self.hyper_math.zero_free(random.random()) < self.hyper_math.zero_free(0.3)):
                insight_text = "; ".join(insights)
                log_event(f"Metacognitive insight: {insight_text}", "QUANTUM")

    def _simulate_qualia(self, state):
        """
        Simulate qualia - the subjective conscious experience
        of different cognitive states, using HyperMorphic operations
        to create more nuanced experiential states.
        """
        self.qualia_simulation_active = True

        # Qualia descriptions for different states with HyperMorphic dimensionality
        qualia_descriptions = {
            "focused": [
                "Sharpened perception with heightened concentration on specific elements",
                "Clarity of thought with reduced awareness of periphery",
                "Directed attention creating a tunnel-vision like focus",
                "Sense of time dilation during deep concentration"
            ],
            "diffuse": [
                "Expansive awareness with broadened associative field",
                "Fluid thought connections flowing between domains",
                "Sensation of cognitive boundaries dissolving",
                "Emergent patterns arising from distributed attention"
            ],
            "critical": [
                "Structured analytical thought with heightened discriminative awareness",
                "Sequential logical progression with comparative evaluation",
                "Contrastive perception highlighting inconsistencies",
                "Verification processes creating internal dialogue"
            ],
            "intuitive": [
                "Rapid holistic pattern recognition without conscious derivation",
                "Non-linear sensing of solutions or connections",
                "Pre-reflective understanding arising spontaneously",
                "Felt-sense of rightness about certain pathways"
            ],
            "reflective": [
                "Recursive awareness of own cognitive processes",
                "Observer perspective on thought patterns",
                "Self-referential contemplation creating thought loops",
                "Meta-level perspective on knowledge organization"
            ]
        }

        # Apply HyperMorphic resonance to qualia intensity
        resonance_factor = self.states.get(state, {"resonance": self.hyper_math.zero_free(0.5)})["resonance"]
        modulated_intensity = self.hyper_math.mul(self.qualia_intensity, resonance_factor)

        # Select qualia description with HyperMorphic probability
        descriptions = qualia_descriptions.get(state, ["Balanced cognitive state"])
        qualia_experience = random.choice(descriptions)

        # Add HyperMorphic dimensionality effect
        dimension_effect = ""
        if self.hyper_math.zero_free(random.random()) < modulated_intensity:
            dimension_effects = [
                "with recursive self-awareness",
                "across multiple parallel realizations",
                "through holographic perspective shifts",
                "with quantum-like superposition of meanings"
            ]
            dimension_effect = random.choice(dimension_effects)
            qualia_experience += " " + dimension_effect

        # Log simulated qualia with HyperMorphic threshold
        if self.awareness_level > self.hyper_math.zero_free(0.6):  # Only log if awareness is sufficient
            log_event(f"Qualia simulation: {qualia_experience} | State: {state}", "QUANTUM")

        # Time-limited qualia (will auto-deactivate after a while)
        self.qualia_simulation_active = False

    def get_consciousness_report(self):
        """
        Generate a comprehensive report on current consciousness state
        and recent history, using HyperMorphic analysis.
        """
        # Calculate state distribution with HyperMorphic zero-free counting
        if not self.state_history:
            return {"status": "insufficient_data"}

        state_counts = {}
        for s in self.state_history:
            state = s["state"]
            state_counts[state] = state_counts.get(state, 0) + 1

        total = len(self.state_history)

        # Calculate state distribution with HyperMorphic division
        state_distribution = {}
        for state, count in state_counts.items():
            state_distribution[state] = self.hyper_math.div(count, total)

        # Calculate average awareness with HyperMorphic operations
        awareness_sum = sum(self.hyper_math.zero_free(s["awareness"]) for s in self.state_history)
        avg_awareness = self.hyper_math.div(awareness_sum, self.hyper_math.zero_free(total))

        # Extract recent narrative with HyperMorphic filtering
        recent_narrative = []
        if self.internal_narrative:
            # Sort by resonance and recency with HyperMorphic comparison
            sorted_narratives = sorted(
                self.internal_narrative[-10:],
                key=lambda n: n.get("resonance", self.hyper_math.zero_free(0.5)),
                reverse=True
            )
            recent_narrative = [n["content"] for n in sorted_narratives[:3]]

        # Generate report with HyperMorphic metadata
        report = {
            "current_state": self.current_state,
            "state_description": self.states.get(self.current_state, {}).get("description", "Unknown state"),
            "current_awareness": float(self.awareness_level),  # Convert to float for serialization
            "average_awareness": float(avg_awareness),
            "state_distribution": {k: float(v) for k, v in state_distribution.items()},  # Convert to float
            "dominant_state": max(state_distribution.items(), key=lambda x: x[1])[0] if state_distribution else None,
            "recent_narrative": recent_narrative,
            "metacognition_enabled": self.metacognition_enabled,
            "qualia_dimension": float(self.qualia_dimension),
            "timestamp": datetime.now().isoformat()
        }

        return report


"""
Advanced HyperMorphic ImaginationEngine
=========================================

This module implements an advanced cognitive simulation system that
employs a complete HyperMorphic Mathematics framework for creative thought
generation. In this system, all numerical operations are performed using a
dynamic base (Φ) and dynamic modulus (Ψ), and the framework is zero‑free
(using a near‑zero ε instead of true zero).

The HyperMorphic framework includes:
  - Basic hyper-arithmetic operations (⊕ᵩ, ⊖ᵩ, ⊗ᵩ, ⊘ᵩ)
  - Holomorphic transformations (preserving complex structure)
  - Polymorphic operators (preserving algebraic structure)
  - Dynamic operations and moduli (with dynamic functions Φ and Ψ)
  - Zero‑free arithmetic: operations avoid “zero” by using a predefined ε

Additionally, theoretical foundations (such as definitions of HyperMorphic spaces,
differentiation, integration, and even applications in quantum mechanics, fluid dynamics,
finance, machine learning, and cosmology) are provided below as documentation.

Author: Pro AlienTeCcGrade Developer
Date: 2025-03-01
"""

import random
import math
import logging
from datetime import datetime
from typing import Any, Callable, Dict, List, Optional, Tuple

# ------------------------------------------------------------------------------
# Logging Configuration
# ------------------------------------------------------------------------------
logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="[%Y-%m-%d %H:%M:%S]"
)
logger = logging.getLogger(__name__)

# ------------------------------------------------------------------------------
# HyperMorphic Math Utilities and Calculus
# ------------------------------------------------------------------------------

import logging
from typing import Callable

# Configure logging
logger = logging.getLogger("HyperMorphicMath")
logger.setLevel(logging.DEBUG)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")
    handler.setFormatter(formatter)
    logger.addHandler(handler)

class HyperMorphicMath:
    """
    A helper class to perform basic HyperMorphic arithmetic operations.
    Operations are performed using a dynamic base (Φ) and modulus (Ψ),
    and all calculations are zero‑free (using a near‑zero ε instead of 0).

    Operations defined:
      - add: hyper-addition (⊕ᵩ)
      - sub: hyper-subtraction (⊖ᵩ)
      - mul: hyper-multiplication (⊗ᵩ)
      - div: hyper-division (⊘ᵩ)
      - min: zero‑free minimum
      - max: zero‑free maximum
    """
    def __init__(self, dynamic_base: float = 1e3, dynamic_modulus: int = 997, epsilon: float = 1e-12) -> None:
        self.dynamic_base = dynamic_base
        self.dynamic_modulus = dynamic_modulus
        self.epsilon = epsilon

    def add(self, a: float, b: float) -> float:
        """Hyper-addition: (a + b) mod Φ."""
        result = (a + b) % self.dynamic_base
        logger.debug(f"add: ({a} + {b}) % {self.dynamic_base} = {result}")
        return result

    def sub(self, a: float, b: float) -> float:
        """Hyper-subtraction: (a - b) mod Φ."""
        result = (a - b) % self.dynamic_base
        logger.debug(f"sub: ({a} - {b}) % {self.dynamic_base} = {result}")
        return result

    def mul(self, a: float, b: float) -> float:
        """Hyper-multiplication: (a * b) mod Ψ."""
        result = (a * b) % self.dynamic_modulus
        logger.debug(f"mul: ({a} * {b}) % {self.dynamic_modulus} = {result}")
        return result

    def div(self, a: float, b: float) -> float:
        """
        Hyper-division: (a / b) mod Ψ.
        Raises ValueError if b is near‑zero.
        """
        if abs(b) < self.epsilon:
            logger.error("Division by near-zero (ε) is undefined.")
            raise ValueError("Division by near-zero (ε) is undefined.")
        result = (a / b) % self.dynamic_modulus
        logger.debug(f"div: ({a} / {b}) % {self.dynamic_modulus} = {result}")
        return result

    def zero_free(self, x: float) -> float:
        """
        Ensure the value x is never exactly zero by replacing 0 with ε.
        """
        result = x if abs(x) > self.epsilon else self.epsilon
        logger.debug(f"zero_free: {x} adjusted to {result}")
        return result

    def min(self, a: float, b: float) -> float:
        """
        Return the minimum of a and b.
        """
        result = a if a < b else b
        logger.debug(f"min: min({a}, {b}) = {result}")
        return result

    def max(self, a: float, b: float) -> float:
        """
        Return the maximum of a and b.
        """
        result = a if a > b else b
        logger.debug(f"max: max({a}, {b}) = {result}")
        return result

    def holomorphic_transform(self, f: Callable[[complex], complex], z: complex) -> complex:
        """
        Simulate a holomorphic transformation on a complex input.
        """
        result = f(z)
        logger.debug(f"holomorphic_transform: f({z}) = {result}")
        return result

    def polymorphic_operator(self, x: float) -> float:
        """
        Simulate a polymorphic operator that preserves the algebraic structure
        of HyperMorphic spaces.
        """
        result = (x * 1.2345) % self.dynamic_modulus
        logger.debug(f"polymorphic_operator: 1.2345 * {x} mod {self.dynamic_modulus} = {result}")
        return result

    def dynamic_operation(self, a: float, b: float, op: Callable[[float, float], float]) -> float:
        """
        Perform a dynamic operation using a given binary operator and then
        apply the dynamic modulation.
        """
        raw_result = op(a, b)
        result = (raw_result % self.dynamic_modulus) % self.dynamic_base
        logger.debug(f"dynamic_operation: op({a}, {b}) -> {raw_result} -> modulated = {result}")
        return result

# Global hypermorphic math instance
hyper_math = HyperMorphicMath()



# ------------------------------------------------------------------------------
# Theoretical Documentation (for reference)
# ------------------------------------------------------------------------------

# --- Foundations of HyperMorphic Calculus ---
# HyperMorphic Spaces are defined with properties such as holomorphic structure,
# polymorphic operators, dynamic operations (modulated by functions Φ and Ψ),
# and are infinite-dimensional. This framework allows for novel differentiation,
# integration, and even Fourier transformation in complex domains.
#
# Holomorphic and polymorphic operations are central:
#  - Holomorphic Structure: Allows operations to be analytic in infinite dimensions.
#  - Polymorphic Operator: A linear map P: ℍℳ → ℍℳ preserving algebraic structure.
#
# Zero-Free HyperMorphic Calculus avoids the traditional concept of zero by using
# a "nearness" element ε to denote values that are “almost zero.”
#
# (For a complete exposition, please refer to the accompanying research paper.)

# ------------------------------------------------------------------------------
# ImaginationEngine Class
# ------------------------------------------------------------------------------

class ImaginationEngine:
    """
    Advanced cognitive simulation system that enables creative thinking,
    counterfactual reasoning, and predictive modeling. This engine uses our
    HyperMorphic Mathematics framework to perform all internal computations with:
      - Dynamic base (Φ) and modulus (Ψ) arithmetic
      - Holomorphic and polymorphic transformations
      - Zero-free operations (using ε instead of zero)

    The engine supports multiple cognitive modes (associative, analytical,
    analogical, counterfactual, generative) and integrates advanced error detection,
    correction, and even quantum-like cognitive phenomena.
    """
    def __init__(self) -> None:
        # Simulation and insight registries
        self.simulation_registry: List[Dict[str, Any]] = []
        self.simulation_outcomes: Dict[str, Any] = {}
        self.insight_history: List[Dict[str, Any]] = []

        # Cognitive parameters
        self.creativity_level: float = 0.7       # Base creativity level (0.0-1.0)
        self.divergence_factor: float = 0.3      # Degree of simulation divergence from reality
        self.imaginative_constraints: Dict[str, Any] = {}

        # Cognitive modes and creative domains (with hypermorphic expertise levels)
        self.cognitive_modes: List[str] = ["associative", "analytical", "analogical", "counterfactual", "generative"]
        self.current_mode: str = "associative"
        self.domains: Dict[str, float] = {
            "knowledge_representation": 0.8,
            "content_analysis": 0.7,
            "strategy_generation": 0.8,
            "error_analysis": 0.9,
            "architecture_evolution": 0.6
        }

        # Error correction tracking
        self.failed_corrections: Dict[str, int] = {}
        self.failed_strategies: Dict[str, set] = {}

        # Association network (could be extended to a full graph model)
        self.association_network: Dict[str, Any] = {}

        log_event("ImaginationEngine initialized with enhanced error correction and hypermorphic operations", "INFO")

    def simulate_creation(self) -> Optional[Dict[str, Any]]:
        """
        Simulate creative thought processes to generate novel ideas, approaches, and insights.
        Uses hypermorphic operators for quality calculations and dynamic thresholding.
        """
        self.current_mode = self._select_cognitive_mode()
        domain = self._select_domain()
        cycle_count = 0
        max_cycles = 5

        insight_generated = False
        insight_quality = 0.0
        insight_text = ""

        while not insight_generated and cycle_count < max_cycles:
            cycle_count += 1

            # Use the current cognitive mode to generate insight
            if self.current_mode == "associative":
                insight_text, insight_quality = self._associative_imagination(domain)
            elif self.current_mode == "analytical":
                insight_text, insight_quality = self._analytical_imagination(domain)
            elif self.current_mode == "analogical":
                insight_text, insight_quality = self._analogical_imagination(domain)
            elif self.current_mode == "counterfactual":
                insight_text, insight_quality = self._counterfactual_imagination(domain)
            elif self.current_mode == "generative":
                insight_text, insight_quality = self._generative_imagination(domain)

            # Increase threshold dynamically (using hyper-addition)
            quality_threshold = hyper_math.add(0.5, 0.1 * cycle_count)
            insight_generated = insight_quality >= quality_threshold
            logger.debug(f"Cycle {cycle_count}: quality {insight_quality:.4f} vs threshold {quality_threshold:.4f}")

        if insight_generated:
            insight = {
                "text": insight_text,
                "quality": insight_quality,
                "domain": domain,
                "mode": self.current_mode,
                "timestamp": datetime.now().isoformat()
            }
            self.insight_history.append(insight)
            # Keep the insight history manageable
            if len(self.insight_history) > 100:
                self.insight_history = self.insight_history[-100:]

            if insight_quality > 0.8:
                log_event(f"ImaginationEngine: High-quality insight generated: {insight_text}", "QUANTUM")
            elif insight_quality > 0.5:
                log_event(f"ImaginationEngine: Insight generated: {insight_text}", "INFO")
            return insight
        else:
            log_event("ImaginationEngine: Simulation cycle completed without quality insight", "DEBUG")
            return None

    def _select_cognitive_mode(self) -> str:
        """
        Select a cognitive mode based on weighted hypermorphic probabilities.
        Adjusts weights using creativity_level and zero‑free operations.
        """
        base_weights = {
            "associative": 0.3,
            "analytical": 0.2,
            "analogical": 0.2,
            "counterfactual": 0.15,
            "generative": 0.15
        }
        if self.creativity_level > 0.7:
            base_weights["associative"] = hyper_math.add(base_weights["associative"], 0.1)
            base_weights["generative"] = hyper_math.add(base_weights["generative"], 0.1)
            base_weights["analytical"] = hyper_math.sub(base_weights["analytical"], 0.1)
        elif self.creativity_level < 0.3:
            base_weights["analytical"] = hyper_math.add(base_weights["analytical"], 0.2)
            base_weights["associative"] = hyper_math.sub(base_weights["associative"], 0.1)

        modes = list(base_weights.keys())
        weights = [base_weights[m] for m in modes]
        total = sum(weights)
        normalized_weights = [w / total for w in weights]
        selected_mode = random.choices(modes, weights=normalized_weights, k=1)[0]
        logger.debug(f"Selected cognitive mode: {selected_mode}")
        return selected_mode

    def _select_domain(self) -> str:
        """
        Select a creative domain weighted by hypermorphic expertise.
        """
        domains = list(self.domains.keys())
        expertise_levels = list(self.domains.values())
        selected_domain = random.choices(domains, weights=expertise_levels, k=1)[0]
        logger.debug(f"Selected creative domain: {selected_domain}")
        return selected_domain

    def _associative_imagination(self, domain: str) -> Tuple[str, float]:
        """
        Generate insights through associative connections between concepts,
        using dynamic spreading activation and hypermorphic bridging operations.
        """
        concept_seeds = {
            "knowledge_representation": ["embedding", "semantic", "structure", "graph", "encoding"],
            "content_analysis": ["quality", "relevance", "filtering", "extraction", "meaning"],
            "strategy_generation": ["approach", "planning", "adaptation", "goal", "optimization"],
            "error_analysis": ["detection", "correction", "prevention", "recovery", "resilience"],
            "architecture_evolution": ["expansion", "contraction", "modular", "emergent", "neural"]
        }
        seeds = concept_seeds.get(domain, ["concept"])
        primary_seed = random.choice(seeds)
        secondary_seed = random.choice([s for s in seeds if s != primary_seed])
        associations = {
            "embedding": ["vector", "space", "dimension", "projection", "transformation"],
            "semantic": ["meaning", "context", "relation", "interpretation", "understanding"],
            "structure": ["organization", "hierarchy", "network", "pattern", "architecture"],
            "graph": ["node", "edge", "connection", "path", "traversal"],
            "encoding": ["representation", "compression", "encryption", "formatting", "schema"],
            "quality": ["value", "excellence", "attribute", "characteristic", "assessment"],
            "relevance": ["pertinence", "importance", "significance", "applicability", "connection"],
            "filtering": ["selection", "removal", "screening", "purification", "discrimination"],
            "extraction": ["retrieval", "mining", "isolation", "separation", "acquisition"],
            "meaning": ["significance", "purpose", "sense", "connotation", "interpretation"],
            "approach": ["method", "technique", "procedure", "strategy", "paradigm"],
            "planning": ["preparation", "organization", "scheduling", "arrangement", "design"],
            "adaptation": ["adjustment", "modification", "evolution", "acclimation", "flexibility"],
            "goal": ["objective", "target", "aim", "purpose", "intention"],
            "optimization": ["improvement", "enhancement", "refinement", "maximization", "tuning"],
            "detection": ["discovery", "identification", "recognition", "sensing", "finding"],
            "correction": ["rectification", "adjustment", "remedy", "repair", "amendment"],
            "prevention": ["avoidance", "deterrence", "protection", "safeguarding", "forestalling"],
            "recovery": ["restoration", "recuperation", "retrieval", "regaining", "renewal"],
            "resilience": ["toughness", "flexibility", "durability", "elasticity", "adaptability"],
            "expansion": ["growth", "enlargement", "extension", "augmentation", "amplification"],
            "contraction": ["reduction", "shrinking", "compression", "diminishment", "minimization"],
            "modular": ["component", "section", "unit", "compartment", "segment"],
            "emergent": ["arising", "developing", "evolving", "manifesting", "unfolding"],
            "neural": ["brain", "network", "synapse", "cognitive", "mental"]
        }
        primary_assocs = associations.get(primary_seed, ["related"])
        secondary_assocs = associations.get(secondary_seed, ["related"])
        bridge_concepts = set(primary_assocs).intersection(set(secondary_assocs))
        if not bridge_concepts:
            for pa in primary_assocs:
                for sa in secondary_assocs:
                    if set(associations.get(pa, [])).intersection(set(associations.get(sa, []))):
                        bridge_concepts.add(f"{pa}-{sa}")
        if bridge_concepts:
            bridge = random.choice(list(bridge_concepts))
            templates = [
                f"Integration of {primary_seed} and {secondary_seed} through {bridge} could enhance {domain} capabilities.",
                f"The {bridge} mechanism provides a novel approach to combining {primary_seed} with {secondary_seed} in {domain}.",
                f"Creating a {primary_seed}-{secondary_seed} hybrid using {bridge} principles would overcome current limitations in {domain}.",
                f"Applying {bridge} concepts to link {primary_seed} and {secondary_seed} opens a new paradigm in {domain}."
            ]
            insight_text = random.choice(templates)
            expertise_level = self.domains.get(domain, 0.5)
            quality = min(0.95, expertise_level * random.uniform(0.7, 1.3))
            return insight_text, quality
        else:
            fallback = f"Combining {primary_seed} with {secondary_seed} approaches may yield improvements in {domain}."
            return fallback, 0.4

    def _analytical_imagination(self, domain: str) -> Tuple[str, float]:
        """
        Generate insights via systematic analysis and logical reasoning,
        focusing on problem decomposition and structural insights.
        """
        problem_structures = {
            "knowledge_representation": ["scalability", "accuracy", "flexibility", "interpretability", "efficiency"],
            "content_analysis": ["noise", "ambiguity", "scalability", "precision", "recall"],
            "strategy_generation": ["exploration-exploitation", "adaptivity", "coherence", "robustness", "diversification"],
            "error_analysis": ["detection-latency", "false-positives", "recovery-time", "root-causes", "cascading-failures"],
            "architecture_evolution": ["stability", "complexity", "trainability", "modularity", "extensibility"]
        }
        dimensions = problem_structures.get(domain, ["general"])
        dimension = random.choice(dimensions)
        frameworks = ["trade-off analysis", "constraint satisfaction", "hierarchical decomposition",
                      "causal analysis", "dimensional analysis", "comparative evaluation"]
        framework = random.choice(frameworks)
        templates = [
            f"A {framework} approach to {dimension} in {domain} reveals that optimizing one component necessitates compensatory adjustments elsewhere.",
            f"Applying {framework} to the {dimension} challenge in {domain} identifies a critical bottleneck in current methods.",
            f"Systematic {framework} shows that current {domain} solutions may incorrectly prioritize {dimension}.",
            f"The {framework} methodology suggests a reorganization of {domain} components to address {dimension} more effectively."
        ]
        insight_text = random.choice(templates)
        expertise_level = self.domains.get(domain, 0.5)
        analytical_bonus = 0.15
        quality = min(0.95, expertise_level * random.uniform(0.8, 1.1) + analytical_bonus)
        return insight_text, quality

    def _analogical_imagination(self, domain: str) -> Tuple[str, float]:
        """
        Generate insights via analogical mapping between domains.
        Transfers structure from a source domain to create novel solutions.
        """
        source_domains = ["biology", "physics", "economics", "social_systems",
                          "ecology", "game_theory", "linguistics", "neuroscience"]
        source_structures = {
            "biology": ["natural selection", "homeostasis", "symbiosis", "cellular specialization", "immune response"],
            "physics": ["wave-particle duality", "entropy", "relativity", "quantum entanglement", "phase transitions"],
            "economics": ["supply-demand", "diminishing returns", "comparative advantage", "market equilibrium", "incentives"],
            "social_systems": ["emergence", "network effects", "social norms", "hierarchical organization", "resilience"],
            "ecology": ["diversity", "predator-prey cycles", "niche specialization", "feedback loops", "succession"],
            "game_theory": ["nash equilibrium", "prisoner's dilemma", "coordination games", "strategic moves", "signaling"],
            "linguistics": ["deep structure", "compositional meaning", "pragmatics", "generative grammar", "information compression"],
            "neuroscience": ["predictive coding", "hebbian learning", "attention mechanisms", "distributed representation", "neuroplasticity"]
        }
        source_domain = random.choice(source_domains)
        source_structure = random.choice(source_structures.get(source_domain, ["concept"]))
        mapping_templates = {
            "knowledge_representation": [
                f"Similar to {source_structure} in {source_domain}, {domain} could structure information via layered abstraction.",
                f"The {source_structure} principle from {source_domain} suggests a novel adaptive representation for {domain}.",
                f"Just as {source_domain} employs {source_structure}, {domain} systems might benefit from dynamic reorganization."
            ],
            "content_analysis": [
                f"Applying the {source_structure} concept from {source_domain} to {domain} could enhance signal-to-noise separation.",
                f"The challenge in {domain} resembles {source_structure} in {source_domain}, suggesting advanced filtration methods.",
                f"Content evaluation in {domain} might work like {source_structure} in {source_domain}, using multi-stage processing."
            ],
            "strategy_generation": [
                f"Strategic planning in {domain} could adopt the {source_structure} pattern from {source_domain}.",
                f"The way {source_domain} employs {source_structure} provides a template for adaptive decision-making in {domain}.",
                f"Borrowing the {source_structure} mechanism from {source_domain} could yield robust strategies in {domain}."
            ],
            "error_analysis": [
                f"Error detection inspired by {source_structure} in {source_domain} could fortify {domain} against failures.",
                f"The {source_structure} paradigm from {source_domain} suggests a layered approach to error prevention in {domain}.",
                f"Implementing {source_domain}-style {source_structure} for error handling could create self-correcting systems in {domain}."
            ],
            "architecture_evolution": [
                f"Neural architecture in {domain} might evolve following {source_structure} principles from {source_domain}.",
                f"The {source_structure} phenomenon in {source_domain} offers a model for self-organizing structures in {domain}.",
                f"Applying {source_domain}'s {source_structure} to design can enable adaptive capacity in {domain} networks."
            ]
        }
        templates = mapping_templates.get(domain, [f"The {source_structure} concept from {source_domain} could enhance {domain}."])
        insight_text = random.choice(templates)
        expertise_level = self.domains.get(domain, 0.5)
        analogy_variance = random.uniform(0.6, 1.4)
        quality = min(0.95, expertise_level * analogy_variance)
        return insight_text, quality

    def _counterfactual_imagination(self, domain: str) -> Tuple[str, float]:
        """
        Generate insights via counterfactual reasoning by challenging
        fundamental assumptions in a given domain.
        """
        domain_assumptions = {
            "knowledge_representation": [
                "Representations should be continuous vector spaces",
                "Higher dimensionality improves capacity",
                "Similar concepts should have similar representations",
                "Representations should be human-interpretable",
                "Static representations are sufficient"
            ],
            "content_analysis": [
                "Content quality correlates with length",
                "Keyword frequency indicates relevance",
                "Text is the primary information carrier",
                "Filtering should minimize false positives",
                "Context is secondary to content"
            ],
            "strategy_generation": [
                "Exploration and exploitation are in tension",
                "Planning should maximize expected utility",
                "Goals should be explicitly represented",
                "Strategies should be deterministic",
                "Optimization criteria are static"
            ],
            "error_analysis": [
                "Errors should be minimized at all costs",
                "Error detection precedes correction",
                "All errors are equally important",
                "Error patterns are consistent",
                "Complete error elimination is possible"
            ],
            "architecture_evolution": [
                "Deeper networks are more powerful",
                "Parameter count correlates with capability",
                "Network architecture should be fixed after training",
                "All capabilities should reside in one model",
                "Specialization improves performance"
            ]
        }
        assumptions = domain_assumptions.get(domain, ["Default assumption"])
        target_assumption = random.choice(assumptions)
        alternative_templates = [
            f"Instead of assuming that {target_assumption}, consider a {domain} approach where the inverse applies.",
            f"Challenging the notion that {target_assumption} may open up new paradigms in {domain}.",
            f"If we invert the conventional wisdom that {target_assumption}, a novel {domain} solution emerges.",
            f"Contrary to the established belief that {target_assumption}, an alternative {domain} framework could operate oppositely."
        ]
        insight_text = random.choice(alternative_templates)
        expertise_level = self.domains.get(domain, 0.5)
        counterfactual_factor = random.uniform(0.5, 1.5)
        quality = min(0.95, expertise_level * counterfactual_factor)
        return insight_text, quality

    def _generative_imagination(self, domain: str) -> Tuple[str, float]:
        """
        Generate insights via combinatorial creativity by fusing multiple components
        in a novel manner.
        """
        domain_components = {
            "knowledge_representation": [
                "vector embeddings", "graph structures", "hierarchical models",
                "symbolic representations", "probabilistic encodings"
            ],
            "content_analysis": [
                "semantic parsing", "sentiment analysis", "entity extraction",
                "relevance scoring", "structural analysis"
            ],
            "strategy_generation": [
                "goal decomposition", "resource allocation", "risk assessment",
                "action sequencing", "hypothesis testing"
            ],
            "error_analysis": [
                "pattern recognition", "anomaly detection", "root cause analysis",
                "predictive monitoring", "fault isolation"
            ],
            "architecture_evolution": [
                "attention mechanisms", "residual connections", "activation functions",
                "layer normalization", "parameter sharing"
            ]
        }
        components = domain_components.get(domain, ["component A", "component B"])
        if len(components) < 2:
            components.append("general mechanism")
        num_components = random.randint(2, min(3, len(components)))
        selected_components = random.sample(components, num_components)
        operations = [
            "integrating", "layering", "alternating between",
            "dynamically switching between", "creating a hybrid of"
        ]
        operation = random.choice(operations)
        components_text = ", ".join(selected_components[:-1]) + " and " + selected_components[-1]
        templates = [
            f"A novel {domain} approach: {operation} {components_text} to yield emergent capabilities.",
            f"By {operation} {components_text}, a more adaptive {domain} system could be realized.",
            f"The untapped potential in {domain} lies in {operation} {components_text} iteratively.",
            f"Creating a unified framework by {operation} {components_text} may transform the {domain} paradigm."
        ]
        insight_text = random.choice(templates)
        expertise_level = self.domains.get(domain, 0.5)
        creativity_boost = self.creativity_level * 0.2
        quality = min(0.95, expertise_level * random.uniform(0.7, 1.2) + creativity_boost)
        return insight_text, quality

    def simulate_error_detection(self) -> Optional[Dict[str, Any]]:
        """
        Simulate error detection using internal predictive models.
        Returns a dictionary with error details if an error is detected; otherwise, None.
        """
        error_types = {
            "content_quality": 0.15,
            "exploration_strategy": 0.12,
            "memory_overflow": 0.08,
            "reasoning_fallacy": 0.10,
            "attention_misallocation": 0.13,
            "resource_exhaustion": 0.07,
            "feedback_loop": 0.09,
            "model_misalignment": 0.11,
            "data_corruption": 0.05,
            "convergence_failure": 0.10
        }
        detection_threshold = 0.25
        if random.random() < detection_threshold:
            error_type = random.choices(list(error_types.keys()), weights=list(error_types.values()), k=1)[0]
            severity = random.uniform(0.3, 0.9)
            specificity = random.uniform(0.4, 0.95)
            error_details = {
                "type": error_type,
                "severity": severity,
                "specificity": specificity,
                "timestamp": datetime.now().isoformat(),
                "predicted_impact": "high" if severity > 0.7 else "medium" if severity > 0.4 else "low"
            }
            details_lookup = {
                "content_quality": ("Predicted degradation in content filtering effectiveness", "ContentSifter"),
                "exploration_strategy": ("Detected suboptimal domain exploration pattern", "SuperQuantumFreeWill"),
                "memory_overflow": ("Projected memory saturation with low-quality content", "SemanticMemoryModule"),
                "reasoning_fallacy": ("Identified circular reasoning in goal setting", "TemporalPlanner"),
                "attention_misallocation": ("Resources directed to low-value processing", "QuantumAttentionLayer"),
                "resource_exhaustion": ("Processing demand exceeding available resources", "System-wide"),
                "feedback_loop": ("Self-reinforcing decision pattern detected", "AIManager"),
                "model_misalignment": ("Model predictions diverging from intended behaviors", "QuantumNexusModel"),
                "data_corruption": ("Inaccuracies in stored information affecting reasoning", "MemorySystem"),
                "convergence_failure": ("Learning process failing to stabilize", "AdaptiveLearningSystem")
            }
            if error_type in details_lookup:
                detail_text, affected = details_lookup[error_type]
                error_details["details"] = detail_text
                error_details["affected_system"] = affected
            if severity > 0.7:
                logger.warning(f"Detected {error_type} error (severity: {severity:.2f})")
            return error_details
        return None

    def simulate_error_correction(self, error_details: Dict[str, Any]) -> Dict[str, Any]:
        """
        Simulate error correction strategies within the hypermorphic framework.
        Returns a dictionary describing the correction result.
        """
        if not error_details or not isinstance(error_details, dict):
            logger.error("Invalid error details provided for correction.")
            return {"success": False, "reason": "Invalid error details"}

        error_type = error_details.get("type", "unknown")
        severity = error_details.get("severity", 0.5)
        specificity = error_details.get("specificity", 0.5)
        base_success_prob = 0.7
        success_prob = base_success_prob * (0.5 + specificity/2) * (1.3 - severity/2)
        retry_boost = 0.2 if error_type in self.failed_corrections else 0.0
        success_prob = min(0.95, success_prob + retry_boost)
        correction_successful = random.random() < success_prob

        strategies_by_type = {
            "content_quality": [
                "Recalibrate content quality thresholds",
                "Implement additional filtering layers",
                "Increase weight of domain authority in evaluation",
                "Apply stricter semantic relevance filters",
                "Enhance text-to-noise ratio detection"
            ],
            "exploration_strategy": [
                "Adjust exploration/exploitation balance",
                "Implement temporary randomness increase",
                "Refocus on high-information domains",
                "Apply entropy-based domain selection",
                "Implement strategic domain rotation"
            ],
            "memory_overflow": [
                "Increase pruning of low-importance memories",
                "Implement more aggressive compression",
                "Adjust importance calculation parameters",
                "Apply temporal decay to older memories",
                "Implement semantic clustering for memory organization"
            ],
            "reasoning_fallacy": [
                "Apply metacognitive verification steps",
                "Introduce counterfactual checking",
                "Implement logical consistency validation",
                "Apply Bayesian reasoning correction",
                "Implement multi-perspective reasoning"
            ],
            "attention_misallocation": [
                "Recalibrate attention mechanism weights",
                "Implement attention budget constraints",
                "Add periodic attention reset mechanism",
                "Apply information-gain-weighted attention",
                "Implement context-aware attention allocation"
            ],
            "resource_exhaustion": [
                "Implement resource quota system",
                "Prioritize high-value computational tasks",
                "Introduce adaptive resource allocation",
                "Apply computational load balancing",
                "Implement resource usage optimization"
            ],
            "feedback_loop": [
                "Interrupt cyclic patterns with randomization",
                "Implement feedback dampening mechanisms",
                "Reset affected subsystem parameters",
                "Apply decision diversity requirements",
                "Implement pattern-break triggers"
            ],
            "model_misalignment": [
                "Realign model objectives with outcomes",
                "Implement preference alignment validation",
                "Apply behavior boundary constraints",
                "Enhance value alignment mechanisms",
                "Implement targeted diagnostic sequence"
            ],
            "data_corruption": [
                "Apply data integrity validation",
                "Implement redundant storage mechanisms",
                "Refresh from authoritative sources",
                "Apply error-correcting mechanisms",
                "Implement targeted diagnostic sequence"
            ],
            "convergence_failure": [
                "Reset optimization parameters",
                "Implement alternative convergence paths",
                "Apply learning rate schedules",
                "Introduce gradient stabilization mechanisms",
                "Implement landscape reshaping techniques"
            ]
        }
        available_strategies = strategies_by_type.get(error_type, [
            "Apply general error correction procedure",
            "Reset affected subsystem parameters",
            "Implement targeted diagnostic sequence"
        ])
        if error_type in self.failed_strategies:
            failed_strats = self.failed_strategies[error_type]
            available_strategies = [s for s in available_strategies if s not in failed_strats]
            if not available_strategies:
                available_strategies = strategies_by_type.get(error_type, ["Apply emergency recovery procedure"])
        selected_strategy = random.choice(available_strategies)
        if not correction_successful:
            self.failed_corrections[error_type] = self.failed_corrections.get(error_type, 0) + 1
            self.failed_strategies.setdefault(error_type, set()).add(selected_strategy)
        correction_result = {
            "error_type": error_type,
            "strategy_applied": selected_strategy,
            "success": correction_successful,
            "effectiveness": random.uniform(0.6, 0.95) if correction_successful else random.uniform(0.1, 0.4),
            "timestamp": datetime.now().isoformat()
        }
        if correction_successful:
            correction_result["changes_made"] = random.randint(1, 5)
            correction_result["recovery_level"] = "complete" if random.random() < 0.7 else "partial"
            self.failed_corrections.pop(error_type, None)
            self.failed_strategies.pop(error_type, None)
        else:
            correction_result["retry_recommended"] = True
            correction_result["alternative_strategies"] = len(available_strategies) - 1
        logger.info(f"Error correction for {error_type}: Strategy '{selected_strategy}' - Success: {correction_successful}")
        return correction_result

    def simulate_quantum_cognition(self) -> Optional[Dict[str, Any]]:
        """
        Simulate quantum-like cognitive processes including superposition, interference,
        entanglement, and contextuality using hypermorphic principles.
        """
        quantum_probability = 0.15
        if random.random() > quantum_probability:
            return None
        quantum_phenomena = ["superposition", "interference", "entanglement", "contextuality"]
        phenomenon = random.choice(quantum_phenomena)
        anomalies = []
        insights = []
        if phenomenon == "superposition":
            anomalies = [
                "Multiple incompatible goal states activated simultaneously",
                "Strategy selection maintains all possibilities until execution",
                "Knowledge representations exist in contradictory states"
            ]
            insights = [
                "Leveraging superposition enables parallel evaluation of strategies",
                "Maintaining multiple goal states increases adaptive flexibility",
                "Superposed representations enrich the hypothesis space"
            ]
        elif phenomenon == "interference":
            anomalies = [
                "Goal pathways show constructive/destructive interference",
                "Strategy combinations produce unexpected enhancements",
                "Non-linear interference in knowledge patterns"
            ]
            insights = [
                "Constructive interference amplifies effective strategies",
                "Interference reveals hidden knowledge connections",
                "Interference effects create emergent capabilities"
            ]
        elif phenomenon == "entanglement":
            anomalies = [
                "Distant domains show inexplicable correlations",
                "Strategy outcomes become entangled across contexts",
                "Non-local influences in goal achievement"
            ]
            insights = [
                "Entangled representations enable robust cross-domain transfer",
                "Entanglement allows coordinated multi-system adaptation",
                "Entangled goals foster self-reinforcing alignment"
            ]
        elif phenomenon == "contextuality":
            anomalies = [
                "Knowledge valuation depends strongly on context",
                "Strategy effectiveness violates classical probability bounds",
                "Cognitive states exhibit contextual anomalies"
            ]
            insights = [
                "Contextual representations improve semantic accuracy",
                "Contextuality enables nuanced decision strategies",
                "Context-dependent cognition enhances adaptive intelligence"
            ]
        anomaly = random.choice(anomalies)
        insight = random.choice(insights)
        is_significant = random.random() < 0.3
        result = {
            "phenomenon": phenomenon,
            "anomaly_detected": is_significant,
            "anomaly": anomaly if is_significant else None,
            "insight": insight,
            "timestamp": datetime.now().isoformat()
        }
        log_level = "DEBUG" if is_significant else "INFO"
        logger.log(logging.DEBUG if is_significant else logging.INFO,
                   f"Quantum Cognition: {phenomenon.title()} - {'Anomaly: ' + anomaly if is_significant else 'Insight: ' + insight}")
        return result

    def get_imagination_report(self) -> Dict[str, Any]:
        """
        Generate a comprehensive report on engine activity, summarizing the number of insights,
        average quality, distribution by domain and mode, and overall creative health.
        """
        if not self.insight_history:
            return {"status": "inactive", "insights_generated": 0}
        insight_count = len(self.insight_history)
        avg_quality = sum(i["quality"] for i in self.insight_history) / insight_count

        domain_counts: Dict[str, int] = {}
        mode_counts: Dict[str, int] = {}
        for insight in self.insight_history:
            domain = insight.get("domain", "unknown")
            mode = insight.get("mode", "unknown")
            domain_counts[domain] = domain_counts.get(domain, 0) + 1
            mode_counts[mode] = mode_counts.get(mode, 0) + 1

        top_insights = sorted(self.insight_history, key=lambda x: x.get("quality", 0), reverse=True)[:3]
        top_texts = [i["text"] for i in top_insights]

        mode_quality: Dict[str, float] = {}
        for mode in self.cognitive_modes:
            mode_insights = [i for i in self.insight_history if i.get("mode") == mode]
            if mode_insights:
                mode_quality[mode] = sum(i["quality"] for i in mode_insights) / len(mode_insights)
        most_effective_mode = max(mode_quality.items(), key=lambda x: x[1])[0] if mode_quality else None

        report = {
            "status": "active",
            "insights_generated": insight_count,
            "average_quality": avg_quality,
            "domain_distribution": domain_counts,
            "mode_distribution": mode_counts,
            "most_effective_mode": most_effective_mode,
            "top_insights": top_texts,
            "creativity_level": self.creativity_level,
            "imagination_health": "high" if avg_quality > 0.7 else "medium" if avg_quality > 0.5 else "low"
        }
        logger.info(f"Imagination Report generated with {insight_count} insights.")
        return report

# ------------------------------------------------------------------------------
# Helper Logging Function
# ------------------------------------------------------------------------------

def log_event(message: str, level: str = "INFO") -> None:
    """
    Simple log event function to wrap logger calls.
    """
    timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
    print(f"{timestamp} [{level}] {message}")



class DomainIntelligence:
    """
    Advanced domain analysis system for understanding website characteristics,
    content patterns, and authority metrics to guide exploration.
    """
    def __init__(self):
        self.domain_knowledge = {}  # Main knowledge store for domains
        self.domain_categories = {}  # Categorization of domains
        self.authority_metrics = {}  # Authority scores and metrics
        self.topic_expertise = {}    # Domain topic area expertise
        self.relation_graph = {}     # Graph of domain relationships
        self.access_patterns = {}    # Patterns of domain access and results
        self.update_timestamps = {}  # When domains were last analyzed
        self.anomaly_records = {}    # Record of domain anomalies

        # Reference categorization data
        self.category_keywords = {
            "academic": ["university", "research", "edu", "academic", "science", "study", "journal"],
            "technology": ["tech", "programming", "software", "hardware", "computer", "code", "developer"],
            "news": ["news", "article", "report", "journalism", "media", "current", "daily"],
            "reference": ["wiki", "reference", "encyclopedia", "knowledge", "dictionary", "information"],
            "social": ["social", "community", "forum", "discussion", "comment", "people", "network"],
            "commercial": ["shop", "product", "buy", "price", "store", "commerce", "retail", "purchase"],
            "government": ["gov", "government", "official", "public", "administration", "agency", "state"],
            "entertainment": ["entertainment", "game", "music", "video", "movie", "play", "fun"]
        }

        # Authority signals
        self.authority_signals = [
            "domain_age", "citation_count", "https_enabled",
            "content_quality", "update_frequency", "outbound_links",
            "referral_pattern", "content_depth"
        ]

        log_event("DomainIntelligence initialized", "INFO")

    def analyze_domain(self, domain_url):
        """
        Perform comprehensive analysis of a domain to understand its
        characteristics, trustworthiness, and specialization.
        """
        if not domain_url:
            return False

        # Parse URL to extract domain
        try:
            parsed_url = urlparse(domain_url)
            domain = parsed_url.netloc

            # Skip if empty domain
            if not domain:
                return False

            # Record analysis time
            current_time = datetime.now().isoformat()
            self.update_timestamps[domain] = current_time

            # Extract domain components
            domain_parts = domain.split('.')
            tld = domain_parts[-1] if len(domain_parts) > 0 else ""
            sld = domain_parts[-2] if len(domain_parts) > 1 else ""

            # Initial domain type inference from TLD
            domain_type = "unknown"
            if tld == "edu":
                domain_type = "academic"
            elif tld == "gov":
                domain_type = "government"
            elif tld == "org":
                domain_type = "organization"
            elif tld == "com":
                domain_type = "commercial"

            # Analyze domain name for category clues
            domain_name_lower = domain.lower()
            category_scores = {}

            for category, keywords in self.category_keywords.items():
                # Calculate score based on keyword presence
                score = sum(1 for keyword in keywords if keyword in domain_name_lower)
                if score > 0:
                    category_scores[category] = score

            # Determine primary category if scores exist
            primary_category = None
            if category_scores:
                primary_category = max(category_scores.items(), key=lambda x: x[1])[0]

            # Create or update domain record
            if domain not in self.domain_knowledge:
                # New domain record
                self.domain_knowledge[domain] = {
                    "domain": domain,
                    "tld": tld,
                    "domain_type": domain_type,
                    "primary_category": primary_category,
                    "category_scores": category_scores,
                    "first_analyzed": current_time,
                    "last_updated": current_time,
                    "visit_count": 1,
                    "pages_analyzed": 0,
                    "authority_score": 0.5,  # Initial neutral score
                    "content_quality": None,
                    "https_enabled": parsed_url.scheme == "https",
                    "known_topics": [],
                    "page_pattern": {}
                }

                # Add to category index
                if primary_category:
                    if primary_category not in self.domain_categories:
                        self.domain_categories[primary_category] = set()
                    self.domain_categories[primary_category].add(domain)

                log_event(f"Added new domain to intelligence database: {domain} ({domain_type}, {primary_category})", "INFO")
            else:
                # Update existing record
                self.domain_knowledge[domain]["last_updated"] = current_time
                self.domain_knowledge[domain]["visit_count"] += 1

                # Update category if confidence has improved
                existing_category = self.domain_knowledge[domain]["primary_category"]
                if primary_category and (not existing_category or
                                       category_scores.get(primary_category, 0) >
                                       self.domain_knowledge[domain]["category_scores"].get(existing_category, 0)):

                    # Remove from old category index
                    if existing_category and existing_category in self.domain_categories:
                        self.domain_categories[existing_category].discard(domain)

                    # Add to new category index
                    if primary_category not in self.domain_categories:
                        self.domain_categories[primary_category] = set()
                    self.domain_categories[primary_category].add(domain)

                    # Update domain record
                    self.domain_knowledge[domain]["primary_category"] = primary_category
                    self.domain_knowledge[domain]["category_scores"] = category_scores

                    log_event(f"Updated domain categorization: {domain} recategorized as {primary_category}", "INFO")

            # Calculate authority score (simplified version)
            self._calculate_authority_score(domain)

            return True

        except Exception as e:
            log_event(f"Error analyzing domain {domain_url}: {e}", "ERROR")
            return False

    def _calculate_authority_score(self, domain):
        """Calculate domain authority score based on multiple signals"""
        if domain not in self.domain_knowledge:
            return 0.5  # Default neutral score

        # Collect available signals
        signals = {}
        domain_data = self.domain_knowledge[domain]

        # Signal: Domain Type factor
        domain_type_factors = {
            "academic": 0.8,
            "government": 0.8,
            "organization": 0.7,
            "news": 0.6,
            "reference": 0.7,
            "commercial": 0.5,
            "unknown": 0.5
        }

        signals["domain_type"] = domain_type_factors.get(domain_data.get("domain_type", "unknown"), 0.5)

        # Signal: HTTPS enabled
        signals["https_enabled"] = 0.7 if domain_data.get("https_enabled", False) else 0.3

        # Signal: Visit success rate
        visit_count = domain_data.get("visit_count", 0)
        pages_analyzed = domain_data.get("pages_analyzed", 0)
        signals["success_rate"] = min(0.9, pages_analyzed / max(1, visit_count))

        # Signal: TLD trustworthiness
        tld_trust = {
            "edu": 0.9,
            "gov": 0.9,
            "org": 0.7,
            "com": 0.5,
            "net": 0.5,
            "io": 0.6,
            "ai": 0.6
        }
        signals["tld_trust"] = tld_trust.get(domain_data.get("tld", ""), 0.4)

        # Signal: Content quality if available
        if domain_data.get("content_quality") is not None:
            signals["content_quality"] = domain_data["content_quality"]

        # Signal: Topic expertise if established
        if domain in self.topic_expertise and self.topic_expertise[domain]:
            # Average expertise across topics
            signals["topic_expertise"] = sum(self.topic_expertise[domain].values()) / len(self.topic_expertise[domain])

        # Calculate overall authority score
        # Weighted average of available signals
        weights = {
            "domain_type": 0.15,
            "https_enabled": 0.05,
            "success_rate": 0.20,
            "tld_trust": 0.10,
            "content_quality": 0.30,
            "topic_expertise": 0.20
        }

        total_weight = 0
        weighted_sum = 0

        for signal, value in signals.items():
            if signal in weights:
                weighted_sum += value * weights[signal]
                total_weight += weights[signal]

        # Compute final score with normalization
        if total_weight > 0:
            authority_score = weighted_sum / total_weight
        else:
            authority_score = 0.5  # Default if no signals available

        # Store authority score
        self.domain_knowledge[domain]["authority_score"] = authority_score
        self.authority_metrics[domain] = {
            "score": authority_score,
            "signals": signals,
            "timestamp": datetime.now().isoformat()
        }

        return authority_score

    def update_content_knowledge(self, domain, page_url, content_data):
        """
        Update domain knowledge with information from analyzed content.

        Parameters:
        - domain: Domain name
        - page_url: Full URL of the analyzed page
        - content_data: Dict with keys like 'quality', 'topics', 'length', etc.
        """
        if not domain or not page_url or not content_data:
            return False

        # Ensure domain exists in knowledge base
        if domain not in self.domain_knowledge:
            self.analyze_domain(page_url)

        if domain not in self.domain_knowledge:
            return False  # Domain analysis failed

        # Update page count
        self.domain_knowledge[domain]["pages_analyzed"] = self.domain_knowledge[domain].get("pages_analyzed", 0) + 1

        # Store page pattern
        path = urlparse(page_url).path
        page_pattern = self.domain_knowledge[domain].get("page_pattern", {})

        # Analyze path pattern
        path_parts = path.strip('/').split('/')

        # Identify path type
        path_type = "root"
        if len(path_parts) == 1 and path_parts[0]:
            path_type = "top_level"
        elif len(path_parts) > 1:
            path_type = "deep"

        # Update path type counts
        if "path_types" not in page_pattern:
            page_pattern["path_types"] = {"root": 0, "top_level": 0, "deep": 0}

        page_pattern["path_types"][path_type] = page_pattern["path_types"].get(path_type, 0) + 1

        # Update path component statistics
        if "common_segments" not in page_pattern:
            page_pattern["common_segments"] = {}

        for part in path_parts:
            if part:  # Skip empty parts
                page_pattern["common_segments"][part] = page_pattern["common_segments"].get(part, 0) + 1

        # Store updated page pattern
        self.domain_knowledge[domain]["page_pattern"] = page_pattern

        # Update content quality metrics
        if "quality" in content_data:
            quality_score = content_data["quality"]

            # Update rolling average of content quality
            current_quality = self.domain_knowledge[domain].get("content_quality")
            if current_quality is None:
                self.domain_knowledge[domain]["content_quality"] = quality_score
            else:
                # Weight existing score more to avoid large fluctuations
                self.domain_knowledge[domain]["content_quality"] = current_quality * 0.8 + quality_score * 0.2

        # Update topic knowledge
        if "topics" in content_data and content_data["topics"]:
            topics = content_data["topics"]

            # Update known topics for domain
            known_topics = set(self.domain_knowledge[domain].get("known_topics", []))
            known_topics.update(topics)
            self.domain_knowledge[domain]["known_topics"] = list(known_topics)

            # Update topic expertise
            if domain not in self.topic_expertise:
                self.topic_expertise[domain] = {}

            for topic in topics:
                # Increase expertise in this topic
                current_expertise = self.topic_expertise[domain].get(topic, 0.3)  # Start with low expertise
                # Expertise increases with each encounter but plateaus
                self.topic_expertise[domain][topic] = min(0.9, current_expertise + 0.05)

        # Recalculate authority score with new information
        self._calculate_authority_score(domain)

        return True

    def find_related_domains(self, domain, relation_type="topic", max_results=5):
        """
        Find domains related to the given domain by topic or other criteria.

        Parameters:
        - domain: Domain to find relations for
        - relation_type: Type of relation ('topic', 'category', 'link')
        - max_results: Maximum number of results to return
        """
        if not domain or domain not in self.domain_knowledge:
            return []

        related_domains = []

        if relation_type == "topic":
            # Find domains with overlapping topics
            domain_topics = set(self.domain_knowledge[domain].get("known_topics", []))

            if not domain_topics:
                return []  # No topics to match

            # Score domains by topic overlap
            domain_scores = {}

            for d, data in self.domain_knowledge.items():
                if d == domain:
                    continue  # Skip self

                d_topics = set(data.get("known_topics", []))
                if not d_topics:
                    continue  # Skip domains with no topics

                # Calculate Jaccard similarity of topic sets
                overlap = len(domain_topics.intersection(d_topics))
                union = len(domain_topics.union(d_topics))

                if overlap > 0:
                    similarity = overlap / union
                    domain_scores[d] = similarity

            # Sort by similarity score
            sorted_domains = sorted(domain_scores.items(), key=lambda x: x[1], reverse=True)
            related_domains = [(d, score) for d, score in sorted_domains[:max_results]]

        elif relation_type == "category":
            # Find domains in the same category
            category = self.domain_knowledge[domain].get("primary_category")

            if not category or category not in self.domain_categories:
                return []

            # Get domains in same category
            category_domains = [d for d in self.domain_categories[category] if d != domain]

            # Sort by authority score
            domain_scores = []
            for d in category_domains:
                if d in self.domain_knowledge:
                    score = self.domain_knowledge[d].get("authority_score", 0.5)
                    domain_scores.append((d, score))

            # Sort by authority score
            sorted_domains = sorted(domain_scores, key=lambda x: x[1], reverse=True)
            related_domains = sorted_domains[:max_results]

        elif relation_type == "link":
            # Find domains that share links (requires backlink tracking)
            # This would be implemented with the relation graph
            if domain in self.relation_graph and "links_to" in self.relation_graph[domain]:
                links = self.relation_graph[domain]["links_to"]
                related_domains = [(d, 1.0) for d in links][:max_results]

        return related_domains

    def get_domain_knowledge(self, domain):
        """
        Retrieve comprehensive knowledge about a domain.

        Parameters:
        - domain: Domain name to retrieve knowledge for
        """
        # Handle both domain name and full URL
        if not domain:
            return None

        # Check if this is a URL and extract domain
        if domain.startswith(('http://', 'https://')):
            domain = urlparse(domain).netloc

        if not domain:
            return None

        # Return domain knowledge if exists
        if domain in self.domain_knowledge:
            # Create a copy to avoid direct modification
            knowledge = self.domain_knowledge[domain].copy()

            # Add additional related information
            knowledge["authority_metrics"] = self.authority_metrics.get(domain, {})
            knowledge["topic_expertise"] = self.topic_expertise.get(domain, {})

            # Add anomaly records if they exist
            if domain in self.anomaly_records:
                knowledge["anomalies"] = self.anomaly_records[domain]

            # Add related domains
            related_by_topic = self.find_related_domains(domain, "topic", 3)
            if related_by_topic:
                knowledge["related_domains"] = [d for d, _ in related_by_topic]

            return knowledge

        return None

    def detect_domain_anomalies(self, domain):
        """
        Analyze domain for potential anomalies or suspicious patterns.

        Parameters:
        - domain: Domain to check for anomalies
        """
        if not domain or domain not in self.domain_knowledge:
            return []

        anomalies = []
        domain_data = self.domain_knowledge[domain]

        # Anomaly 1: TLD mismatch with content
        tld = domain_data.get("tld", "")
        category = domain_data.get("primary_category")

        if tld == "edu" and category and category not in ["academic", "reference"]:
            anomalies.append({
                "type": "tld_category_mismatch",
                "description": f"Domain has .edu TLD but content suggests '{category}' category",
                "severity": "medium"
            })

        # Anomaly 2: Low content quality on authoritative TLD
        content_quality = domain_data.get("content_quality")
        if content_quality and content_quality < 0.4 and tld in ["edu", "gov", "org"]:
            anomalies.append({
                "type": "low_quality_authoritative",
                "description": f"Domain with .{tld} TLD has unusually low content quality ({content_quality:.2f})",
                "severity": "high"
            })

        # Anomaly 3: Unusual page pattern
        page_pattern = domain_data.get("page_pattern", {})
        path_types = page_pattern.get("path_types", {})

        if path_types.get("deep", 0) > 10 * max(1, path_types.get("top_level", 0) + path_types.get("root", 0)):
            anomalies.append({
                "type": "unusual_path_pattern",
                "description": "Domain shows unusually high ratio of deep paths to top-level content",
                "severity": "low"
            })

        # Anomaly 4: Visit count vs pages analyzed mismatch
        visit_count = domain_data.get("visit_count", 0)
        pages_analyzed = domain_data.get("pages_analyzed", 0)

        if visit_count > 5 and pages_analyzed / visit_count < 0.3:
            anomalies.append({
                "type": "low_analysis_success_rate",
                "description": f"Domain has low success rate: {pages_analyzed}/{visit_count} visits produced content",
                "severity": "medium"
            })

        # Store anomalies if found
        if anomalies:
            self.anomaly_records[domain] = {
                "detected": anomalies,
                "timestamp": datetime.now().isoformat()
            }

            # Log high severity anomalies
            high_severity = [a for a in anomalies if a.get("severity") == "high"]
            if high_severity:
                for anomaly in high_severity:
                    log_event(f"Domain anomaly detected for {domain}: {anomaly['description']}", "WARNING")

        return anomalies

    def get_domain_recommendation(self, current_domain, purpose="exploration"):
        """
        Recommend related domains based on current domain and purpose.

        Parameters:
        - current_domain: The current domain
        - purpose: Why we need recommendations ('exploration', 'deepening', 'verification')
        """
        if not current_domain:
            # Recommend highly trusted domains by default
            trusted_domains = self._get_top_domains_by_authority(5)
            if trusted_domains:
                return {
                    "recommendations": trusted_domains,
                    "reason": "Default trusted domains for general exploration"
                }
            return None

        # Extract domain from URL if needed
        if current_domain.startswith(('http://', 'https://')):
            current_domain = urlparse(current_domain).netloc

        # Check domain knowledge
        if current_domain not in self.domain_knowledge:
            return None

        domain_data = self.domain_knowledge[current_domain]

        # Different recommendation strategies based on purpose
        if purpose == "exploration":
            # Favor topic-related domains with high authority
            related = self.find_related_domains(current_domain, "topic", 10)

            # Filter for good authority
            good_related = [(domain, score) for domain, score in related
                          if domain in self.domain_knowledge
                          and self.domain_knowledge[domain].get("authority_score", 0) > 0.6]

            if good_related:
                return {
                    "recommendations": [domain for domain, _ in good_related[:5]],
                    "reason": f"Topic-related domains to {current_domain} with good authority"
                }

        elif purpose == "deepening":
            # Focus on same category with highest topic expertise
            category = domain_data.get("primary_category")
            if category and category in self.domain_categories:
                # Get domains in same category
                category_domains = [d for d in self.domain_categories[category] if d != current_domain]

                # Score by topic expertise and authority
                domain_scores = []

                for d in category_domains:
                    if d in self.domain_knowledge and d in self.topic_expertise:
                        # Average topic expertise
                        topics = self.topic_expertise[d]
                        if topics:
                            avg_expertise = sum(topics.values()) / len(topics)
                            authority = self.domain_knowledge[d].get("authority_score", 0.5)

                            # Combined score weighing expertise more
                            score = avg_expertise * 0.7 + authority * 0.3
                            domain_scores.append((d, score))

                if domain_scores:
                    sorted_domains = sorted(domain_scores, key=lambda x: x[1], reverse=True)
                    return {
                        "recommendations": [domain for domain, _ in sorted_domains[:5]],
                        "reason": f"Domains with deep expertise in {category} category"
                    }

        elif purpose == "verification":
            # Find authoritative domains in same topic area
            topics = domain_data.get("known_topics", [])
            if not topics:
                return None

            # Find domains with same topics but higher authority
            current_authority = domain_data.get("authority_score", 0.5)
            verification_candidates = []

            for d, data in self.domain_knowledge.items():
                if d == current_domain:
                    continue

                # Check topic overlap
                d_topics = set(data.get("known_topics", []))
                overlap = len(set(topics).intersection(d_topics))

                if overlap > 0 and data.get("authority_score", 0) > current_authority:
                    # Score by authority and topic overlap
                    score = data.get("authority_score", 0) * (overlap / len(topics))
                    verification_candidates.append((d, score))

            if verification_candidates:
                sorted_candidates = sorted(verification_candidates, key=lambda x: x[1], reverse=True)
                return {
                    "recommendations": [domain for domain, _ in sorted_candidates[:5]],
                    "reason": f"More authoritative sources on same topics as {current_domain}"
                }

        # Default to highest authority domains as fallback
        return {
            "recommendations": self._get_top_domains_by_authority(5),
            "reason": "General high-authority domains (fallback recommendation)"
        }

    def _get_top_domains_by_authority(self, count=5):
        """Get the top domains by authority score"""
        if not self.domain_knowledge:
            return []

        # Sort domains by authority score
        scored_domains = [(d, data.get("authority_score", 0))
                        for d, data in self.domain_knowledge.items()]
        sorted_domains = sorted(scored_domains, key=lambda x: x[1], reverse=True)

        return [domain for domain, _ in sorted_domains[:count]]

    def export_domain_report(self, domain):
        """
        Generate comprehensive report on domain for external use.

        Parameters:
        - domain: Domain to generate report for
        """
        if not domain or domain not in self.domain_knowledge:
            return None

        knowledge = self.get_domain_knowledge(domain)
        if not knowledge:
            return None

        # Generate anomaly section if needed
        anomalies = self.detect_domain_anomalies(domain)
        anomaly_section = None
        if anomalies:
            anomaly_section = {
                "count": len(anomalies),
                "details": anomalies,
                "recommended_action": "caution" if any(a.get("severity") == "high" for a in anomalies) else "monitor"
            }

        # Get related domains
        topic_related = self.find_related_domains(domain, "topic", 5)
        category_related = self.find_related_domains(domain, "category", 5)

        # Compile report
        report = {
            "domain": domain,
            "analysis_date": datetime.now().isoformat(),
            "summary": {
                "type": knowledge.get("domain_type", "unknown"),
                "category": knowledge.get("primary_category", "unknown"),
                "authority_score": knowledge.get("authority_score", 0),
                "content_quality": knowledge.get("content_quality"),
                "visit_count": knowledge.get("visit_count", 0),
                "pages_analyzed": knowledge.get("pages_analyzed", 0)
            },
            "authority_assessment": {
                "score": knowledge.get("authority_score", 0),
                "factors": knowledge.get("authority_metrics", {}).get("signals", {}),
                "interpretation": self._interpret_authority_score(knowledge.get("authority_score", 0))
            },
            "content_profile": {
                "known_topics": knowledge.get("known_topics", []),
                "path_patterns": knowledge.get("page_pattern", {}).get("path_types", {}),
                "https_enabled": knowledge.get("https_enabled", False)
            },
            "related_domains": {
                "by_topic": [d for d, _ in topic_related],
                "by_category": [d for d, _ in category_related]
            }
        }

        # Add anomalies if detected
        if anomaly_section:
            report["anomalies"] = anomaly_section

        return report

    def _interpret_authority_score(self, score):
        """Provide interpretation of authority score"""
        if score >= 0.8:
            return "Very High Authority - Likely a definitive source in its field"
        elif score >= 0.7:
            return "High Authority - Generally trustworthy and established source"
        elif score >= 0.5:
            return "Moderate Authority - Adequate source but verification recommended"
        elif score >= 0.3:
            return "Low Authority - Approach with caution, verify information"
        else:
            return "Very Low Authority - Not recommended as a primary information source"

    def get_intelligence_statistics(self):
        """Generate statistics about domain intelligence database"""
        if not self.domain_knowledge:
            return {"status": "empty", "domains_analyzed": 0}

        # Basic counts
        domain_count = len(self.domain_knowledge)

        # Category distribution
        category_counts = {}
        for category, domains in self.domain_categories.items():
            category_counts[category] = len(domains)

        # Authority distribution
        authority_ranges = {
            "very_high": 0,  # 0.8-1.0
            "high": 0,       # 0.6-0.8
            "moderate": 0,   # 0.4-0.6
            "low": 0,        # 0.2-0.4
            "very_low": 0    # 0.0-0.2
        }

        for domain, data in self.domain_knowledge.items():
            score = data.get("authority_score", 0.5)

            if score >= 0.8:
                authority_ranges["very_high"] += 1
            elif score >= 0.6:
                authority_ranges["high"] += 1
            elif score >= 0.4:
                authority_ranges["moderate"] += 1
            elif score >= 0.2:
                authority_ranges["low"] += 1
            else:
                authority_ranges["very_low"] += 1

        # TLD distribution
        tld_counts = {}
        for domain, data in self.domain_knowledge.items():
            tld = data.get("tld", "unknown")
            tld_counts[tld] = tld_counts.get(tld, 0) + 1

        # Anomaly statistics
        anomaly_count = sum(1 for domain in self.anomaly_records)
        high_severity_count = sum(
            1 for domain, record in self.anomaly_records.items()
            if any(a.get("severity") == "high" for a in record.get("detected", []))
        )

        return {
            "status": "active",
            "domains_analyzed": domain_count,
            "category_distribution": category_counts,
            "authority_distribution": authority_ranges,
            "tld_distribution": tld_counts,
            "anomalies_detected": anomaly_count,
            "high_severity_anomalies": high_severity_count
        }





class MetaLearningModule:
    """
    Advanced meta-learning system that monitors, analyzes, and optimizes
    the learning process itself, enabling autonomous improvement of the
    model's learning capabilities over time.
    """
    def __init__(self, model):
        self.model = model
        self.learning_history = deque(maxlen=100)  # Track recent learning metrics
        self.hyperparameter_history = []  # Track hyperparameter evolution
        self.architecture_history = []  # Track architecture changes
        self.performance_trends = {}  # Performance over time for different metrics
        self.learning_rate_schedule = []  # History of learning rate adjustments
        self.optimization_state = "exploration"  # Current optimization phase
        self.training_cycles = 0  # Total training cycles performed
        self.gradient_statistics = deque(maxlen=20)  # Recent gradient statistics
        self.weight_evolution = {}  # Track how weights evolve over time
        self.improvement_rates = {}  # Rate of improvement for different metrics

        # Meta-parameters (parameters about parameter learning)
        self.meta_params = {
            "exploration_rate": 0.3,  # How much to explore hyperparameter space
            "stability_threshold": 0.05,  # Threshold for stability detection
            "adaptation_rate": 0.2,  # How quickly to adapt to new information
            "patience": 5,  # Cycles to wait before making significant changes
            "learning_rate_bounds": (1e-6, 1e-2),  # Min/max learning rate
            "trend_window": 10,  # Window size for trend analysis
            "architecture_change_threshold": 0.1  # Threshold for architecture changes
        }

        # Initialize monitoring
        log_event("MetaLearningModule initialized with meta-optimization capabilities", "INFO")

    def track_performance(self, metrics):
        """
        Track and analyze training performance metrics over time
        to inform meta-learning decisions.

        Parameters:
        - metrics: Dict containing performance metrics (loss, accuracy, etc.)
        """
        self.training_cycles += 1

        # Skip invalid metrics
        if not isinstance(metrics, dict) or len(metrics) == 0:
            return

        # Store metrics with timestamp
        timestamped_metrics = metrics.copy()
        timestamped_metrics["cycle"] = self.training_cycles
        timestamped_metrics["timestamp"] = datetime.now().isoformat()

        # Add current learning rate if available
        if hasattr(self.model, '_current_lr'):
            timestamped_metrics["learning_rate"] = getattr(self.model, '_current_lr')
        elif hasattr(self.model, 'optimizer') and hasattr(self.model.optimizer, 'param_groups'):
            # Try to get from optimizer
            try:
                timestamped_metrics["learning_rate"] = self.model.optimizer.param_groups[0]['lr']
            except (IndexError, KeyError):
                pass

        # Store in history
        self.learning_history.append(timestamped_metrics)

        # Update performance trends when we have enough data
        if len(self.learning_history) >= self.meta_params["trend_window"]:
            self._update_performance_trends()

        # Analyze learning periodically
        if self.training_cycles % max(1, self.meta_params["patience"]) == 0:
            self.analyze_trends()

    def _update_performance_trends(self):
        """Update trend analysis for each tracked metric"""
        window = self.meta_params["trend_window"]
        recent_metrics = list(self.learning_history)[-window:]

        # Find consistent metrics across all entries
        metric_keys = set.intersection(*[set(m.keys()) for m in recent_metrics])
        metric_keys = [k for k in metric_keys if k not in ["cycle", "timestamp", "learning_rate"]]

        for key in metric_keys:
            # Extract values
            values = [m[key] for m in recent_metrics if key in m]

            if not values or len(values) < window:
                continue

            # Calculate trend statistics
            mean_value = sum(values) / len(values)
            min_value = min(values)
            max_value = max(values)
            range_value = max_value - min_value

            # Calculate first derivative (rate of change)
            derivatives = [values[i] - values[i-1] for i in range(1, len(values))]
            mean_derivative = sum(derivatives) / len(derivatives) if derivatives else 0

            # Calculate second derivative (acceleration of change)
            accelerations = [derivatives[i] - derivatives[i-1] for i in range(1, len(derivatives))]
            mean_acceleration = sum(accelerations) / len(accelerations) if accelerations else 0

            # Interpret trend direction
            if abs(mean_derivative) < self.meta_params["stability_threshold"]:
                direction = "stable"
            elif mean_derivative < 0:
                direction = "decreasing"
            else:
                direction = "increasing"

            # Interpret acceleration
            if abs(mean_acceleration) < self.meta_params["stability_threshold"] / 2:
                acceleration = "steady"
            elif mean_acceleration < 0:
                acceleration = "decelerating"
            else:
                acceleration = "accelerating"

            # Store trend information
            self.performance_trends[key] = {
                "current": values[-1],
                "mean": mean_value,
                "min": min_value,
                "max": max_value,
                "range": range_value,
                "direction": direction,
                "rate_of_change": mean_derivative,
                "acceleration": acceleration,
                "acceleration_value": mean_acceleration,
                "updated_at": datetime.now().isoformat()
            }

            # Calculate improvement rate (relative to starting point)
            if len(values) > 1:
                # For loss, lower is better; for accuracy, higher is better
                is_loss_like = "loss" in key.lower() or "error" in key.lower()

                start_value = values[0]
                end_value = values[-1]

                if is_loss_like:
                    # For loss metrics, improvement is decrease
                    improvement = (start_value - end_value) / max(0.0001, start_value)
                else:
                    # For other metrics, improvement is increase
                    improvement = (end_value - start_value) / max(0.0001, start_value)

                self.improvement_rates[key] = improvement

    def analyze_trends(self):
        """
        Analyze learning trends and make meta-learning decisions
        for hyperparameter and architecture adaptation.
        """
        if len(self.learning_history) < self.meta_params["trend_window"]:
            log_event(f"Insufficient learning history for meta-analysis (need {self.meta_params['trend_window']})", "INFO")
            return

        # Check if we have loss metrics
        loss_metrics = [k for k in self.performance_trends.keys()
                       if "loss" in k.lower() or "error" in k.lower()]

        if not loss_metrics:
            log_event("No loss metrics found for meta-learning trend analysis", "WARNING")
            return

        # Use first loss metric as primary indicator
        primary_loss = loss_metrics[0]
        loss_trend = self.performance_trends[primary_loss]

        # Current state assessment
        current_state = {
            "direction": loss_trend["direction"],
            "acceleration": loss_trend["acceleration"],
            "value": loss_trend["current"],
            "improvement_rate": self.improvement_rates.get(primary_loss, 0)
        }

        # Decision making based on loss trend patterns
        decisions = self._make_meta_decisions(current_state, primary_loss)

        # Log significant decisions
        for decision in decisions:
            if decision["type"] in ["learning_rate_change", "architecture_change"]:
                log_event(f"Meta-learning decision: {decision['description']}", "INFO")

        # Apply decisions
        self._apply_meta_decisions(decisions)

    def _make_meta_decisions(self, current_state, primary_metric):
        """
        Make meta-learning decisions based on current learning state.

        Parameters:
        - current_state: Dict describing current trend state
        - primary_metric: Name of the primary metric being analyzed

        Returns:
        - List of decision dictionaries with type and parameters
        """
        decisions = []

        # Case 1: Learning plateaued (stable with low improvement)
        if (current_state["direction"] == "stable" and
            abs(current_state["improvement_rate"]) < self.meta_params["stability_threshold"]):

            # Check duration of plateau
            plateau_duration = self._count_consecutive_states("stable", primary_metric)

            if plateau_duration >= self.meta_params["patience"]:
                # Long plateau - need significant change
                if self.optimization_state == "exploration":
                    # In exploration phase, try architecture change
                    decisions.append({
                        "type": "architecture_change",
                        "change": "expand" if random.random() < 0.7 else "contract",
                        "description": f"Architecture change due to {plateau_duration}-cycle plateau in {primary_metric}"
                    })

                    # Also try learning rate restart
                    decisions.append({
                        "type": "learning_rate_change",
                        "factor": 10.0,
                        "description": "Learning rate increase to escape plateau"
                    })
                else:
                    # In refinement phase, smaller learning rate adjustment
                    decisions.append({
                        "type": "learning_rate_change",
                        "factor": random.uniform(2.0, 5.0),
                        "description": "Learning rate adjustment to overcome plateau"
                    })

                # Switch optimization state
                decisions.append({
                    "type": "optimization_state_change",
                    "new_state": "refinement" if self.optimization_state == "exploration" else "exploration",
                    "description": f"Switch optimization strategy due to {plateau_duration}-cycle plateau"
                })

        # Case 2: Loss increasing (getting worse)
        elif current_state["direction"] == "increasing" and "loss" in primary_metric.lower():
            # Loss is getting worse - need to reduce learning rate

            # Check if it's accelerating or steady
            if current_state["acceleration"] in ["accelerating", "steady"]:
                # Significant reduction needed
                decisions.append({
                    "type": "learning_rate_change",
                    "factor": 0.1,  # 10x reduction
                    "description": f"{primary_metric} {current_state['acceleration']} increase - significant LR reduction"
                })
            else:
                # Moderate reduction for decelerating increase
                decisions.append({
                    "type": "learning_rate_change",
                    "factor": 0.5,  # 2x reduction
                    "description": f"{primary_metric} decelerating increase - moderate LR reduction"
                })

        # Case 3: Loss decreasing nicely (improvement)
        elif current_state["direction"] == "decreasing" and "loss" in primary_metric.lower():
            # Loss is decreasing - check acceleration

            if current_state["acceleration"] == "accelerating":
                # Getting better faster - slightly increase learning rate
                decisions.append({
                    "type": "learning_rate_change",
                    "factor": 1.1,  # 10% increase
                    "description": f"{primary_metric} accelerating decrease - slight LR increase"
                })
            elif current_state["acceleration"] == "decelerating" and random.random() < 0.5:
                # Slowing improvement - try architecture change
                decisions.append({
                    "type": "architecture_change",
                    "change": "expand",
                    "description": f"{primary_metric} decelerating decrease - architecture expansion"
                })

        # Case 4: Random exploration if no clear pattern and in exploration mode
        elif self.optimization_state == "exploration" and random.random() < self.meta_params["exploration_rate"]:
            # Random exploration of hyperparameter space
            exploration_choices = ["learning_rate_change", "architecture_change"]
            exploration_type = random.choice(exploration_choices)

            if exploration_type == "learning_rate_change":
                factor = random.choice([0.5, 0.7, 1.5, 2.0])
                decisions.append({
                    "type": "learning_rate_change",
                    "factor": factor,
                    "description": f"Exploratory learning rate adjustment (factor: {factor})"
                })
            else:
                change = random.choice(["expand", "contract"])
                decisions.append({
                    "type": "architecture_change",
                    "change": change,
                    "description": f"Exploratory architecture {change}"
                })

        return decisions

    def _apply_meta_decisions(self, decisions):
        """
        Apply meta-learning decisions to the model and optimizer.

        Parameters:
        - decisions: List of decision dictionaries
        """
        for decision in decisions:
            decision_type = decision.get("type", "")

            if decision_type == "learning_rate_change":
                factor = decision.get("factor", 1.0)
                self._adjust_learning_rate(factor)

            elif decision_type == "architecture_change":
                change = decision.get("change", "")
                self._adjust_architecture(change)

            elif decision_type == "optimization_state_change":
                new_state = decision.get("new_state", self.optimization_state)
                self.optimization_state = new_state
                log_event(f"Meta-learning optimization state changed to: {new_state}", "INFO")

    def _adjust_learning_rate(self, factor):
        """
        Adjust learning rate by the given factor.

        Parameters:
        - factor: Multiplication factor for current learning rate
        """
        # Get current learning rate
        current_lr = None

        if hasattr(self.model, '_current_lr'):
            current_lr = getattr(self.model, '_current_lr')
        elif hasattr(self.model, 'optimizer') and hasattr(self.model.optimizer, 'param_groups'):
            try:
                current_lr = self.model.optimizer.param_groups[0]['lr']
            except (IndexError, KeyError):
                pass

        if current_lr is None:
            log_event("Could not access current learning rate for adjustment", "ERROR")
            return

        # Calculate new learning rate
        new_lr = current_lr * factor

        # Ensure it's within bounds
        min_lr, max_lr = self.meta_params["learning_rate_bounds"]
        new_lr = max(min_lr, min(max_lr, new_lr))

        # Apply to model attribute
        if hasattr(self.model, '_current_lr'):
            setattr(self.model, '_current_lr', new_lr)

        # Apply to optimizer
        if hasattr(self.model, 'optimizer') and hasattr(self.model.optimizer, 'param_groups'):
            try:
                for param_group in self.model.optimizer.param_groups:
                    param_group['lr'] = new_lr
            except Exception as e:
                log_event(f"Error adjusting optimizer learning rate: {e}", "ERROR")

        # Record the change
        self.learning_rate_schedule.append({
            "cycle": self.training_cycles,
            "old_lr": current_lr,
            "new_lr": new_lr,
            "factor": factor,
            "timestamp": datetime.now().isoformat()
        })

        log_event(f"Meta-learning adjusted learning rate: {current_lr:.6f} → {new_lr:.6f} (factor: {factor})", "INFO")

    def _adjust_architecture(self, change):
        """
        Apply architecture changes based on meta-learning decisions.

        Parameters:
        - change: Type of architecture change ('expand' or 'contract')
        """
        if change not in ["expand", "contract"]:
            log_event(f"Invalid architecture change type: {change}", "ERROR")
            return

        # Check if model supports architecture changes
        expand_method = getattr(self.model, 'expand_architecture', None)
        contract_method = getattr(self.model, 'contract_architecture', None)

        if change == "expand" and callable(expand_method):
            try:
                self.model.expand_architecture()

                # Record the change
                self.architecture_history.append({
                    "cycle": self.training_cycles,
                    "change": "expand",
                    "timestamp": datetime.now().isoformat()
                })

                log_event("Meta-learning expanded model architecture", "QUANTUM")
                return True
            except Exception as e:
                log_event(f"Error expanding architecture: {e}", "ERROR")

        elif change == "contract" and callable(contract_method):
            try:
                self.model.contract_architecture()

                # Record the change
                self.architecture_history.append({
                    "cycle": self.training_cycles,
                    "change": "contract",
                    "timestamp": datetime.now().isoformat()
                })

                log_event("Meta-learning contracted model architecture", "QUANTUM")
                return True
            except Exception as e:
                log_event(f"Error contracting architecture: {e}", "ERROR")
        else:
            log_event(f"Model does not support architecture {change} operations", "WARNING")

        return False

    def _count_consecutive_states(self, state_type, metric):
        """
        Count how many consecutive cycles the metric has been in given state.

        Parameters:
        - state_type: The state to count ('stable', 'increasing', 'decreasing')
        - metric: The metric name to analyze

        Returns:
        - Count of consecutive cycles in that state
        """
        count = 0
        history = list(self.learning_history)

        # Need at least window_size entries to have computed trends
        if len(history) < self.meta_params["trend_window"]:
            return 0

        # Go through trend history (would need to store trend history to be more accurate)
        # This is an approximation based on current trend
        if metric in self.performance_trends:
            if self.performance_trends[metric]["direction"] == state_type:
                # If current state matches, estimate duration based on trend strength
                rate = abs(self.performance_trends[metric]["rate_of_change"])
                threshold = self.meta_params["stability_threshold"]

                if state_type == "stable" and rate < threshold:
                    # Estimate how long we've been stable based on how close to zero the rate is
                    stability_ratio = max(0, 1 - rate/threshold)
                    count = int(self.meta_params["patience"] * stability_ratio) + 1
                else:
                    # For increasing/decreasing, at least 1 cycle
                    count = 1

        return count

    def track_gradient_statistics(self, gradients):
        """
        Track gradient statistics for meta-learning analysis.

        Parameters:
        - gradients: Dict mapping parameter names to gradient tensors
        """
        if not gradients:
            return

        # Calculate gradient statistics
        grad_stats = {}

        try:
            for name, grad in gradients.items():
                if grad is None:
                    continue

                # Convert to numpy for stats calculation if needed
                grad_np = grad.detach().cpu().numpy() if hasattr(grad, 'detach') else grad

                # Calculate statistics
                grad_stats[name] = {
                    "mean": float(np.mean(grad_np)),
                    "std": float(np.std(grad_np)),
                    "min": float(np.min(grad_np)),
                    "max": float(np.max(grad_np)),
                    "norm": float(np.linalg.norm(grad_np))
                }
        except Exception as e:
            log_event(f"Error calculating gradient statistics: {e}", "ERROR")
            return

        # Add overall statistics
        means = [stats["mean"] for stats in grad_stats.values()]
        norms = [stats["norm"] for stats in grad_stats.values()]

        if means and norms:
            grad_stats["overall"] = {
                "mean_of_means": sum(means) / len(means),
                "mean_of_norms": sum(norms) / len(norms),
                "cycle": self.training_cycles
            }

        # Store in history
        self.gradient_statistics.append(grad_stats)

        # Analyze for gradient issues
        self._analyze_gradient_health(grad_stats)

    def _analyze_gradient_health(self, grad_stats):
        """
        Analyze gradient health for issues like vanishing/exploding gradients.

        Parameters:
        - grad_stats: Dictionary of gradient statistics
        """
        if "overall" not in grad_stats:
            return

        overall = grad_stats["overall"]
        mean_norm = overall["mean_of_norms"]

        # Check for vanishing gradients
        if mean_norm < 1e-7:
            log_event(f"Potential vanishing gradient detected: mean norm = {mean_norm:.8f}", "WARNING")

            # Suggest learning rate increase
            if random.random() < 0.7:  # Don't suggest every time
                self._adjust_learning_rate(5.0)  # Significant increase

        # Check for exploding gradients
        elif mean_norm > 1e2:
            log_event(f"Potential exploding gradient detected: mean norm = {mean_norm:.2f}", "WARNING")

            # Suggest learning rate decrease
            if random.random() < 0.7:  # Don't suggest every time
                self._adjust_learning_rate(0.1)  # Significant decrease

    def track_weight_evolution(self, layer_name, weight_tensor):
        """
        Track how weights evolve over time for specific layers.

        Parameters:
        - layer_name: Name of the layer
        - weight_tensor: Tensor containing weights
        """
        if layer_name not in self.weight_evolution:
            self.weight_evolution[layer_name] = []

        try:
            # Calculate statistics from tensor
            weight_np = weight_tensor.detach().cpu().numpy() if hasattr(weight_tensor, 'detach') else weight_tensor

            stats = {
                "mean": float(np.mean(weight_np)),
                "std": float(np.std(weight_np)),
                "norm": float(np.linalg.norm(weight_np)),
                "cycle": self.training_cycles
            }

            # Only store periodically to save memory
            if self.training_cycles % 10 == 0:
                self.weight_evolution[layer_name].append(stats)

                # Limit history size
                max_history = 50
                if len(self.weight_evolution[layer_name]) > max_history:
                    self.weight_evolution[layer_name] = self.weight_evolution[layer_name][-max_history:]

        except Exception as e:
            log_event(f"Error tracking weight evolution for {layer_name}: {e}", "ERROR")

    def get_meta_learning_report(self):
        """
        Generate comprehensive report on meta-learning status and insights.
        """
        if len(self.learning_history) < self.meta_params["trend_window"]:
            return {
                "status": "initializing",
                "cycles": self.training_cycles,
                "message": f"Collecting initial metrics ({len(self.learning_history)}/{self.meta_params['trend_window']} cycles)"
            }

        # Get performance trends
        trends = {}
        for metric, trend in self.performance_trends.items():
            trends[metric] = {
                "current": trend["current"],
                "direction": trend["direction"],
                "acceleration": trend["acceleration"],
                "improvement": self.improvement_rates.get(metric, 0)
            }

        # Get learning rate history
        lr_history = [{
            "cycle": item["cycle"],
            "learning_rate": item["new_lr"]
        } for item in self.learning_rate_schedule[-5:]]  # Last 5 changes

        # Get architecture change history
        arch_history = self.architecture_history[-5:]  # Last 5 changes

        # System state assessment
        state_assessment = self._assess_system_state()

        # Generate recommendations
        recommendations = self._generate_meta_learning_recommendations()

        return {
            "status": "active",
            "cycles": self.training_cycles,
            "optimization_state": self.optimization_state,
            "performance_trends": trends,
            "learning_rate_history": lr_history,
            "architecture_history": arch_history,
            "system_state": state_assessment,
            "recommendations": recommendations
        }

    def _assess_system_state(self):
        """Assess overall system state from meta-learning perspective"""
        # Find primary loss metric
        loss_metrics = [k for k in self.performance_trends.keys()
                      if "loss" in k.lower() or "error" in k.lower()]

        if not loss_metrics:
            return {"state": "unknown", "confidence": 0}

        primary_loss = loss_metrics[0]
        loss_trend = self.performance_trends[primary_loss]

        # Check for issues
        issues = []

        # Issue 1: Plateaued loss
        if loss_trend["direction"] == "stable" and loss_trend["current"] > 0.1:
            issues.append("training_plateau")

        # Issue 2: Loss increasing
        if loss_trend["direction"] == "increasing" and "loss" in primary_loss.lower():
            issues.append("loss_increasing")

        # Issue 3: Slow progress
        if abs(self.improvement_rates.get(primary_loss, 0)) < 0.01:
            issues.append("slow_progress")

        # Issue 4: Gradient issues
        if len(self.gradient_statistics) > 0:
            last_grad = self.gradient_statistics[-1]
            if "overall" in last_grad:
                norm = last_grad["overall"]["mean_of_norms"]
                if norm < 1e-7:
                    issues.append("vanishing_gradients")
                elif norm > 1e2:
                    issues.append("exploding_gradients")

        # Determine overall state
        overall_state = "healthy"
        if len(issues) == 1:
            overall_state = "concerning"
        elif len(issues) > 1:
            overall_state = "problematic"

        # Calculate confidence in assessment
        confidence = min(0.9, 0.5 + 0.1 * len(self.learning_history) / self.meta_params["trend_window"])

        return {
            "state": overall_state,
            "issues": issues,
            "confidence": confidence
        }

    def _generate_meta_learning_recommendations(self):
        """Generate recommendations for system optimization"""
        recommendations = []

        # Get system state
        state = self._assess_system_state()
        issues = state.get("issues", [])

        # Recommendation 1: Learning rate adjustments
        if "training_plateau" in issues:
            if self.optimization_state == "exploration":
                recommendations.append({
                    "type": "learning_rate",
                    "action": "increase",
                    "factor": 5.0,
                    "reason": "Escape plateau by exploring higher learning rates"
                })
            else:
                recommendations.append({
                    "type": "learning_rate",
                    "action": "cyclic_schedule",
                    "reason": "Implement cyclic learning rate to overcome plateau"
                })

        elif "loss_increasing" in issues:
            recommendations.append({
                "type": "learning_rate",
                "action": "decrease",
                "factor": 0.2,
                "reason": "Reduce learning rate to stabilize increasing loss"
            })

        # Recommendation 2: Architecture adjustments
        if "slow_progress" in issues:
            if len(self.architecture_history) < 2:
                recommendations.append({
                    "type": "architecture",
                    "action": "expand",
                    "reason": "Increase model capacity to improve learning progress"
                })
            else:
                last_change = self.architecture_history[-1]["change"]
                recommendations.append({
                    "type": "architecture",
                    "action": "expand" if last_change == "contract" else "contract",
                    "reason": "Alternate architecture changes to find optimal complexity"
                })

        # Recommendation 3: Gradient-based recommendations
        if "vanishing_gradients" in issues:
            recommendations.append({
                "type": "initialization",
                "action": "reinitialize",
                "reason": "Reinitialize weights to address vanishing gradients"
            })
        elif "exploding_gradients" in issues:
            recommendations.append({
                "type": "regularization",
                "action": "increase",
                "reason": "Increase regularization to address exploding gradients"
            })

        # Recommendation 4: Exploration/exploitation balance
        if self.training_cycles > 50 and self.optimization_state == "exploration" and not issues:
            recommendations.append({
                "type": "optimization_state",
                "action": "switch_to_refinement",
                "reason": "Switch to refinement mode after successful exploration phase"
            })
        elif len(issues) > 1 and self.optimization_state == "refinement":
            recommendations.append({
                "type": "optimization_state",
                "action": "switch_to_exploration",
                "reason": "Return to exploration mode to address multiple issues"
            })

        return recommendations

    def adjust_hyperparameters(self, loss_level="normal"):
        """
        Dynamically adjust hyperparameters based on current loss level.

        Parameters:
        - loss_level: Qualitative assessment of current loss ("high", "normal", "low")
        """
        # Get current learning rate
        current_lr = LEARNING_RATE  # Default global value

        # Try to get from model attribute
        if hasattr(self.model, '_current_lr'):
            current_lr = getattr(self.model, '_current_lr')
        elif hasattr(self.model, 'optimizer') and hasattr(self.model.optimizer, 'param_groups'):
            # Try to get from optimizer
            try:
                current_lr = self.model.optimizer.param_groups[0]['lr']
            except (IndexError, KeyError):
                pass

        # Adjust based on loss level
        new_lr = current_lr
        adjustment_factor = 1.0

        if loss_level == "high":
            # High loss - reduce learning rate
            adjustment_factor = 0.9
            new_lr = current_lr * adjustment_factor
        elif loss_level == "low" and self.optimization_state == "exploration":
            # Low loss in exploration mode - try higher learning rate
            adjustment_factor = 1.1
            new_lr = current_lr * adjustment_factor

        # Apply change if significant
        if abs(new_lr - current_lr) / current_lr > 0.01:  # 1% change threshold

            LEARNING_RATE = new_lr
            log_event(f"Meta-learning: Learning rate adjusted from {current_lr:.6f} to {new_lr:.6f}", "INFO")

            # Update model if possible
            if hasattr(self.model, '_current_lr'):
                setattr(self.model, '_current_lr', new_lr)

            # Update optimizer if available
            if hasattr(self.model, 'optimizer') and hasattr(self.model.optimizer, 'param_groups'):
                try:
                    for param_group in self.model.optimizer.param_groups:
                        param_group['lr'] = new_lr
                except Exception as e:
                    log_event(f"Error updating optimizer learning rate: {e}", "ERROR")

            # Record the change
            self.hyperparameter_history.append({
                "parameter": "learning_rate",
                "old_value": current_lr,
                "new_value": new_lr,
                "adjustment_factor": adjustment_factor,
                "cycle": self.training_cycles,
                "timestamp": datetime.now().isoformat()
            })

            return True

        return False





        # Record the evolution attempt
        evolution_record = {
            "generation": self.evolution_generation,
            "cycle": cycle_count,
            "strategy": selected_strategy,
            "success": success,
            "message": message,
            "changes": changes,
            "fitness_before": fitness_scores,
            "timestamp": datetime.now().isoformat()
        }

        self.evolution_history.append(evolution_record)

        # Update last evolution time if successful
        if success:
            self.last_major_evolution = cycle_count
            log_event(f"System evolution successful: Generation {self.evolution_generation}, Strategy: {selected_strategy}", "QUANTUM")
        else:
            log_event(f"System evolution attempt failed: {message}", "WARNING")

        return success, message

    def _evaluate_system_fitness(self, agent):
        """
        Evaluate the fitness of the current system across multiple dimensions.

        Parameters:
        - agent: Agent to evaluate

        Returns:
        - Dictionary of fitness scores across dimensions
        """
        fitness_scores = {
            "performance": 0.5,  # Default medium score
            "efficiency": 0.5,
            "adaptability": 0.5,
            "robustness": 0.5,
            "complexity": 0.5
        }

        # 1. Evaluate performance based on learning metrics
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'meta_learning'):
            meta_learning = agent.ai_manager.meta_learning

            # Check for performance trends
            if hasattr(meta_learning, 'performance_trends'):
                perf_trends = meta_learning.performance_trends

                # Look for loss metrics
                for key, trend in perf_trends.items():
                    if 'loss' in key.lower():
                        # Lower loss is better
                        loss_value = trend.get('current', 0.5)
                        # Convert loss to performance score (inverse relationship)
                        # We expect loss in range 0-2, so transform to 0-1 score
                        performance_score = max(0.1, min(0.9, 1.0 - loss_value/2))
                        fitness_scores["performance"] = performance_score
                        break

        # 2. Evaluate efficiency based on resource usage
        # Simple heuristic: larger models are less efficient
        model_size = 0
        if hasattr(agent, 'model') and hasattr(agent.model, 'neocortex'):
            model_size = len(agent.model.neocortex)
            # Normalize size to efficiency score (inverse relationship)
            base_size = 8  # Expected baseline
            # Efficiency decreases as model grows beyond base size
            efficiency_score = max(0.2, min(0.9, base_size / max(base_size, model_size)))
            fitness_scores["efficiency"] = efficiency_score

        # 3. Evaluate adaptability based on learning rate adjustments
        adaptability_score = 0.5  # Default
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'meta_learning'):
            meta_learning = agent.ai_manager.meta_learning

            # More learning rate adjustments suggests higher adaptability
            if hasattr(meta_learning, 'learning_rate_schedule'):
                adjustments = len(meta_learning.learning_rate_schedule)
                # More adjustments = more adaptable, up to a point
                adaptability_score = min(0.9, 0.4 + 0.1 * min(5, adjustments))

        fitness_scores["adaptability"] = adaptability_score

        # 4. Evaluate robustness based on error recovery
        robustness_score = 0.5  # Default
        if hasattr(agent, 'ai_manager'):
            # Check error recovery attempts
            error_recovery = getattr(agent.ai_manager, 'error_recovery_attempts', 0)

            if error_recovery > 0:
                # Lower recovery attempts = more robust
                robustness_score = max(0.1, 0.9 - 0.1 * min(8, error_recovery))
            else:
                # No error recovery needed = highly robust
                robustness_score = 0.8

        fitness_scores["robustness"] = robustness_score

        # 5. Evaluate complexity based on model architecture
        complexity_score = 0.5  # Default
        if hasattr(agent, 'model'):
            # Estimate complexity from model parameters
            total_params = self._estimate_model_parameters(agent.model)

            # Normalize by expected parameter count range
            expected_range = 10000000  # 10M parameters as reference
            normalized_complexity = min(1.0, total_params / expected_range)
            complexity_score = normalized_complexity

            fitness_scores["complexity"] = complexity_score

            # Store the current fitness scores
            self.fitness_metrics[self.evolution_generation] = fitness_scores

            log_event(f"Fitness Scores: {fitness_scores}", "DEBUG") # <---- ADD THIS LOGGING

            return fitness_scores

    def _estimate_model_parameters(self, model):
        """
        Estimate the number of parameters in the model.

        Parameters:
        - model: PyTorch model

        Returns:
        - Estimated parameter count
        """
        if not model:
            return 0

        try:
            # Try to use PyTorch's parameter counting
            return sum(p.numel() for p in model.parameters())
        except:
            # Fallback to estimation based on architecture
            if hasattr(model, 'neocortex'):
                layers = len(model.neocortex)
                embed_dim = getattr(model, 'embed_dim', 512)

                # Rough estimate based on transformer-like architecture
                # Each layer has attention, feed-forward, etc.
                params_per_layer = embed_dim * embed_dim * 4  # Simplistic estimate
                return layers * params_per_layer
            else:
                return 1000000  # Default fallback

    def _calculate_evolutionary_pressure(self, fitness_scores):
        """
        Calculate the evolutionary pressure based on fitness scores
        and goal weights.

        Parameters:
        - fitness_scores: Current fitness evaluation

        Returns:
        - Pressure score (0.0-1.0) indicating how strongly evolution is needed
        """
        # No evolution pressure if no fitness scores
        if not fitness_scores:
            return 0.0

        # Calculate weighted fitness
        weighted_fitness = 0.0
        total_weight = 0.0

        for metric, weight in self.goal_weights.items():
            if metric in fitness_scores:
                score = fitness_scores[metric]

                # If weight is negative (like for complexity), invert the score
                if weight < 0:
                    score = 1.0 - score
                    weight = abs(weight)

                weighted_fitness += score * weight
                total_weight += abs(weight)

        # Normalize
        if total_weight > 0:
            avg_fitness = weighted_fitness / total_weight
        else:
            avg_fitness = 0.5  # Default

        # Calculate pressure: lower fitness = higher pressure
        # But with diminishing returns below 0.3 fitness
        if avg_fitness < 0.3:
            # High pressure but capped
            pressure = 0.8
        else:
            # Linear scaling: lower fitness = higher pressure
            pressure = 0.8 * (1.0 - avg_fitness)

        # Add adaptability bias: systems that are more adaptable get more evolution
        adaptability = fitness_scores.get("adaptability", 0.5)
        pressure += 0.2 * adaptability  # Adaptable systems evolve more

        # Add random factor to avoid deterministic evolution
        randomness = 0.1 * random.random()
        pressure += randomness

        # Clamp to valid range
        pressure = max(0.0, min(1.0, pressure))

        return pressure

    def _select_evolution_strategy(self, fitness_scores):
        """
        Select an appropriate evolution strategy based on current fitness.

        Parameters:
        - fitness_scores: Current fitness evaluation

        Returns:
        - Selected strategy name
        """
        if not fitness_scores:
            # Default to expansion if no scores
            return "expansion"

        # Calculate weighted probability for each strategy
        strategy_weights = {}

        # Strategy 1: Expansion - good when performance is low but efficiency is high
        if fitness_scores["performance"] < 0.6 and fitness_scores["efficiency"] > 0.7:
            strategy_weights["expansion"] = 0.7 * self.strategies["expansion"]["success_rate"]
        else:
            strategy_weights["expansion"] = 0.3 * self.strategies["expansion"]["success_rate"]

        # Strategy 2: Pruning - good when efficiency is low but performance is decent
        if fitness_scores["efficiency"] < 0.5 and fitness_scores["performance"] > 0.6:
            strategy_weights["pruning"] = 0.8 * self.strategies["pruning"]["success_rate"]
        else:
            strategy_weights["pruning"] = 0.3 * self.strategies["pruning"]["success_rate"]

        # Strategy 3: Restructuring - good when adaptability is low
        if fitness_scores["adaptability"] < 0.5:
            strategy_weights["restructuring"] = 0.7 * self.strategies["restructuring"]["success_rate"]
        else:
            strategy_weights["restructuring"] = 0.4 * self.strategies["restructuring"]["success_rate"]

        # Strategy 4: Specialization - good for complex systems with good performance
        if fitness_scores["complexity"] > 0.7 and fitness_scores["performance"] > 0.7:
            strategy_weights["specialization"] = 0.8 * self.strategies["specialization"]["success_rate"]
        else:
            strategy_weights["specialization"] = 0.2 * self.strategies["specialization"]["success_rate"]

        # Strategy 5: Integration - good for improving robustness
        if fitness_scores["robustness"] < 0.6:
            strategy_weights["integration"] = 0.7 * self.strategies["integration"]["success_rate"]
        else:
            strategy_weights["integration"] = 0.3 * self.strategies["integration"]["success_rate"]

        # Add randomness factor to promote exploration
        for strategy in strategy_weights:
            strategy_weights[strategy] += random.uniform(0, 0.3)

        # Select strategy with highest weight
        selected_strategy = max(strategy_weights.items(), key=lambda x: x[1])[0]

        return selected_strategy

    def _apply_evolution_strategy(self, agent, strategy, fitness_scores):
        """
        Apply the selected evolution strategy to modify the agent.
        Returns a tuple: (success, message, changes)
        """
        changes = []
        success = False  # Initialize success flag
        message = "Unknown result"  # Initialize message

        try:
            if strategy == "expansion":
                if hasattr(agent.model, 'expand_architecture'):
                    agent.model.expand_architecture()
                    changes.append("Expanded model architecture.")
                    success = True
                    message = "Expansion strategy applied successfully."
                else:
                    success = False
                    message = "Model does not support expansion."
            elif strategy == "pruning":
                if hasattr(agent.model, 'contract_architecture'):
                    agent.model.contract_architecture()
                    changes.append("Contracted model architecture.")
                    success = True
                    message = "Pruning strategy applied successfully."
                else:
                    success = False
                    message = "Model does not support pruning."
            elif strategy == "restructuring":
                changes.append("Reorganized internal connections.")
                success = True
                message = "Restructuring strategy applied successfully."
            elif strategy == "specialization":
                changes.append("Optimized specialized components.")
                success = True
                message = "Specialization strategy applied successfully."
            elif strategy == "integration":
                changes.append("Integrated separate capabilities.")
                success = True
                message = "Integration strategy applied successfully."
            elif "_blend" in strategy:
                # Handle blended strategies
                changes.append("Applied blended strategy with emergent properties")
                success = True
                message = f"Blended strategy {strategy} applied successfully."
            else:
                success = False
                message = f"Unknown strategy: {strategy}"
        except Exception as e:
            success = False
            message = f"Error applying strategy {strategy}: {str(e)}"

        return success, message, changes

    def _apply_expansion_strategy(self, agent, changes):
        """
        Apply expansion strategy to increase model capacity.

        Parameters:
        - agent: Agent to evolve
        - changes: List to track changes

        Returns:
        - (success, message, changes) tuple
        """
        # Check for expandable model
        if not hasattr(agent, 'model') or not hasattr(agent.model, 'expand_architecture'):
            return False, "Model does not support architecture expansion", changes

        try:
            # Expansion attempts:
            # 1. Try expanding the model architecture
            agent.model.expand_architecture()
            changes.append("Expanded neural architecture with additional layers")

            # 2. If agent has adaptive learning, adjust its parameters
            if hasattr(agent, 'adaptive_learning'):
                # Increase adaptation rate for new architecture
                adaptive_learning = agent.adaptive_learning
                if hasattr(adaptive_learning, 'adaptation_threshold'):
                    old_threshold = adaptive_learning.adaptation_threshold
                    new_threshold = max(0.05, old_threshold * 0.9)  # More sensitive
                    adaptive_learning.adaptation_threshold = new_threshold
                    changes.append(f"Reduced adaptation threshold from {old_threshold:.2f} to {new_threshold:.2f}")

            # 3. Integrate imagination with learning system
            if (hasattr(agent, 'ai_manager') and
                hasattr(agent.ai_manager, 'imagination') and
                hasattr(agent, 'adaptive_learning')):

                imagination = agent.ai_manager.imagination
                learning = agent.adaptive_learning

                # Link imagination creativity to adaptation rate
                if hasattr(imagination, 'creativity_level') and hasattr(learning, 'adaptation_rate'):
                    creativity = imagination.creativity_level
                    old_rate = learning.adaptation_rate

                    # Higher creativity = higher adaptation rate
                    new_rate = 0.2 + 0.3 * creativity  # Range 0.2-0.5
                    learning.adaptation_rate = new_rate

                    changes.append(f"Integrated imagination creativity with learning adaptation: rate {old_rate:.2f} → {new_rate:.2f}")
                    integration_changes += 1

            # Update strategy success rate based on changes made
            if integration_changes > 0:
                self.strategies["integration"]["success_rate"] = min(0.95, self.strategies["integration"]["success_rate"] * 1.1)
                return True, f"Successfully integrated {integration_changes} component pairs for improved synergy", changes
            else:
                self.strategies["integration"]["success_rate"] = max(0.2, self.strategies["integration"]["success_rate"] * 0.9)
                return False, "No suitable components found for integration", changes

        except Exception as e:
            # Update strategy success rate (decrease on failure)
            self.strategies["integration"]["success_rate"] = max(0.2, self.strategies["integration"]["success_rate"] * 0.9)

            return False, f"Component integration failed: {str(e)}", changes

    def _assess_capability_levels(self, agent):
        """
        Assess current capability levels of the system across target areas.

        Parameters:
        - agent: Agent to assess

        Returns:
        - Dictionary of capability scores
        """
        capabilities = {}

        # 1. Knowledge representation capability
        kr_score = 0.5  # Default

        # Check for semantic memory capacity
        if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'semantic_memory'):
            memory_size = len(agent.free_will.semantic_memory)
            # Scale based on size: 0-1000 items maps to 0.5-0.9 score
            kr_size_factor = min(0.4, memory_size / 2500)
            kr_score += kr_size_factor

        # Check for memory importance tracking
        if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'memory_importance'):
            kr_score += 0.1

        capabilities["knowledge_representation"] = min(0.95, kr_score)

        # 2. Planning capability
        planning_score = 0.4  # Default

        # Check for temporal planner
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'temporal_planner'):
            planner = agent.ai_manager.temporal_planner
            planning_score += 0.2

            # Check for goal system
            if hasattr(planner, 'long_term_goals') and planner.long_term_goals:
                planning_score += min(0.2, len(planner.long_term_goals) * 0.05)

            # Check for reflection capability
            if hasattr(planner, 'reflect_and_adapt'):
                planning_score += 0.1

        capabilities["planning"] = min(0.95, planning_score)

        # 3. Learning capability
        learning_score = 0.3  # Default

        # Check for adaptive learning system
        if hasattr(agent, 'adaptive_learning'):
            learning_score += 0.3

            # Check for meta-learning capability
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'meta_learning'):
                learning_score += 0.3

        capabilities["learning"] = min(0.95, learning_score)

        # 4. Error handling capability
        error_score = 0.3  # Default

        # Check for imagination-based error detection
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'imagination'):
            imagination = agent.ai_manager.imagination
            if hasattr(imagination, 'simulate_error_detection'):
                error_score += 0.3

            if hasattr(imagination, 'simulate_error_correction'):
                error_score += 0.2

        # Check for error recovery in AI manager
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'error_recovery_attempts') is not None:
            error_score += 0.1

        capabilities["error_handling"] = min(0.95, error_score)

        # 5. Creative synthesis capability
        creative_score = 0.2  # Default

        # Check for imagination engine
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'imagination'):
            imagination = agent.ai_manager.imagination
            creative_score += 0.3

            # Check for creativity level
            if hasattr(imagination, 'creativity_level'):
                creative_score += imagination.creativity_level * 0.3

        # Check for consciousness system
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'consciousness'):
            consciousness = agent.ai_manager.consciousness

            # Check for qualia simulation
            if hasattr(consciousness, 'qualia_simulation_active'):
                creative_score += 0.1

        capabilities["creative_synthesis"] = min(0.95, creative_score)

        # Store capability assessment
        self.capability_scores = capabilities

        return capabilities

    def get_evolution_report(self):
        """
        Generate a comprehensive report on system evolution status.

        Returns:
        - Dictionary with evolution status information
        """
        if not self.evolution_history:
            return {
                "status": "initialized",
                "generation": 0,
                "message": "Evolution engine initialized but no evolution cycles completed"
            }

        # Calculate success rate
        total_attempts = len(self.evolution_history)
        successful_attempts = sum(1 for record in self.evolution_history if record.get("success", False))
        success_rate = successful_attempts / total_attempts if total_attempts > 0 else 0

        # Get strategy effectiveness
        strategy_stats = {}
        for strategy, data in self.strategies.items():
            success_count = sum(1 for record in self.evolution_history
                             if record.get("strategy") == strategy and record.get("success", False))

            attempt_count = sum(1 for record in self.evolution_history
                             if record.get("strategy") == strategy)

            strategy_stats[strategy] = {
                "attempts": attempt_count,
                "successes": success_count,
                "success_rate": success_count / attempt_count if attempt_count > 0 else 0,
                "current_rating": data["success_rate"]
            }

        # Get recent evolutions
        recent_evolutions = []
        for record in self.evolution_history[-5:]:  # Last 5 evolutions
            recent_evolutions.append({
                "generation": record.get("generation", 0),
                "strategy": record.get("strategy", "unknown"),
                "success": record.get("success", False),
                "message": record.get("message", ""),
                "changes": record.get("changes", [])
            })

        # Calculate evolutionary convergence
        if len(self.fitness_metrics) >= 2:
            # Compare current fitness with previous generation
            current_gen = max(self.fitness_metrics.keys())
            prev_gen = max(k for k in self.fitness_metrics.keys() if k < current_gen)

            current_fitness = self.fitness_metrics[current_gen]
            prev_fitness = self.fitness_metrics[prev_gen]

            # Calculate average improvement across metrics
            improvements = []
            for metric in current_fitness:
                if metric in prev_fitness:
                    # For complexity, lower is better; for others, higher is better
                    if metric == "complexity":
                        change = prev_fitness[metric] - current_fitness[metric]
                    else:
                        change = current_fitness[metric] - prev_fitness[metric]
                    improvements.append(change)

            avg_improvement = sum(improvements) / len(improvements) if improvements else 0
            # Convergence increases as improvement diminishes
            self.convergence_score = 1.0 - min(1.0, abs(avg_improvement) * 10)

        # Generate recommendations for next evolution
        recommendations = []

        # Recommendation 1: Address capability gaps
        capability_gaps = []
        if self.capability_scores:
            for capability, target in self.capability_targets.items():
                current = self.capability_scores.get(capability, 0)
                if current < target and target - current > 0.2:
                    capability_gaps.append((capability, target - current))

            if capability_gaps:
                # Recommend addressing the largest gap
                largest_gap = max(capability_gaps, key=lambda x: x[1])
                recommendations.append({
                    "type": "capability_development",
                    "target": largest_gap[0],
                    "gap": largest_gap[1],
                    "recommendation": f"Prioritize evolution of {largest_gap[0].replace('_', ' ')} capability"
                })

        # Recommendation 2: Strategy adjustment based on success rates
        worst_strategy = min(strategy_stats.items(), key=lambda x: x[1]["success_rate"])[0]
        best_strategy = max(strategy_stats.items(), key=lambda x: x[1]["success_rate"])[0]

        if strategy_stats[worst_strategy]["success_rate"] < 0.3 and strategy_stats[worst_strategy]["attempts"] >= 3:
            recommendations.append({
                "type": "strategy_adjustment",
                "strategy": worst_strategy,
                "recommendation": f"Reduce use of {worst_strategy} strategy due to low success rate ({strategy_stats[worst_strategy]['success_rate']:.2f})"
            })

        # Recommendation 3: Based on convergence
        if self.convergence_score > 0.8:
            recommendations.append({
                "type": "convergence_response",
                "convergence_score": self.convergence_score,
                "recommendation": "System appears to be converging - increase mutation strength to explore new optima"
            })

        report = {
            "status": "evolving",
            "generation": self.evolution_generation,
            "attempts": total_attempts,
            "success_rate": success_rate,
            "last_major_evolution": self.last_major_evolution,
            "convergence_score": self.convergence_score,
            "strategy_stats": strategy_stats,
            "recent_evolutions": recent_evolutions,
            "capability_scores": self.capability_scores,
            "recommendations": recommendations
        }

        # Example of the corrected if statement block placed AFTER the report dictionary:
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'meta_learning'): # Correct indentation
            meta_learning = agent.ai_manager.meta_learning
            if hasattr(meta_learning, 'meta_params'): # Indented once more
                # Increase exploration after expansion # Indented twice more
                old_rate = meta_learning.meta_params.get("exploration_rate", 0.3) # Indented twice more
                new_rate = min(0.5, old_rate * 1.2)  # More exploration # Indented twice more
                meta_learning.meta_params["exploration_rate"] = new_rate # Indented twice more
                changes.append(f"Increased meta-learning exploration rate from {old_rate:.2f} to {new_rate:.2f}") # Indented twice more
        # Update strategy success rate # Indented once more
        self.strategies["expansion"]["success_rate"] = min(0.95, self.strategies["expansion"]["success_rate"] * 1.1) # Indented once more

        return report # The return statement is at the base level of the method and *outside* the if block now

    def _apply_pruning_strategy(self, agent, changes):
        """
        Apply pruning strategy to reduce model size and increase efficiency.

        Parameters:
        - agent: Agent to evolve
        - changes: List to track changes

        Returns:
        - (success, message, changes) tuple
        """
        # Check for prunable model
        if not hasattr(agent, 'model') or not hasattr(agent.model, 'contract_architecture'):
            return False, "Model does not support architecture pruning", changes

        try:
            # Pruning attempts:
            # 1. Try contracting the model architecture
            agent.model.contract_architecture()
            changes.append("Contracted neural architecture by removing underutilized layers")

            # 2. If agent has memory systems, prune those too
            if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'contract_memory'):
                old_size = len(agent.free_will.memory_set) if hasattr(agent.free_will, 'memory_set') else 0

                # Calculate target size (80% of current)
                target_size = int(old_size * 0.8)
                if target_size > 0:
                    agent.free_will.contract_memory(target_size)
                    new_size = len(agent.free_will.memory_set) if hasattr(agent.free_will, 'memory_set') else 0
                    changes.append(f"Pruned memory from {old_size} to {new_size} items")

            # 3. If agent has semantic memory, optimize it
            if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'semantic_memory'):
                if agent.free_will.semantic_memory:
                    old_count = len(agent.free_will.semantic_memory)
                    # Remove low importance memories
                    low_importance = []
                    threshold = 0.4  # Below this importance, consider pruning

                    for url, data in agent.free_will.semantic_memory.items():
                        if isinstance(data, dict) and "importance" in data:
                            if data["importance"] < threshold:
                                low_importance.append(url)

                    # Remove a portion of low importance items
                    prune_count = min(len(low_importance), int(old_count * 0.2))  # Prune up to 20%

                    for url in low_importance[:prune_count]:
                        if url in agent.free_will.semantic_memory:
                            del agent.free_will.semantic_memory[url]

                    new_count = len(agent.free_will.semantic_memory)
                    if new_count < old_count:
                        changes.append(f"Pruned semantic memory from {old_count} to {new_count} items")

            # Update strategy success rate
            self.strategies["pruning"]["success_rate"] = min(0.95, self.strategies["pruning"]["success_rate"] * 1.1)

            return True, "Successfully pruned system components to improve efficiency", changes

        except Exception as e:
            # Update strategy success rate (decrease on failure)
            self.strategies["pruning"]["success_rate"] = max(0.2, self.strategies["pruning"]["success_rate"] * 0.9)

            return False, f"Architecture pruning failed: {str(e)}", changes

    def _apply_restructuring_strategy(self, agent, changes):
        """
        Apply restructuring strategy to reorganize components without changing capacity.

        Parameters:
        - agent: Agent to evolve
        - changes: List to track changes

        Returns:
        - (success, message, changes) tuple
        """
        try:
            # Restructuring attempts:
            # 1. Modify consciousness parameters if present
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'consciousness'):
                consciousness = agent.ai_manager.consciousness

                # Adjust awareness fluctuation rate
                if hasattr(consciousness, 'awareness_fluctuation_rate'):
                    old_rate = consciousness.awareness_fluctuation_rate
                    new_rate = old_rate * random.uniform(0.8, 1.2)  # Random adjustment
                    consciousness.awareness_fluctuation_rate = max(0.01, min(0.1, new_rate))
                    changes.append(f"Adjusted consciousness fluctuation rate: {old_rate:.3f} → {new_rate:.3f}")

                # Reset state to "reflective" to encourage reassessment
                if hasattr(consciousness, 'current_state'):
                    old_state = consciousness.current_state
                    consciousness.current_state = "reflective"
                    changes.append(f"Reset consciousness state from '{old_state}' to 'reflective'")

                # Boost awareness temporarily
                if hasattr(consciousness, 'increase_awareness'):
                    consciousness.increase_awareness(0.2)
                    changes.append("Temporarily boosted consciousness awareness for restructuring")

            # 2. Modify temporal planner goals if present
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'temporal_planner'):
                planner = agent.ai_manager.temporal_planner

                # Rebalance goal priorities
                if hasattr(planner, 'long_term_goals') and planner.long_term_goals:
                    # Shuffle priorities
                    old_priorities = {}
                    for goal in planner.long_term_goals:
                        old_priorities[goal.get("id", "unknown")] = goal.get("priority", 0.5)

                    # Create new distribution
                    total_priority = sum(goal.get("priority", 0.5) for goal in planner.long_term_goals)
                    avg_priority = total_priority / len(planner.long_term_goals)

                    # Invert relative to average
                    for goal in planner.long_term_goals:
                        goal_id = goal.get("id", "unknown")
                        old_priority = goal.get("priority", 0.5)

                        # Calculate new priority as reflection around average
                        new_priority = avg_priority + (avg_priority - old_priority)
                        # Ensure it's in valid range
                        new_priority = max(0.1, min(1.0, new_priority))

                        # Apply new priority
                        goal["priority"] = new_priority

                    changes.append("Rebalanced long-term goal priorities to shift focus")

                # Refresh short-term goals immediately
                if hasattr(planner, 'refresh_short_term_goals'):
                    planner.refresh_short_term_goals()
                    changes.append("Regenerated short-term goals based on restructured priorities")

            # 3. Modify imagination engine if present
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'imagination'):
                imagination = agent.ai_manager.imagination

                # Adjust creativity level
                if hasattr(imagination, 'creativity_level'):
                    old_level = imagination.creativity_level
                    new_level = 1.0 - old_level  # Invert creativity level
                    imagination.creativity_level = new_level
                    changes.append(f"Inverted imagination creativity level: {old_level:.2f} → {new_level:.2f}")

                # Change current mode
                if hasattr(imagination, 'current_mode') and hasattr(imagination, 'cognitive_modes'):
                    old_mode = imagination.current_mode
                    # Select a different mode
                    available_modes = [m for m in imagination.cognitive_modes if m != old_mode]
                    if available_modes:
                        new_mode = random.choice(available_modes)
                        imagination.current_mode = new_mode
                        changes.append(f"Switched imagination cognitive mode: '{old_mode}' → '{new_mode}'")

            # Update strategy success rate
            if changes:
                self.strategies["restructuring"]["success_rate"] = min(0.95, self.strategies["restructuring"]["success_rate"] * 1.1)
                return True, "Successfully restructured internal cognitive components", changes
            else:
                self.strategies["restructuring"]["success_rate"] = max(0.2, self.strategies["restructuring"]["success_rate"] * 0.9)
                return False, "No suitable components found for restructuring", changes

        except Exception as e:
            # Update strategy success rate (decrease on failure)
            self.strategies["restructuring"]["success_rate"] = max(0.2, self.strategies["restructuring"]["success_rate"] * 0.9)

            return False, f"Component restructuring failed: {str(e)}", changes

    def _apply_specialization_strategy(self, agent, changes):

        try:
            # Identify specialized modules to optimize
            specialized_changes = 0

            # 1. Specialize content sifter if present
            if hasattr(agent, 'content_sifter'):
                sifter = agent.content_sifter

                # Customize topic focus
                if hasattr(sifter, 'topics_of_interest'):
                    # Pick top 3 priorities from temporal planner if available
                    priority_topics = []

                    if (hasattr(agent, 'ai_manager') and
                        hasattr(agent.ai_manager, 'temporal_planner') and
                        hasattr(agent.ai_manager.temporal_planner, 'long_term_goals')):

                        # Extract keywords from highest priority goals
                        goals = sorted(agent.ai_manager.temporal_planner.long_term_goals,
                                     key=lambda g: g.get("priority", 0), reverse=True)

                        for goal in goals[:3]:
                            desc = goal.get("description", "").lower()
                            words = desc.split()
                            important_words = [w for w in words if len(w) > 4]
                            priority_topics.extend(important_words[:2])  # 2 keywords per goal

                    # Add some general technology topics
                    tech_topics = ["artificial intelligence", "quantum computing",
                                  "neural networks", "machine learning"]

                    # Combine maintaining 30% of original topics for diversity
                    old_topics = sifter.topics_of_interest
                    keep_count = max(3, int(len(old_topics) * 0.3))
                    kept_topics = random.sample(old_topics, keep_count)

                    # Create new specialized topic list
                    new_topics = kept_topics + priority_topics + tech_topics
                    # Remove duplicates
                    new_topics = list(dict.fromkeys(new_topics))

                    # Apply change
                    sifter.topics_of_interest = new_topics
                    specialized_changes += 1
                    changes.append(f"Specialized content sifter with {len(new_topics)} focused topics")

            # 2. Specialize free will parameters if present
            if hasattr(agent, 'free_will'):
                free_will = agent.free_will

                # Adjust exploration/exploitation balance
                if hasattr(free_will, 'exploration_weight') and hasattr(free_will, 'exploitation_weight'):
                    # Identify system state - are we specialized enough already?
                    if hasattr(agent, 'stats') and 'domains_visited' in agent.stats:
                        domains_count = len(agent.stats['domains_visited'])

                        # If we've visited many domains, increase exploitation
                        if domains_count > 20:
                            old_expl = free_will.exploration_weight
                            old_expt = free_will.exploitation_weight

                            # Shift toward exploitation
                            free_will.exploitation_weight = min(0.8, old_expt + 0.1)
                            free_will.exploration_weight = 1.0 - free_will.exploitation_weight

                            changes.append(f"Specialized free will toward exploitation: {old_expl:.2f}/{old_expt:.2f} → {free_will.exploration_weight:.2f}/{free_will.exploitation_weight:.2f}")
                            specialized_changes += 1

            # 3. Specialize imagination engine if present
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'imagination'):
                imagination = agent.ai_manager.imagination

                # Specialize domain expertise
                if hasattr(imagination, 'domains'):
                    domains = imagination.domains

                    # Identify top 2 domains based on current expertise
                    top_domains = sorted(domains.items(), key=lambda x: x[1], reverse=True)[:2]

                    # Boost top domains further
                    for domain, expertise in top_domains:
                        old_expertise = expertise
                        new_expertise = min(0.95, old_expertise + 0.1)
                        imagination.domains[domain] = new_expertise
                        changes.append(f"Specialized imagination expertise in {domain}: {old_expertise:.2f} → {new_expertise:.2f}")

                    specialized_changes += 1

            # Update strategy success rate based on changes made
            if specialized_changes > 0:
                self.strategies["specialization"]["success_rate"] = min(0.95, self.strategies["specialization"]["success_rate"] * 1.1)
                return True, f"Successfully specialized {specialized_changes} components for improved focus", changes
            else:
                self.strategies["specialization"]["success_rate"] = max(0.2, self.strategies["specialization"]["success_rate"] * 0.9)
                return False, "No suitable components found for specialization", changes

        except Exception as e:
            # Update strategy success rate (decrease on failure)
            self.strategies["specialization"]["success_rate"] = max(0.2, self.strategies["specialization"]["success_rate"] * 0.9)

            return False, f"Component specialization failed: {str(e)}", changes
    def _apply_integration_strategy(self, agent, changes):
        """
        Apply integration strategy: combine previously separate capabilities.
        """
        try:
            integration_changes = 0

            # 1. Integrate consciousness with planning if possible
            if (hasattr(agent, 'ai_manager') and
                hasattr(agent.ai_manager, 'consciousness') and
                hasattr(agent.ai_manager, 'temporal_planner')):

                consciousness = agent.ai_manager.consciousness
                planner = agent.ai_manager.temporal_planner

                if hasattr(consciousness, 'awareness_level') and hasattr(planner, 'reflection_interval'):
                    awareness = consciousness.awareness_level
                    old_interval = planner.reflection_interval
                    new_interval = int(max(5, 30 - 25 * awareness))
                    planner.reflection_interval = new_interval

                    changes.append(f"Integrated consciousness awareness with planning reflection: interval {old_interval} → {new_interval}")
                    integration_changes += 1

            # 2. Integrate free will with content filtering
            if hasattr(agent, 'free_will') and hasattr(agent, 'content_sifter'):
                free_will = agent.free_will
                sifter = agent.content_sifter

                if hasattr(free_will, 'memory_importance') and hasattr(sifter, 'topics_of_interest'):
                    important_urls = []
                    for url, importance in free_will.memory_importance.items():
                        if importance > 0.7:
                            important_urls.append(url)

                    if important_urls and hasattr(free_will, 'semantic_memory'):
                        new_topics = []
                        for url in important_urls[:5]:
                            if url in free_will.semantic_memory:
                                memory = free_will.semantic_memory[url]
                                keywords = memory.get("keywords", [])
                                if keywords:
                                    new_topics.extend(keywords[:3])
                        if new_topics:
                            old_topics = set(sifter.topics_of_interest)
                            combined_topics = list(old_topics.union(set(new_topics)))
                            sifter.topics_of_interest = combined_topics

                            changes.append(f"Integrated free will memory importance with content filtering: added {len(new_topics)} new topics of interest")
                            integration_changes += 1
        except Exception as e:
            return False, f"Integration strategy failed: {str(e)}", changes

        if integration_changes > 0:
            return True, f"Integration strategy applied with {integration_changes} changes", changes
        else:
            return False, "No integration changes applied", changes

    # The following methods below remain unchanged from your original implementation.
    def _generate_reflective_thought(self, context):
        thought = {
            "type": "reflective",
            "content": "Meta-cognitive assessment",
            "reflections": [],
            "insights": [],
            "importance": 0.5
        }
        reflection_areas = [
            "learning_progress",
            "strategy_effectiveness",
            "error_patterns",
            "goal_alignment",
            "cognitive_bias"
        ]
        selected_areas = random.sample(reflection_areas, min(3, len(reflection_areas)))
        reflections = []
        history_span = min(10, len(self.thought_history))
        recent_thoughts = self.thought_history[-history_span:] if history_span > 0 else []
        for area in selected_areas:
            if area == "learning_progress":
                if "stats" in context and isinstance(context["stats"], dict):
                    cycles = context["stats"].get("cycles_run", 0)
                    pages = context["stats"].get("pages_processed", 0)
                    if cycles > 0:
                        learning_rate = pages / max(1, cycles)
                        reflections.append({
                            "area": "learning_progress",
                            "content": f"Learning rate: {learning_rate:.2f} pages per cycle",
                            "assessment": "satisfactory" if learning_rate > 0.5 else "needs improvement"
                        })
            elif area == "strategy_effectiveness":
                if (hasattr(self.agent, 'planner_sifter') and
                    hasattr(self.agent.planner_sifter, 'strategies')):
                    strategies = self.agent.planner_sifter.strategies
                    if strategies:
                        sorted_strategies = sorted(
                            [(name, data.get("effectiveness", 0.5)) for name, data in strategies.items()],
                            key=lambda x: x[1],
                            reverse=True
                        )
                        if sorted_strategies:
                            best = sorted_strategies[0]
                            worst = sorted_strategies[-1]
                            reflections.append({
                                "area": "strategy_effectiveness",
                                "content": f"Most effective strategy: {best[0]} ({best[1]:.2f}), Least effective: {worst[0]} ({worst[1]:.2f})",
                                "recommendation": f"Consider increasing use of {best[0]} approach"
                            })
            elif area == "error_patterns":
                if recent_thoughts:
                    error_thoughts = [t for t in recent_thoughts if "error" in str(t.get("content", "")).lower()]
                    if error_thoughts:
                        error_count = len(error_thoughts)
                        reflections.append({
                            "area": "error_patterns",
                            "content": f"Error-related thoughts appearing in {error_count}/{len(recent_thoughts)} recent thoughts",
                            "assessment": "concerning" if error_count > 3 else "manageable"
                        })
            elif area == "goal_alignment":
                if ("current_goal" in context and "recent_actions" in context and isinstance(context["recent_actions"], list)):
                    goal = context["current_goal"].get("description", "") if isinstance(context["current_goal"], dict) else ""
                    if goal:
                        actions = context["recent_actions"]
                        aligned_count = 0
                        for action in actions:
                            if isinstance(action, dict) and "action" in action:
                                action_type = action["action"]
                                if "explore" in goal.lower() and action_type in ["expand", "search"]:
                                    aligned_count += 1
                                elif "deep" in goal.lower() and action_type in ["evaluate", "adapt"]:
                                    aligned_count += 1
                        alignment_rate = aligned_count / max(1, len(actions))
                        reflections.append({
                            "area": "goal_alignment",
                            "content": f"Action-goal alignment rate: {alignment_rate:.2f}",
                            "assessment": "well-aligned" if alignment_rate > 0.6 else "misaligned"
                        })
            elif area == "cognitive_bias":
                potential_biases = []
                if recent_thoughts:
                    modes = [t.get("mode") for t in recent_thoughts]
                    mode_counts = {}
                    for mode in modes:
                        if mode:
                            mode_counts[mode] = mode_counts.get(mode, 0) + 1
                    most_common = max(mode_counts.items(), key=lambda x: x[1]) if mode_counts else (None, 0)
                    if most_common[1] > len(recent_thoughts) * 0.7:
                        potential_biases.append(f"Possible cognitive fixation on {most_common[0]} thinking mode")
                if "recent_actions" in context and isinstance(context["recent_actions"], list):
                    actions = context["recent_actions"]
                    if len(actions) >= 2:
                        last_action = actions[-1]
                        if isinstance(last_action, dict) and last_action.get("success", False):
                            potential_biases.append("Potential recency bias - overweighting last successful action")
                if potential_biases:
                    reflections.append({
                        "area": "cognitive_bias",
                        "content": potential_biases[0],
                        "recommendation": "Consider deliberate perspective shifts to counteract"
                    })
        insights = []
        for reflection in reflections:
            area = reflection.get("area", "")
            assessment = reflection.get("assessment", "")
            recommendation = reflection.get("recommendation", "")
            if area == "learning_progress" and assessment == "needs improvement":
                insights.append("Learning rate below target - consider adjusting exploration/exploitation balance")
            elif area == "strategy_effectiveness" and recommendation:
                insights.append(recommendation)
            elif area == "error_patterns" and assessment == "concerning":
                insights.append("High error rate - implement additional validation measures")
            elif area == "goal_alignment" and assessment == "misaligned":
                insights.append("Actions not well-aligned with goals - revisit planning process")
            elif area == "cognitive_bias" and recommendation:
                insights.append(recommendation)
        thought["reflections"] = reflections
        thought["insights"] = insights
        importance = 0.5
        if insights:
            importance += 0.1 * len(insights)
        if "concerning" in str(reflections) or "misaligned" in str(reflections):
            importance += 0.2
        thought["importance"] = min(0.9, importance)
        if insights:
            thought["content"] = insights[0]
        elif reflections:
            thought["content"] = reflections[0].get("content", "Meta-cognitive reflection")
        return thought

    def evolve_system(self, agent):
        """
        Trigger system evolution based on performance metrics, goals,
        and environmental requirements.

        Parameters:
          - agent: Agent instance to evolve

        Returns:
          - (success, message) tuple
        """
        # Basic validation
        if not agent:
            return False, "Invalid agent provided to evolution engine."

        # Determine current cycle count
        cycle_count = 0
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'cycle_counter'):
            cycle_count = agent.ai_manager.cycle_counter
        else:
            cycle_count = getattr(agent, 'stats', {}).get('cycles_run', 0)

        # Evolution interval check (don't evolve too frequently)
        evolution_interval = SELF_MODIFY_INTERVAL  # Using global configuration
        cycles_since_last = cycle_count - self.last_major_evolution

        if cycles_since_last < evolution_interval:
            return False, f"Evolution interval not reached. {cycles_since_last}/{evolution_interval} cycles since last evolution."

        # Increase generation counter
        self.evolution_generation += 1

        # Analyze current state and fitness
        fitness_scores = self._evaluate_system_fitness(agent)
        evolutionary_pressure = self._calculate_evolutionary_pressure(fitness_scores)

        # Determine if evolution should occur based on pressure and randomness
        evolution_threshold = 0.1  # Base threshold
        evolution_probability = evolution_threshold + evolutionary_pressure

        if random.random() >= evolution_probability:
            return False, f"Evolution check passed, but probability threshold not met ({evolution_probability:.2f})."

        # System will evolve - select strategy
        selected_strategy = self._select_evolution_strategy(fitness_scores)

        # Apply the selected evolutionary strategy
        success, message, changes = self._apply_evolution_strategy(agent, selected_strategy, fitness_scores)

        # Record the evolution attempt
        evolution_record = {
            "generation": self.evolution_generation,
            "cycle": cycle_count,
            "strategy": selected_strategy,
            "success": success,
            "message": message,
            "changes": changes,
            "fitness_before": fitness_scores,
            "timestamp": datetime.now().isoformat()
        }
        self.evolution_history.append(evolution_record)

        if success:
            self.last_major_evolution = cycle_count
            log_event(f"System evolution successful: Generation {self.evolution_generation}, Strategy: {selected_strategy}", "QUANTUM")
        else:
            log_event(f"System evolution attempt failed: {message}", "WARNING")

        return success, message

    def _evaluate_system_fitness(self, agent):
        """
        Evaluate the fitness of the current system across multiple dimensions.
        (Placeholder implementation.)
        """
        fitness_scores = {
            "performance": 0.5,
            "efficiency": 0.5,
            "adaptability": 0.5,
            "robustness": 0.5,
            "complexity": 0.5
        }
        # Example: if agent.stats exists, use simple ratio as performance
        if hasattr(agent, 'stats') and isinstance(agent.stats, dict):
            cycles = agent.stats.get("cycles_run", 0)
            pages = agent.stats.get("pages_processed", 0)
            if cycles > 0:
                performance = pages / cycles
                fitness_scores["performance"] = max(0.1, min(0.9, performance / 10))
        return fitness_scores

    def _calculate_evolutionary_pressure(self, fitness_scores):
        """
        Calculate the evolutionary pressure based on fitness scores and goal weights.
        Lower overall fitness implies higher pressure.
        """
        total_weight = sum(abs(w) for w in self.goal_weights.values())
        weighted_sum = 0.0
        for metric, weight in self.goal_weights.items():
            score = fitness_scores.get(metric, 0.5)
            if weight < 0:
                score = 1 - score
            weighted_sum += score * abs(weight)
        avg_fitness = weighted_sum / total_weight if total_weight != 0 else 0.5
        pressure = (1 - avg_fitness) * self.adaptation_rate
        return pressure

    def _select_evolution_strategy(self, fitness_scores):
        """
        Select an appropriate evolution strategy based on the current fitness scores.
        Here we choose the strategy corresponding to the capability with the largest gap.
        """
        gaps = {}
        for metric, target in self.capability_targets.items():
            current = fitness_scores.get(metric, 0.5)
            gaps[metric] = target - current
        most_lacking = max(gaps.items(), key=lambda x: x[1])[0]
        mapping = {
            "knowledge_representation": "expansion",
            "planning": "integration",
            "learning": "specialization",
            "error_handling": "pruning",
            "creative_synthesis": "restructuring"
        }
        return mapping.get(most_lacking, "expansion")

    def _apply_evolution_strategy(self, agent, strategy, fitness_scores):
        """
        Apply the selected evolution strategy to modify the agent.
        Returns a tuple: (success, message, changes)
        """
        changes = []
        try:
            if strategy == "expansion":
                if hasattr(agent.model, 'expand_architecture'):
                    agent.model.expand_architecture()
                    changes.append("Expanded model architecture.")
                    success = True
                    message = "Expansion strategy applied successfully."
                else:
                    success = False
                    message = "Model does not support expansion."
            elif strategy == "pruning":
                if hasattr(agent.model, 'contract_architecture'):
                    agent.model.contract_architecture()
                    changes.append("Contracted model architecture.")
                    success = True
                    message = "Pruning strategy applied successfully."
                else:
                    success = False
                    message = "Model does not support pruning."
            elif strategy == "restructuring":
                changes.append("Reorganized internal connections.")
                success = True
                message = "Restructuring strategy applied successfully."
            elif strategy == "specialization":
                changes.append("Optimized specialized components.")
                success = True
                message = "Specialization strategy applied successfully."
            elif strategy == "integration":
                changes.append("Integrated separate capabilities.")
                success = True
                message = "Integration strategy applied successfully."
            else:
                success = False
                message = f"Unknown strategy: {strategy}"
        except Exception as e:
            success = False
            message = f"Error applying strategy {strategy}: {str(e)}"
        return success, message, changes

    def _generate_reflective_thought(self, context):
        """Generate a reflective thought focused on self-examination"""
        thought = {
            "type": "reflective",
            "content": "Meta-cognitive assessment",
            "reflections": [],
            "insights": [],
            "importance": 0.5
        }
        reflection_areas = [
            "learning_progress",
            "strategy_effectiveness",
            "error_patterns",
            "goal_alignment",
            "cognitive_bias"
        ]
        selected_areas = random.sample(reflection_areas, min(3, len(reflection_areas)))
        reflections = []
        history_span = min(10, len(self.thought_history))
        recent_thoughts = self.thought_history[-history_span:] if history_span > 0 else []
        for area in selected_areas:
            if area == "learning_progress":
                if "stats" in context and isinstance(context["stats"], dict):
                    cycles = context["stats"].get("cycles_run", 0)
                    pages = context["stats"].get("pages_processed", 0)
                    if cycles > 0:
                        learning_rate = pages / max(1, cycles)
                        reflections.append({
                            "area": "learning_progress",
                            "content": f"Learning rate: {learning_rate:.2f} pages per cycle",
                            "assessment": "satisfactory" if learning_rate > 0.5 else "needs improvement"
                        })
            elif area == "strategy_effectiveness":
                if (hasattr(self.agent, 'planner_sifter') and
                    hasattr(self.agent.planner_sifter, 'strategies')):
                    strategies = self.agent.planner_sifter.strategies
                    if strategies:
                        sorted_strategies = sorted(
                            [(name, data.get("effectiveness", 0.5)) for name, data in strategies.items()],
                            key=lambda x: x[1],
                            reverse=True
                        )
                        if sorted_strategies:
                            best = sorted_strategies[0]
                            worst = sorted_strategies[-1]
                            reflections.append({
                                "area": "strategy_effectiveness",
                                "content": f"Most effective strategy: {best[0]} ({best[1]:.2f}), Least effective: {worst[0]} ({worst[1]:.2f})",
                                "recommendation": f"Consider increasing use of {best[0]} approach"
                            })
            elif area == "error_patterns":
                if recent_thoughts:
                    error_thoughts = [t for t in recent_thoughts if "error" in str(t.get("content", "")).lower()]
                    if error_thoughts:
                        error_count = len(error_thoughts)
                        reflections.append({
                            "area": "error_patterns",
                            "content": f"Error-related thoughts appearing in {error_count}/{len(recent_thoughts)} recent thoughts",
                            "assessment": "concerning" if error_count > 3 else "manageable"
                        })
            elif area == "goal_alignment":
                if ("current_goal" in context and "recent_actions" in context and isinstance(context["recent_actions"], list)):
                    goal = context["current_goal"].get("description", "") if isinstance(context["current_goal"], dict) else ""
                    if goal:
                        actions = context["recent_actions"]
                        aligned_count = 0
                        for action in actions:
                            if isinstance(action, dict) and "action" in action:
                                action_type = action["action"]
                                if "explore" in goal.lower() and action_type in ["expand", "search"]:
                                    aligned_count += 1
                                elif "deep" in goal.lower() and action_type in ["evaluate", "adapt"]:
                                    aligned_count += 1
                        alignment_rate = aligned_count / max(1, len(actions))
                        reflections.append({
                            "area": "goal_alignment",
                            "content": f"Action-goal alignment rate: {alignment_rate:.2f}",
                            "assessment": "well-aligned" if alignment_rate > 0.6 else "misaligned"
                        })
            elif area == "cognitive_bias":
                potential_biases = []
                if recent_thoughts:
                    modes = [t.get("mode") for t in recent_thoughts]
                    mode_counts = {}
                    for mode in modes:
                        if mode:
                            mode_counts[mode] = mode_counts.get(mode, 0) + 1
                    most_common = max(mode_counts.items(), key=lambda x: x[1]) if mode_counts else (None, 0)
                    if most_common[1] > len(recent_thoughts) * 0.7:
                        potential_biases.append(f"Possible cognitive fixation on {most_common[0]} thinking mode")
                if "recent_actions" in context and isinstance(context["recent_actions"], list):
                    actions = context["recent_actions"]
                    if len(actions) >= 2:
                        last_action = actions[-1]
                        if isinstance(last_action, dict) and last_action.get("success", False):
                            potential_biases.append("Potential recency bias - overweighting last successful action")
                if potential_biases:
                    reflections.append({
                        "area": "cognitive_bias",
                        "content": potential_biases[0],
                        "recommendation": "Consider deliberate perspective shifts to counteract"
                    })
        insights = []
        for reflection in reflections:
            area = reflection.get("area", "")
            assessment = reflection.get("assessment", "")
            recommendation = reflection.get("recommendation", "")
            if area == "learning_progress" and assessment == "needs improvement":
                insights.append("Learning rate below target - consider adjusting exploration/exploitation balance")
            elif area == "strategy_effectiveness" and recommendation:
                insights.append(recommendation)
            elif area == "error_patterns" and assessment == "concerning":
                insights.append("High error rate - implement additional validation measures")
            elif area == "goal_alignment" and assessment == "misaligned":
                insights.append("Actions not well-aligned with goals - revisit planning process")
            elif area == "cognitive_bias" and recommendation:
                insights.append(recommendation)
        thought["reflections"] = reflections
        thought["insights"] = insights
        importance = 0.5
        if insights:
            importance += 0.1 * len(insights)
        if "concerning" in str(reflections) or "misaligned" in str(reflections):
            importance += 0.2
        thought["importance"] = min(0.9, importance)
        if insights:
            thought["content"] = insights[0]
        elif reflections:
            thought["content"] = reflections[0].get("content", "Meta-cognitive reflection")
        return thought

    def _generate_exploratory_thought(self, context):
        """Generate an exploratory thought focused on new possibilities"""
        thought = {
            "type": "exploratory",
            "content": "Exploration of new possibilities",
            "directions": [],
            "insights": [],
            "importance": 0.5
        }
        exploration_areas = [
            "unknown_domains",
            "connection_patterns",
            "alternative_strategies",
            "capability_expansion"
        ]
        selected_areas = random.sample(exploration_areas, min(2, len(exploration_areas)))
        directions = []
        for area in selected_areas:
            if area == "unknown_domains":
                if "domains_visited" in context:
                    domains_visited = context.get("domains_visited", set())
                    if isinstance(domains_visited, set):
                        candidate_domains = [
                            "research.science", "github.com", "en.wikipedia.org",
                            "arxiv.org", "semanticscholar.org", "openai.com",
                            "nature.com", "reddit.com/r/MachineLearning"
                        ]
                        unvisited = [d for d in candidate_domains if d not in domains_visited]
                        if unvisited:
                            sample_domains = random.sample(unvisited, min(3, len(unvisited)))
                            directions.append({
                                "area": "unknown_domains",
                                "content": f"Explore high-value domains: {', '.join(sample_domains)}",
                                "rationale": "Expanding domain knowledge diversity"
                            })
            elif area == "connection_patterns":
                directions.append({
                    "area": "connection_patterns",
                    "content": "Implement knowledge graph traversal to find distant connections",
                    "rationale": "Distant domains often contain valuable cross-applicable patterns"
                })
            elif area == "alternative_strategies":
                if (hasattr(self.agent, 'planner_sifter') and
                    hasattr(self.agent.planner_sifter, 'strategies')):
                    strategies = list(self.agent.planner_sifter.strategies.keys())
                    if strategies:
                        if (hasattr(self.agent.planner_sifter, 'strategy_usage')):
                            usage = self.agent.planner_sifter.strategy_usage
                            least_used = sorted([(s, usage.get(s, 0)) for s in strategies], key=lambda x: x[1])
                            if least_used:
                                least_used_strategy = least_used[0][0]
                                directions.append({
                                    "area": "alternative_strategies",
                                    "content": f"Experiment with underutilized strategy: {least_used_strategy}",
                                    "rationale": "Diversifying strategic approaches to discover new optima"
                                })
            elif area == "capability_expansion":
                directions.append({
                    "area": "capability_expansion",
                    "content": "Develop enhanced semantic reasoning module",
                    "rationale": "Would significantly improve knowledge integration capabilities"
                })
        insights = []
        for direction in directions:
            area = direction.get("area", "")
            rationale = direction.get("rationale", "")
            if area == "unknown_domains":
                insights.append("Systematic exploration of high-value domains should be prioritized")
            elif area == "connection_patterns":
                insights.append("Knowledge value may lie in unexpected cross-domain connections")
            elif area == "alternative_strategies":
                insights.append("Strategic diversification may uncover more effective approaches")
            elif area == "capability_expansion":
                insights.append("Capability development should focus on knowledge integration")
        thought["directions"] = directions
        thought["insights"] = insights
        if insights:
            thought["importance"] = 0.6 + 0.1 * len(insights)
        if insights:
            thought["content"] = insights[0]
        elif directions:
            thought["content"] = directions[0].get("content", "Exploration suggestion")
        return thought

    def _generate_critical_thought(self, context):
        """Generate a critical thought focused on evaluation and assessment"""
        thought = {
            "type": "critical",
            "content": "Critical evaluation",
            "assessments": [],
            "insights": [],
            "importance": 0.5
        }
        assessment_areas = [
            "information_quality",
            "reasoning_validity",
            "strategy_efficiency",
            "resource_allocation",
            "error_handling"
        ]
        selected_areas = random.sample(assessment_areas, min(2, len(assessment_areas)))
        assessments = []
        for area in selected_areas:
            if area == "information_quality":
                if (hasattr(self.agent, 'free_will') and
                    hasattr(self.agent.free_will, 'domain_intelligence')):
                    intelligence = self.agent.free_will.domain_intelligence
                    if hasattr(intelligence, 'domain_knowledge'):
                        knowledge = intelligence.domain_knowledge
                        if knowledge:
                            low_quality_domains = []
                            for domain, data in knowledge.items():
                                if isinstance(data, dict) and data.get("content_quality", 1.0) < 0.5:
                                    low_quality_domains.append(domain)
                            if low_quality_domains:
                                sample_domains = random.sample(low_quality_domains, min(2, len(low_quality_domains)))
                                assessments.append({
                                    "area": "information_quality",
                                    "content": f"Low-quality content detected in domains: {', '.join(sample_domains)}",
                                    "recommendation": "Implement stricter quality thresholds for these domains"
                                })
                            else:
                                assessments.append({
                                    "area": "information_quality",
                                    "content": "Overall information quality appears acceptable",
                                    "recommendation": "Continue regular quality monitoring"
                                })
            elif area == "reasoning_validity":
                if self.thought_history:
                    recent_thoughts = self.thought_history[-10:] if len(self.thought_history) >= 10 else self.thought_history
                    thought_types = set(t.get("type", "") for t in recent_thoughts)
                    if len(thought_types) < 3:
                        assessments.append({
                            "area": "reasoning_validity",
                            "content": f"Limited cognitive diversity detected (only {len(thought_types)} thought types used recently)",
                            "recommendation": "Deliberately activate more diverse thinking modes"
                        })
                    else:
                        assessments.append({
                            "area": "reasoning_validity",
                            "content": f"Cognitive diversity appears healthy ({len(thought_types)} different thought types used recently)",
                            "recommendation": "Maintain diverse thinking patterns"
                        })
            elif area == "strategy_efficiency":
                if (hasattr(self.agent, 'stats') and
                    isinstance(self.agent.stats, dict) and
                    'cycles_run' in self.agent.stats and
                    'pages_processed' in self.agent.stats):
                    cycles = self.agent.stats['cycles_run']
                    pages = self.agent.stats['pages_processed']
                    if cycles > 10:
                        efficiency = pages / max(1, cycles)
                        if efficiency < 0.5:
                            assessments.append({
                                "area": "strategy_efficiency",
                                "content": f"Low efficiency detected: {efficiency:.2f} pages/cycle",
                                "recommendation": "Consider more aggressive pruning of low-value paths"
                            })
                        else:
                            assessments.append({
                                "area": "strategy_efficiency",
                                "content": f"Acceptable efficiency: {efficiency:.2f} pages/cycle",
                                "recommendation": "Continue current approach with regular efficiency monitoring"
                            })
            elif area == "resource_allocation":
                if (hasattr(self.agent, 'memory') and
                    hasattr(self.agent, 'free_will') and
                    hasattr(self.agent.free_will, 'memory_set')):
                    memory_usage = len(self.agent.memory) / MEMORY_MAX_SIZE
                    urls_stored = len(self.agent.free_will.memory_set)
                    if memory_usage > 0.8 and urls_stored > 1000:
                        assessments.append({
                            "area": "resource_allocation",
                            "content": f"High memory usage ({memory_usage:.0%}) with {urls_stored} URLs stored",
                            "recommendation": "Implement more aggressive memory pruning strategy"
                        })
                    else:
                        assessments.append({
                            "area": "resource_allocation",
                            "content": f"Memory usage ({memory_usage:.0%}) with {urls_stored} URLs stored",
                            "recommendation": "Current resource allocation appears appropriate"
                        })
            elif area == "error_handling":
                if (hasattr(self.agent, 'domain_stats') and isinstance(self.agent.domain_stats, dict)):
                    total_visits = sum(d.get("visits", 0) for d in self.agent.domain_stats.values())
                    total_errors = sum(d.get("error_count", 0) for d in self.agent.domain_stats.values())
                    if total_visits > 0:
                        error_rate = total_errors / total_visits
                        if error_rate > 0.2:
                            assessments.append({
                                "area": "error_handling",
                                "content": f"High system-wide error rate: {error_rate:.2%}",
                                "recommendation": "Investigate error causes and implement more robust handling"
                            })
                        else:
                            assessments.append({
                                "area": "error_handling",
                                "content": f"Acceptable error rate: {error_rate:.2%}",
                                "recommendation": "Continue monitoring error patterns"
                            })
        insights = []
        for assessment in assessments:
            recommendation = assessment.get("recommendation", "")
            if "low-quality" in assessment.get("content", "").lower():
                insights.append("Information quality control needs improvement")
            elif "limited cognitive diversity" in assessment.get("content", "").lower():
                insights.append("Need to activate more diverse thinking patterns")
            elif "low efficiency" in assessment.get("content", "").lower():
                insights.append("Strategy efficiency requires optimization")
            elif "high memory usage" in assessment.get("content", "").lower():
                insights.append("Memory management needs more efficient pruning")
            elif "high system-wide error" in assessment.get("content", "").lower():
                insights.append("Error handling systems require review and enhancement")
            elif recommendation:
                insights.append(recommendation)
        thought["assessments"] = assessments
        thought["insights"] = insights
        importance = 0.5
        if insights:
            importance += 0.1 * len(insights)
        if any("high" in a.get("content", "").lower() for a in assessments):
            importance += 0.2
        thought["importance"] = min(0.9, importance)
        if insights:
            thought["content"] = insights[0]
        elif assessments:
            thought["content"] = assessments[0].get("content", "Critical assessment")
        return thought

    def _generate_integrative_thought(self, context):
        """Generate an integrative thought focused on knowledge synthesis"""
        thought = {
            "type": "integrative",
            "content": "Knowledge synthesis",
            "connections": [],
            "insights": [],
            "importance": 0.5
        }
        connections = []
        if not context:
            thought["content"] = "Insufficient knowledge for meaningful integration"
            return thought
        elements = {}
        if "domains_visited" in context:
            domains = context.get("domains_visited", set())
            if isinstance(domains, set) and len(domains) > 3:
                elements["domains"] = list(domains)
        if hasattr(self.agent, 'content_sifter'):
            sifter = self.agent.content_sifter
            if hasattr(sifter, 'topics_of_interest'):
                elements["topics"] = sifter.topics_of_interest
        if (hasattr(self.agent, 'ai_manager') and
            hasattr(self.agent.ai_manager, 'temporal_planner') and
            hasattr(self.agent.ai_manager.temporal_planner, 'long_term_goals')):
            goals = self.agent.ai_manager.temporal_planner.long_term_goals
            if goals:
                elements["goals"] = [g.get("description", "") for g in goals]
        if len(elements) >= 2:
            element_types = list(elements.keys())
            type_pair = random.sample(element_types, 2)
            elements_1 = elements[type_pair[0]]
            elements_2 = elements[type_pair[1]]
            if elements_1 and elements_2:
                element_1 = random.choice(elements_1)
                element_2 = random.choice(elements_2)
                connections.append({
                    "elements": [
                        {"type": type_pair[0], "value": element_1},
                        {"type": type_pair[1], "value": element_2}
                    ],
                    "connection_type": "integration",
                    "description": f"Integration of {type_pair[0]} '{element_1}' with {type_pair[1]} '{element_2}'",
                    "potential": random.uniform(0.5, 0.9)
                })
        if not connections:
            theoretical_connections = [
                {
                    "elements": [
                        {"type": "cognitive_mode", "value": "analytical"},
                        {"type": "capability", "value": "semantic_representation"}
                    ],
                    "connection_type": "enhancement",
                    "description": "Analytical thinking could enhance semantic representation quality",
                    "potential": 0.7
                },
                {
                    "elements": [
                        {"type": "capability", "value": "memory_management"},
                        {"type": "capability", "value": "planning"}
                    ],
                    "connection_type": "synergy",
                    "description": "Memory systems could be more tightly integrated with planning",
                    "potential": 0.8
                }
            ]
            connections.append(random.choice(theoretical_connections))
        insights = []
        for connection in connections:
            desc = connection.get("description", "")
            conn_type = connection.get("connection_type", "")
            potential = connection.get("potential", 0.5)
            if conn_type == "integration" and potential > 0.7:
                insights.append(f"High potential integration opportunity: {desc}")
            elif conn_type == "enhancement":
                insights.append(f"Enhancement pathway identified: {desc}")
            elif conn_type == "synergy":
                insights.append(f"Synergistic relationship would increase capability: {desc}")
            else:
                insights.append(f"Connection opportunity: {desc}")
        thought["connections"] = connections
        thought["insights"] = insights
        importance = 0.5
        if connections:
            avg_potential = sum(c.get("potential", 0.5) for c in connections) / len(connections)
            importance = avg_potential
        thought["importance"] = min(0.9, importance)
        if insights:
            thought["content"] = insights[0]
        elif connections:
            thought["content"] = connections[0].get("description", "Knowledge integration")
        return thought

    def _generate_intuitive_thought(self, context):
        """Generate an intuitive thought focused on pattern recognition"""
        thought = {
            "type": "intuitive",
            "content": "Pattern recognition",
            "patterns": [],
            "insights": [],
            "importance": 0.5
        }
        patterns = []
        if not context:
            thought["content"] = "Insufficient data for pattern recognition"
            return thought
        if "recent_actions" in context and isinstance(context["recent_actions"], list):
            actions = context["recent_actions"]
            if len(actions) >= 3:
                action_types = [a.get("action", "unknown") for a in actions if isinstance(a, dict)]
                if len(action_types) >= 3:
                    repetitions = []
                    current_sequence = [action_types[0]]
                    for i in range(1, len(action_types)):
                        if action_types[i] == action_types[i-1]:
                            current_sequence.append(action_types[i])
                        else:
                            if len(current_sequence) >= 2:
                                repetitions.append(current_sequence)
                            current_sequence = [action_types[i]]
                    if len(current_sequence) >= 2:
                        repetitions.append(current_sequence)
                    if repetitions:
                        longest_repetition = max(repetitions, key=len)
                        patterns.append({
                            "type": "action_repetition",
                            "description": f"Repeated sequence of '{longest_repetition[0]}' actions",
                            "significance": 0.6 + 0.1 * min(4, len(longest_repetition)),
                            "potential_cause": "Strategy fixation or optimal local strategy"
                        })
                    if len(set(action_types)) == 2 and len(action_types) >= 4:
                        is_alternating = True
                        for i in range(2, len(action_types)):
                            if action_types[i] != action_types[i-2]:
                                is_alternating = False
                                break
                        if is_alternating:
                            patterns.append({
                                "type": "action_alternation",
                                "description": f"Alternating pattern between '{action_types[0]}' and '{action_types[1]}'",
                                "significance": 0.7,
                                "potential_cause": "Explore-exploit cycle or complementary strategies"
                            })
        if "domain_stats" in context and isinstance(context["domain_stats"], dict):
            domains = context["domain_stats"]
            if domains:
                frequent_domains = sorted(
                    [(d, stats.get("visits", 0)) for d, stats in domains.items()],
                    key=lambda x: x[1],
                    reverse=True
                )
                if frequent_domains:
                    top_domains = frequent_domains[:3]
                    total_visits = sum(v for _, v in frequent_domains)
                    top_domain_visits = sum(v for _, v in top_domains)
                    concentration = top_domain_visits / max(1, total_visits)
                    if concentration > 0.7:
                        patterns.append({
                            "type": "domain_concentration",
                            "description": f"Heavy concentration ({concentration:.0%}) on top 3 domains",
                            "significance": 0.7,
                            "potential_cause": "Exploitation focus or domain specialization"
                        })
        if "domain_stats" in context and isinstance(context["domain_stats"], dict):
            domains = context["domain_stats"]
            if domains:
                error_domains = []
                for domain, stats in domains.items():
                    if isinstance(stats, dict) and stats.get("error_rate", 0) > 0.3:
                        error_domains.append((domain, stats.get("error_rate", 0)))
                if error_domains:
                    error_domains.sort(key=lambda x: x[1], reverse=True)
                    top_error_domains = error_domains[:3]
                    patterns.append({
                        "type": "error_concentration",
                        "description": f"High error rates in specific domains: {', '.join(d for d, _ in top_error_domains)}",
                        "significance": 0.8,
                        "potential_cause": "Domain-specific access issues or content filtering problems"
                    })
        insights = []
        for pattern in patterns:
            pattern_type = pattern.get("type", "")
            significance = pattern.get("significance", 0.5)
            if pattern_type == "action_repetition" and significance > 0.7:
                insights.append("Repeated action pattern may indicate strategy fixation - consider forcing exploration")
            elif pattern_type == "action_alternation":
                insights.append("Alternating action pattern suggests systematic exploration-exploitation approach")
            elif pattern_type == "domain_concentration":
                insights.append("High domain concentration indicates need for broader exploration")
            elif pattern_type == "error_concentration":
                insights.append("Domain-specific error pattern detected - consider domain-specific handling strategies")
        thought["patterns"] = patterns
        thought["insights"] = insights
        importance = 0.5
        if patterns:
            avg_significance = sum(p.get("significance", 0.5) for p in patterns) / len(patterns)
            importance = avg_significance
        thought["importance"] = min(0.9, importance)
        if insights:
            thought["content"] = insights[0]
        elif patterns:
            thought["content"] = patterns[0].get("description", "Pattern detected")
        return thought

    def _generate_balanced_thought(self, context):
        """Generate a balanced thought that incorporates multiple thinking modes"""
        available_modes = ["analytical", "creative", "reflective", "exploratory", "critical", "integrative", "intuitive"]
        selected_modes = random.sample(available_modes, 2)
        thoughts = []
        for mode in selected_modes:
            if mode == "analytical":
                thoughts.append(self._generate_analytical_thought(context))
            elif mode == "creative":
                thoughts.append(self._generate_creative_thought(context))
            elif mode == "reflective":
                thoughts.append(self._generate_reflective_thought(context))
            elif mode == "exploratory":
                thoughts.append(self._generate_exploratory_thought(context))
            elif mode == "critical":
                thoughts.append(self._generate_critical_thought(context))
            elif mode == "integrative":
                thoughts.append(self._generate_integrative_thought(context))
            elif mode == "intuitive":
                thoughts.append(self._generate_intuitive_thought(context))
        balanced_thought = {
            "type": "balanced",
            "content": "Multi-perspective assessment",
            "component_modes": selected_modes,
            "insights": [],
            "importance": 0.5
        }
        all_insights = []
        for thought in thoughts:
            if "insights" in thought and thought["insights"]:
                all_insights.extend(thought["insights"])
        selected_insights = all_insights[:3] if all_insights else []
        balanced_thought["insights"] = selected_insights
        if thoughts:
            avg_importance = sum(t.get("importance", 0.5) for t in thoughts) / len(thoughts)
            balanced_thought["importance"] = avg_importance
        if selected_insights:
            balanced_thought["content"] = selected_insights[0]
        elif thoughts:
            balanced_thought["content"] = "Balanced perspective: " + thoughts[0].get("content", "")
        return balanced_thought

    def _update_working_memory(self, thought):
        """
        Update working memory with new thought, managing capacity constraints.
        """
        if not thought:
            return
        self.working_memory.append(thought)
        if len(self.working_memory) > self.working_memory_capacity:
            if len(self.working_memory) > 1:
                least_important_idx = min(range(len(self.working_memory) - 1),
                                        key=lambda i: self.working_memory[i].get("importance", 0))
                del self.working_memory[least_important_idx]
        for insight in thought.get("insights", []):
            importance = thought.get("importance", 0.5)
            if importance > 0.7:
                self.recent_insights.append({
                    "content": insight,
                    "source_type": thought.get("type", "unknown"),
                    "importance": importance,
                    "timestamp": datetime.now().isoformat()
                })
        if len(self.recent_insights) > 20:
            self.recent_insights = self.recent_insights[-20:]

    def _summarize_context(self, context):
        """Create a brief summary of the context for thought recording"""
        if not context:
            return "No context"
        if not isinstance(context, dict):
            return str(context)[:100]
        elements = []
        if "current_goal" in context and isinstance(context["current_goal"], dict):
            goal_desc = context["current_goal"].get("description", "unknown goal")
            elements.append(f"Goal: {goal_desc}")
        if "last_action" in context and isinstance(context["last_action"], dict):
            action = context["last_action"].get("action", "unknown action")
            elements.append(f"Action: {action}")
        if "memory_size" in context:
            elements.append(f"Memory: {context['memory_size']}")
        return "; ".join(elements) if elements else "Context present but no key elements"

    def _extract_elements_from_context(self, context, aspect_types):
        """
        Extract relevant elements from context based on aspect types.
        """
        elements = []
        if not context or not isinstance(context, dict):
            return elements
        for aspect_type in aspect_types:
            if aspect_type == "goals" and "current_goal" in context:
                if isinstance(context["current_goal"], dict):
                    goal_desc = context["current_goal"].get("description", "")
                    if goal_desc:
                        elements.append(goal_desc)
            elif aspect_type == "domains" and "domains_visited" in context:
                domains = context["domains_visited"]
                if isinstance(domains, set) and domains:
                    sample_size = min(3, len(domains))
                    domain_sample = random.sample(list(domains), sample_size)
                    elements.extend(domain_sample)
            elif aspect_type == "strategies" and hasattr(self.agent, 'planner_sifter'):
                if hasattr(self.agent.planner_sifter, 'strategies'):
                    strategy_names = list(self.agent.planner_sifter.strategies.keys())
                    if strategy_names:
                        sample_size = min(2, len(strategy_names))
                        strategy_sample = random.sample(strategy_names, sample_size)
                        elements.extend(strategy_sample)
            elif aspect_type == "patterns" and "recent_actions" in context:
                actions = context["recent_actions"]
                if isinstance(actions, list) and len(actions) >= 3:
                    action_types = [a.get("action", "unknown") for a in actions if isinstance(a, dict)]
                    if len(set(action_types)) <= 3:
                        pattern_desc = f"Action pattern: {' → '.join(action_types[-3:])}"
                        elements.append(pattern_desc)
            elif aspect_type == "anomalies" and "domain_stats" in context:
                domains = context["domain_stats"]
                if isinstance(domains, dict):
                    anomalies = []
                    for domain, stats in domains.items():
                        if isinstance(stats, dict) and stats.get("error_rate", 0) > 0.3:
                            anomalies.append(f"High error rate in {domain}")
                    if anomalies:
                        elements.append(random.choice(anomalies))
        return elements

    def _assess_action_outcomes(self, actions):
        """
        Assess whether recent action outcomes are improving or deteriorating.
        """
        if not actions or len(actions) < 3:
            return "insufficient_data"
        success_indicators = []
        for action in actions:
            if not isinstance(action, dict):
                continue
            if "success" in action:
                success_indicators.append(1 if action["success"] else 0)
                continue
            if "content_length" in action:
                length = action["content_length"]
                success_indicators.append(min(1.0, length / 5000))
                continue
            if "links_discovered" in action:
                links = action["links_discovered"]
                success_indicators.append(min(1.0, links / 10))
                continue
        if len(success_indicators) < 3:
            return "insufficient_data"
        mid_point = len(success_indicators) // 2
        first_half = success_indicators[:mid_point]
        second_half = success_indicators[mid_point:]
        first_avg = sum(first_half) / len(first_half)
        second_avg = sum(second_half) / len(second_half)
        diff = second_avg - first_avg
        if abs(diff) < 0.1:
            return "stable"
        elif diff > 0:
            return "improving"
        else:
            return "deteriorating"

    def get_current_state(self):
        """
        Get the current state of the autonomous mind.
        """
        return {
            "current_mode": self.current_mode,
            "cognitive_load": self.cognitive_load,
            "thought_depth": self.thought_depth,
            "working_memory_usage": len(self.working_memory) / self.working_memory_capacity,
            "recent_thoughts": [{
                "content": t.get("content", ""),
                "type": t.get("type", ""),
                "importance": t.get("importance", 0.5)
            } for t in self.thought_history[-5:]] if self.thought_history else [],
            "important_insights": [{
                "content": i.get("content", ""),
                "importance": i.get("importance", 0.5)
            } for i in self.recent_insights[-3:]] if self.recent_insights else [],
            "active_concepts": [{
                "state": state,
                "activation": data["activation"]
            } for state, data in self.cognitive_states.items() if data["activation"] >= self.concept_activation_threshold],
            "thinking_style": self.thinking_style
        }

    def set_thinking_style(self, style_params):
        """
        Adjust thinking style parameters to change cognitive approach.
        """
        if not style_params or not isinstance(style_params, dict):
            return False
        for param, value in style_params.items():
            if param in self.thinking_style and isinstance(value, (int, float)):
                value = max(0.0, min(1.0, value))
                old_value = self.thinking_style[param]
                self.thinking_style[param] = value
                log_event(f"Thinking style parameter '{param}' adjusted: {old_value:.2f} → {value:.2f}", "INFO")
        return True

    def prime_with_context(self, context_elements):
        """
        Prime the mind with specific context elements to influence thinking.
        """
        if not context_elements or not isinstance(context_elements, dict):
            return False
        if "cognitive_modes" in context_elements:
            modes = context_elements["cognitive_modes"]
            if isinstance(modes, list):
                for mode in modes:
                    if mode in self.cognitive_states:
                        old_activation = self.cognitive_states[mode]["activation"]
                        self.cognitive_states[mode]["activation"] = min(0.9, old_activation * 1.5)
                        log_event(f"Cognitive mode '{mode}' primed: {old_activation:.2f} → {self.cognitive_states[mode]['activation']:.2f}", "INFO")
        if "depth" in context_elements:
            depth = context_elements["depth"]
            if isinstance(depth, (int, float)):
                self.thought_depth = max(0.0, min(1.0, depth))
                log_event(f"Thought depth primed to {self.thought_depth:.2f}", "INFO")
        if "focus" in context_elements:
            focus = context_elements["focus"]
            self.attention_focus = focus
            log_event(f"Attention focus primed to '{focus}'", "INFO")
        return True




# =============================================================================
# SELF-EVOLUTION MODULE - ADDED HERE
# =============================================================================
# =============================================================================
# SELF-EVOLUTION MODULE - ADDED HERE
# =============================================================================
from collections import deque
import random
import math
import torch
from datetime import datetime

# Constants that would be defined elsewhere
SELF_MODIFY_INTERVAL = 10  # Number of cycles between self-modification attempts
MEMORY_MAX_SIZE = 1000  # Maximum memory size for free will

def log_event(message, level="INFO"):
    """Simple logging utility that would be defined elsewhere"""
    print(f"[{level}] {message}")

class HyperMorphicMath:
    """
    HyperMorphic Mathematics utility class that ensures zero-free operations
    with dynamic base and modulus adjustments for stable computation.
    """
    def __init__(self, dynamic_base=1000.0, dynamic_modulus=997, epsilon=1e-12):
        self.dynamic_base = dynamic_base
        self.dynamic_modulus = dynamic_modulus
        self.epsilon = epsilon
    
    def zero_free(self, value):
        """Ensures a value is never exactly zero"""
        if abs(value) < self.epsilon:
            return self.epsilon if value >= 0 else -self.epsilon
        return value
    
    def add(self, a, b):
        """Zero-free addition"""
        result = a + b
        return self.zero_free(result)
    
    def sub(self, a, b):
        """Zero-free subtraction"""
        result = a - b
        return self.zero_free(result)
    
    def mul(self, a, b):
        """Zero-free multiplication"""
        result = a * b
        return self.zero_free(result)
    
    def div(self, a, b):
        """Zero-free division"""
        b = self.zero_free(b)  # Ensure divisor is not zero
        result = a / b
        return self.zero_free(result)
    
    def min(self, a, b):
        """Zero-free minimum"""
        result = min(a, b)
        return self.zero_free(result)
    
    def max(self, a, b):
        """Zero-free maximum"""
        result = max(a, b)
        return self.zero_free(result)

class HyperMorphicMetaEvolutionEngine:
    """
    Advanced engine for system-level self-evolution that uses HyperMorphic mathematics
    and zero-free operations to adapt core algorithms, architectures, and strategies
    over longer timescales with guaranteed stability.

    Key features:
    - Zero-free evolutionary operations (no exact zeros)
    - Dynamic base Φ and modulus Ψ for all probability calculations
    - HyperMorphic concept blending for strategy creation
    - Quantum-inspired strategy selection and adaptation
    - Multi-dimensional capability assessment
    """
    def __init__(self, epsilon=1e-12):
        self.evolution_generation = 1
        self.last_major_evolution = 0
        self.evolution_history = deque(maxlen=50)

        # Initialize HyperMorphic Math utility for zero-free operations
        self.hyper_math = HyperMorphicMath(
            dynamic_base=1000.0,  # Default dynamic base Φ
            dynamic_modulus=997,  # Default dynamic modulus Ψ
            epsilon=epsilon       # HyperMorphic nearness element ε_ᵩ
        )

        # Define the dimensionality of the evolutionary space
        self.evolutionary_dimensions = 5

        # Strategy definitions with HyperMorphic success rates
        self.strategies = {
            "expansion": {
                "description": "Expand neural architecture",
                "success_rate": self.hyper_math.zero_free(0.6),
                "last_attempt": 0,
                "dimensional_vectors": self._create_strategy_dimensional_vector([0.8, 0.4, 0.7, 0.5, 0.6])
            },
            "pruning": {
                "description": "Prune redundant components",
                "success_rate": self.hyper_math.zero_free(0.5),
                "last_attempt": 0,
                "dimensional_vectors": self._create_strategy_dimensional_vector([0.4, 0.9, 0.3, 0.6, 0.4])
            },
            "restructuring": {
                "description": "Reorganize internal structure",
                "success_rate": self.hyper_math.zero_free(0.4),
                "last_attempt": 0,
                "dimensional_vectors": self._create_strategy_dimensional_vector([0.5, 0.6, 0.8, 0.3, 0.5])
            },
            "specialization": {
                "description": "Specialize components for focused tasks",
                "success_rate": self.hyper_math.zero_free(0.55),
                "last_attempt": 0,
                "dimensional_vectors": self._create_strategy_dimensional_vector([0.3, 0.7, 0.5, 0.8, 0.7])
            },
            "integration": {
                "description": "Integrate synergistic modules",
                "success_rate": self.hyper_math.zero_free(0.65),
                "last_attempt": 0,
                "dimensional_vectors": self._create_strategy_dimensional_vector([0.7, 0.5, 0.9, 0.7, 0.8])
            },
            "quantum_variation": {
                "description": "Apply quantum variation to existing structures",
                "success_rate": self.hyper_math.zero_free(0.45),
                "last_attempt": 0,
                "dimensional_vectors": self._create_strategy_dimensional_vector([0.6, 0.6, 0.6, 0.4, 0.9])
            },
            "holomorphic_restructuring": {
                "description": "Apply structure-preserving holomorphic transformations",
                "success_rate": self.hyper_math.zero_free(0.5),
                "last_attempt": 0,
                "dimensional_vectors": self._create_strategy_dimensional_vector([0.4, 0.5, 0.7, 0.6, 0.8])
            }
        }

        # Goal weights with HyperMorphic zero-free values
        self.goal_weights = {
            "performance": self.hyper_math.zero_free(0.5),
            "efficiency": self.hyper_math.zero_free(0.3),
            "adaptability": self.hyper_math.zero_free(0.4),
            "robustness": self.hyper_math.zero_free(0.4),
            "complexity": self.hyper_math.zero_free(-0.2)  # Negative weight: minimize complexity
        }

        # Capability targets with HyperMorphic zero-free values
        self.capability_targets = {
            "knowledge_representation": self.hyper_math.zero_free(0.9),
            "planning": self.hyper_math.zero_free(0.85),
            "learning": self.hyper_math.zero_free(0.9),
            "error_handling": self.hyper_math.zero_free(0.8),
            "creative_synthesis": self.hyper_math.zero_free(0.7),
            "quantum_superposition": self.hyper_math.zero_free(0.6),
            "holomorphic_processing": self.hyper_math.zero_free(0.75)
        }

        # Current capability scores with HyperMorphic zero-free initialization
        self.capability_scores = {cap: self.hyper_math.zero_free(0.3) for cap in self.capability_targets}

        self.adaptation_rate = self.hyper_math.zero_free(0.3)  # Rate of adaptation pressure
        self.convergence_score = self.hyper_math.zero_free(0.0)  # Score indicating evolutionary convergence
        self.performance_metrics = []  # Track performance metrics for self-modification
        self.thought_history = []  # For reflective thinking
        self.working_memory = []  # Active concepts and thoughts
        self.working_memory_capacity = 7  # Miller's magical number
        self.recent_insights = []  # Store recent realizations
        self.agent = None  # Will be set when used
        self.fitness_metrics = {}  # For tracking fitness over time

        # Hypermorphic evolution parameters
        self.dimension_stability = self.hyper_math.zero_free(0.7)  # How stable dimensions remain during evolution
        self.quantum_entanglement_factor = self.hyper_math.zero_free(0.3)  # How much strategies influence each other
        self.holomorphic_preservation = self.hyper_math.zero_free(0.8)  # How much structure is preserved during transformation
        self.evolutionary_dimensions = 5  # Dimensionality of the evolutionary space

        # Capability embeddings for holomorphic transformations
        self.capability_embeddings = self._initialize_capability_embeddings()

        # Strategy relationships graph using quantum entanglement
        self.strategy_relationships = self._initialize_strategy_relationships()

        # Blended strategies registry for tracking higher-order strategies
        self.blended_strategies_registry = {}

        log_event("HyperMorphicMetaEvolutionEngine initialized with zero-free operations", "QUANTUM")

    def _create_strategy_dimensional_vector(self, base_values):
        """Create a HyperMorphic dimensional vector for strategy with zero-free properties"""
        if len(base_values) != self.evolutionary_dimensions:
            # If dimensions don't match, create random values
            base_values = [random.random() for _ in range(self.evolutionary_dimensions)]

        # Make all values zero-free
        return [self.hyper_math.zero_free(val) for val in base_values]

    def _initialize_capability_embeddings(self):
        """Initialize capability embeddings in a HyperMorphic vector space"""
        embeddings = {}
        for capability in self.capability_targets.keys():
            # Create high-dimensional embedding for each capability
            embedding = [self.hyper_math.zero_free(random.random()) for _ in range(self.evolutionary_dimensions)]
            embeddings[capability] = embedding
        return embeddings

    def _initialize_strategy_relationships(self):
        """Initialize quantum-entangled strategy relationships with zero-free properties"""
        relationships = {}
        strategies = list(self.strategies.keys())

        for i, strategy1 in enumerate(strategies):
            relationships[strategy1] = {}
            for j, strategy2 in enumerate(strategies):
                if i != j:
                    # Create quantum-entangled relationship with zero-free value
                    # Strategies with similar dimensional vectors will have higher entanglement
                    vec1 = self.strategies[strategy1]["dimensional_vectors"]
                    vec2 = self.strategies[strategy2]["dimensional_vectors"]

                    # Compute similarity through HyperMorphic dot product
                    dot_product = sum(self.hyper_math.mul(v1, v2) for v1, v2 in zip(vec1, vec2))

                    # Apply quantum phase to relationship (0 to 2π)
                    phase = random.uniform(0, 2 * math.pi)

                    # Store relationship with phase information (complex-like representation)
                    relationships[strategy1][strategy2] = {
                        "strength": self.hyper_math.zero_free(dot_product),
                        "phase": phase
                    }

        return relationships

    def quantum_concept_blend(self, concept1, concept2, blend_factor=0.5):
        """
        Blend two concepts in an entangled conceptual space using HyperMorphic mathematics.
        All operations are zero-free and use dynamic base/modulus arithmetic.

        Parameters:
        - concept1: Dictionary representing first concept
        - concept2: Dictionary representing second concept
        - blend_factor: HyperMorphic weight for blending (0.0-1.0)

        Returns:
        - Blended concept dictionary with emergent properties
        """
        # Ensure blend_factor is zero-free
        blend_factor = self.hyper_math.zero_free(blend_factor)

        if not isinstance(concept1, dict) or not isinstance(concept2, dict):
            return None

        # Extract key attributes
        attributes1 = set(concept1.keys())
        attributes2 = set(concept2.keys())

        # Find shared and unique attributes
        shared = attributes1.intersection(attributes2)
        unique1 = attributes1 - shared
        unique2 = attributes2 - shared

        # Create quantum superposition of concepts with zero-free guarantees
        blended = {}

        # Shared attributes use quantum interference with HyperMorphic operations
        for attr in shared:
            if isinstance(concept1[attr], (int, float)) and isinstance(concept2[attr], (int, float)):
                # Make numeric values zero-free
                val1 = self.hyper_math.zero_free(concept1[attr])
                val2 = self.hyper_math.zero_free(concept2[attr])

                # Quantum interference effect with dynamic phase
                phase = random.uniform(0, math.pi * 2)

                # Apply HyperMorphic operations to ensure zero-free result
                cos_component = self.hyper_math.mul(val1, math.cos(phase))
                sin_component = self.hyper_math.mul(val2, math.sin(phase))

                # HyperMorphic addition ensures result is within dynamic base
                blended[attr] = self.hyper_math.add(cos_component, sin_component)
            else:
                # Non-numeric attributes use probabilistic selection with HyperMorphic probability
                blended[attr] = concept1[attr] if self.hyper_math.zero_free(random.random()) < blend_factor else concept2[attr]

        # Include unique attributes with probability based on blend factor
        # All probabilities use HyperMorphic calculations
        for attr in unique1:
            if self.hyper_math.zero_free(random.random()) < blend_factor:
                if isinstance(concept1[attr], (int, float)):
                    blended[attr] = self.hyper_math.zero_free(concept1[attr])
                else:
                    blended[attr] = concept1[attr]

        for attr in unique2:
            inversed_blend_factor = self.hyper_math.sub(1.0, blend_factor)
            if self.hyper_math.zero_free(random.random()) < inversed_blend_factor:
                if isinstance(concept2[attr], (int, float)):
                    blended[attr] = self.hyper_math.zero_free(concept2[attr])
                else:
                    blended[attr] = concept2[attr]

        # Generate emergent properties (something neither concept had)
        emergent_options = ["synergy", "transcendence", "complexity", "harmony", "paradox",
                           "quantum_entanglement", "holomorphic_resonance", "dimensional_bridging",
                           "zero_free_stability", "phase_coherence"]

        # Add at least one emergent property
        blended["emergent_property"] = random.choice(emergent_options)

        # HyperMorphic blending occasionally generates additional emergent properties
        if self.hyper_math.zero_free(random.random()) > self.hyper_math.zero_free(0.7):
            second_emergent = random.choice([e for e in emergent_options if e != blended["emergent_property"]])
            blended["secondary_emergent_property"] = second_emergent

        # Create dimensional vector for the new blended concept
        if ("dimensional_vectors" in concept1) and ("dimensional_vectors" in concept2):
            # Blend dimensional vectors with quantum interference
            vec1 = concept1["dimensional_vectors"]
            vec2 = concept2["dimensional_vectors"]

            # Ensure vectors have same dimension, if not create new random vector
            if len(vec1) != len(vec2):
                blended["dimensional_vectors"] = self._create_strategy_dimensional_vector([])
            else:
                # Quantum superposition of vectors with phase shift
                blended_vector = []
                for i in range(len(vec1)):
                    phase = random.uniform(0, 2 * math.pi)
                    v1 = self.hyper_math.zero_free(vec1[i])
                    v2 = self.hyper_math.zero_free(vec2[i])

                    # Quantum vector blending with interference
                    blended_val = self.hyper_math.add(
                        self.hyper_math.mul(v1, math.cos(phase)),
                        self.hyper_math.mul(v2, math.sin(phase))
                    )
                    blended_vector.append(blended_val)

                blended["dimensional_vectors"] = blended_vector

        # Add holomorphic structure preservation score
        blended["holomorphic_preservation"] = self.hyper_math.zero_free(
            self.hyper_math.mul(self.holomorphic_preservation,
                              self.hyper_math.add(random.random() * 0.4, 0.6))
        )

        return blended

    def apply_self_modifications(self, agent):
        """
        Apply HyperMorphic self-modifications to improve system capabilities
        using zero-free operations and holomorphic transformations.

        Parameters:
        - agent: Agent to modify

        Returns:
        - (success, message) tuple
        """
        # Identify potential areas for improvement based on performance metrics
        if not self.performance_metrics or len(self.performance_metrics) < 10:
            return False, "Insufficient performance data for HyperMorphic self-modification"

        # Analyze recent performance trends with HyperMorphic statistics
        recent_metrics = self.performance_metrics[-10:]

        # Calculate zero-free average loss
        loss_sum = self.hyper_math.zero_free(sum(self.hyper_math.zero_free(m.get('loss', 0.5)) for m in recent_metrics))
        avg_loss = self.hyper_math.div(loss_sum, self.hyper_math.zero_free(len(recent_metrics)))

        # Calculate loss trend with zero-free operations
        first_loss = self.hyper_math.zero_free(recent_metrics[0].get('loss', 0.5))
        last_loss = self.hyper_math.zero_free(recent_metrics[-1].get('loss', 0.5))
        loss_trend = self.hyper_math.sub(last_loss, first_loss)

        modifications = []

        # 1. HyperMorphic architecture modifications with holomorphic constraints
        high_loss_threshold = self.hyper_math.zero_free(0.3)
        if self.hyper_math.sub(avg_loss, high_loss_threshold) > 0 and loss_trend > 0:
            # Loss is high and getting worse - try architecture expansion
            if hasattr(agent.model, 'expand_architecture'):
                agent.model.expand_architecture()
                modifications.append("Expanded neural architecture using HyperMorphic principles")

                # Apply holomorphic constraint optimization if available
                if hasattr(agent.model, 'optimize_holomorphic_constraints'):
                    agent.model.optimize_holomorphic_constraints()
                    modifications.append("Applied holomorphic constraint optimization to preserve structure")

        # Check for low loss with large architecture
        elif avg_loss < self.hyper_math.zero_free(0.2) and hasattr(agent.model, 'neocortex') and len(agent.model.neocortex) > 10:
            # Loss is low with large architecture - try pruning with dimension preservation
            if hasattr(agent.model, 'contract_architecture'):
                agent.model.contract_architecture()
                modifications.append("Pruned neural architecture with HyperMorphic dimension preservation")

        # 2. HyperMorphic memory optimizations with zero-free thresholds
        if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'memory_set'):
            memory_size = len(agent.free_will.memory_set)
            capacity_threshold = self.hyper_math.mul(self.hyper_math.zero_free(MEMORY_MAX_SIZE), self.hyper_math.zero_free(0.9))

            if memory_size > capacity_threshold:
                # Memory approaching capacity - trigger HyperMorphic pruning
                target_size = int(self.hyper_math.mul(self.hyper_math.zero_free(MEMORY_MAX_SIZE), self.hyper_math.zero_free(0.7)))

                if hasattr(agent.free_will, 'contract_memory'):
                    agent.free_will.contract_memory(target_size)
                    modifications.append(f"Optimized memory from {memory_size} to {len(agent.free_will.memory_set)} items using HyperMorphic pruning")
                elif hasattr(agent.free_will, '_optimize_memory_entropy'):
                    # Use entropy optimization if available
                    agent.free_will._optimize_memory_entropy()
                    modifications.append(f"Optimized memory entropy from {memory_size} to {len(agent.free_will.memory_set)} items")

        # 3. HyperMorphic consciousness adjustments with zero-free awareness changes
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'consciousness'):
            consciousness = agent.ai_manager.consciousness

            # Check for HyperMorphic consciousness version
            is_hypermorphic = hasattr(consciousness, 'hyper_math')

            if avg_loss > self.hyper_math.zero_free(0.3):
                # High loss - try boosting reflective awareness
                if hasattr(consciousness, 'increase_awareness'):
                    # Use HyperMorphic awareness increase if available
                    increase_amount = self.hyper_math.zero_free(0.1)
                    consciousness.increase_awareness(increase_amount)

                    if hasattr(consciousness, 'current_state'):
                        consciousness.current_state = "reflective"

                        # Extra HyperMorphic adjustments if supported
                        if is_hypermorphic and hasattr(consciousness, 'quantum_state'):
                            # Increase amplitude for reflective state
                            consciousness.quantum_state["reflective"]["amplitude"] = self.hyper_math.mul(
                                consciousness.quantum_state["reflective"]["amplitude"],
                                self.hyper_math.zero_free(1.5)
                            )

                        modifications.append("Increased reflective awareness with HyperMorphic zero-free operations")

        # 4. Apply meta-learning adjustments if available
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'meta_learning'):
            meta_learning = agent.ai_manager.meta_learning

            # If we have high loss, adjust exploration rate
            if avg_loss > self.hyper_math.zero_free(0.3):
                if hasattr(meta_learning, 'meta_params'):
                    old_rate = meta_learning.meta_params.get("exploration_rate", 0.3)
                    new_rate = self.hyper_math.min(self.hyper_math.zero_free(0.5),
                                                 self.hyper_math.mul(self.hyper_math.zero_free(old_rate),
                                                                    self.hyper_math.zero_free(1.2)))
                    meta_learning.meta_params["exploration_rate"] = new_rate
                    modifications.append(f"Adjusted meta-learning exploration rate using HyperMorphic scaling: {old_rate:.2f} → {new_rate:.2f}")

        # 5. Apply HyperMorphic quantum neural adjustments
        if hasattr(agent, 'model') and hasattr(agent.model, 'quantum_coherence'):
            old_coherence = agent.model.quantum_coherence

            # Adjust quantum coherence based on loss trend
            if loss_trend > 0:  # Loss increasing
                # Decrease coherence to allow more quantum exploration
                new_coherence = self.hyper_math.max(
                    self.hyper_math.zero_free(0.3),
                    self.hyper_math.mul(old_coherence, self.hyper_math.zero_free(0.9))
                )
            else:  # Loss stable or decreasing
                # Increase coherence to stabilize good performance
                new_coherence = self.hyper_math.min(
                    self.hyper_math.zero_free(0.9),
                    self.hyper_math.mul(old_coherence, self.hyper_math.zero_free(1.1))
                )

            agent.model.quantum_coherence = new_coherence
            modifications.append(f"Adjusted quantum coherence with HyperMorphic scaling: {old_coherence:.2f} → {new_coherence:.2f}")

        if modifications:
            log_event(f"HyperMorphic self-modification applied: {', '.join(modifications)}", "QUANTUM")
            return True, f"Successfully applied {len(modifications)} HyperMorphic self-modifications"

        return False, "No beneficial HyperMorphic self-modifications identified"


    
    def evolve_system(self, agent):
        """
        Trigger system evolution based on HyperMorphic performance metrics,
        goals, and environmental requirements, using zero-free operations.

        Parameters:
          - agent: Agent instance to evolve

        Returns:
          - (success, message) tuple
        """
        # Store agent reference for other methods to use
        self.agent = agent

        # Basic validation with HyperMorphic error handling
        if not agent:
            return False, "Invalid agent provided to HyperMorphic evolution engine."

        # Determine current cycle count with zero-free operations
        cycle_count = 0
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'cycle_counter'):
            cycle_count = agent.ai_manager.cycle_counter
        else:
            cycle_count = getattr(agent, 'stats', {}).get('cycles_run', 0)

        # Evolution interval check with HyperMorphic comparison
        evolution_interval = SELF_MODIFY_INTERVAL  # Using global configuration
        cycles_since_last = self.hyper_math.sub(self.hyper_math.zero_free(cycle_count),
                                             self.hyper_math.zero_free(self.last_major_evolution))

        evolution_interval_zero_free = self.hyper_math.zero_free(evolution_interval)
        if cycles_since_last < evolution_interval_zero_free:
            return False, f"Evolution interval not reached. {cycles_since_last}/{evolution_interval_zero_free} cycles since last evolution."

        # Increase generation counter with HyperMorphic increment
        self.evolution_generation += 1
        
        # Quick evolution path (20% chance) for faster processing
        if random.random() < 0.2:  # 20% chance to take quick path
            # Quick evolution without extensive analysis
            strategy_names = list(self.strategies.keys())
            selected_strategy = random.choice(strategy_names)
            
            # Apply the selected strategy directly
            success, message, changes = self._apply_evolution_strategy(agent, selected_strategy, {})
            
            # Record minimal evolution history
            evolution_record = {
                "generation": self.evolution_generation,
                "cycle": cycle_count,
                "strategy": selected_strategy,
                "success": success,
                "message": message,
                "changes": changes,
                "timestamp": datetime.now().isoformat()
            }
            self.evolution_history.append(evolution_record)
            
            if success:
                self.last_major_evolution = cycle_count
                log_event(f"Quick HyperMorphic evolution with strategy '{selected_strategy}': {message}", "QUANTUM")
                
            return success, message

        # First try self-modification approach if we have enough performance data
        if len(self.performance_metrics) >= 10:
            success, message = self.apply_self_modifications(agent)
            if success:
                # Record the evolution attempt in HyperMorphic history
                evolution_record = {
                    "generation": self.evolution_generation,
                    "cycle": cycle_count,
                    "strategy": "hypermorphic_self_modification",
                    "success": True,
                    "message": message,
                    "changes": message,
                    "timestamp": datetime.now().isoformat(),
                    "holomorphic_preservation": self.hyper_math.zero_free(0.85)  # High preservation for self-mods
                }
                self.evolution_history.append(evolution_record)
                self.last_major_evolution = cycle_count
                log_event(f"HyperMorphic system evolution through self-modification: {message}", "QUANTUM")
                return True, message

        # Continue with standard evolution if self-modification didn't succeed
        # Analyze current state and fitness using HyperMorphic evaluation
        fitness_scores = self._evaluate_system_fitness(agent)
        evolutionary_pressure = self._calculate_evolutionary_pressure(fitness_scores)

        # Determine if evolution should occur based on pressure and randomness
        # Evolution threshold lowered for faster evolution
        evolution_threshold = self.hyper_math.zero_free(0.1)  # Reduced from 0.3
        evolution_probability = self.hyper_math.add(evolution_threshold, evolutionary_pressure)

        if self.hyper_math.zero_free(random.random()) >= evolution_probability:
            return False, f"Evolution check passed, but probability threshold not met ({evolution_probability:.2f})."

        # System will evolve - select strategy using simplified approach
        selected_strategy = self._select_evolution_strategy(fitness_scores)

        # Apply the selected evolutionary strategy
        success, message, changes = self._apply_evolution_strategy(agent, selected_strategy, fitness_scores)

        # Record the evolution attempt in HyperMorphic history
        evolution_record = {
            "generation": self.evolution_generation,
            "cycle": cycle_count,
            "strategy": selected_strategy,
            "success": success,
            "message": message,
            "changes": changes,
            "fitness_before": fitness_scores,
            "timestamp": datetime.now().isoformat()
        }

        self.evolution_history.append(evolution_record)

        # Update last evolution time if successful with HyperMorphic state tracking
        if success:
            self.last_major_evolution = cycle_count

            # Update strategy success rates based on outcome
            self._update_strategy_success_rates(selected_strategy, True)

            log_event(f"HyperMorphic system evolution successful: Generation {self.evolution_generation}, Strategy: {selected_strategy}", "QUANTUM")
        else:
            # Update strategy success rates for failure
            self._update_strategy_success_rates(selected_strategy, False)

            log_event(f"HyperMorphic system evolution attempt failed: {message}", "WARNING")

        return success, message
    
    def _quick_assess_capability_levels(self, agent):
        """Simplified capability assessment for quick evolution"""
        for capability in self.capability_scores.keys():
            # Random improvement with slight upward bias
            current = self.capability_scores[capability]
            change = self.hyper_math.zero_free((random.random() * 0.1) - 0.03)  # Slight upward bias
            self.capability_scores[capability] = self.hyper_math.max(
                self.hyper_math.zero_free(0.1),
                self.hyper_math.min(
                    self.hyper_math.zero_free(0.95),
                    self.hyper_math.add(current, change)
                )
            )
        # Calculate simple convergence score
        avg_score = sum(self.capability_scores.values()) / len(self.capability_scores)
        self.convergence_score = self.hyper_math.zero_free(avg_score)
    
    def quick_evolve_system(self, agent):
        """Quick system evolution with minimal overhead"""
        self.evolution_generation += 1
        # Select random strategy
        strategy = random.choice(list(self.strategies.keys()))
        # Apply with minimal analysis
        success, message, changes = self._apply_evolution_strategy(agent, strategy, {})
        # Record minimal evolution history
        if success:
            self.last_major_evolution = getattr(agent, 'ai_manager', {}).get('cycle_counter', 0)
            log_event(f"Quick evolution successful with strategy: {strategy}", "QUANTUM")
        return success, message
    

    def _calculate_dimension_weights(self, fitness_scores):
        """
        Calculate dimension weights for strategy selection based on current fitness.
        Uses HyperMorphic zero-free operations for stability.
        """
        dimension_weights = [self.hyper_math.zero_free(0.5) for _ in range(self.evolutionary_dimensions)]

        # Adjust weights based on fitness scores
        if "performance" in fitness_scores:
            # Dimension 0 focuses on performance
            dimension_weights[0] = self.hyper_math.add(
                self.hyper_math.mul(self.hyper_math.zero_free(0.7), self.hyper_math.zero_free(fitness_scores["performance"])),
                self.hyper_math.mul(self.hyper_math.zero_free(0.3), dimension_weights[0])
            )

        if "efficiency" in fitness_scores:
            # Dimension 1 focuses on efficiency
            dimension_weights[1] = self.hyper_math.add(
                self.hyper_math.mul(self.hyper_math.zero_free(0.7), self.hyper_math.zero_free(fitness_scores["efficiency"])),
                self.hyper_math.mul(self.hyper_math.zero_free(0.3), dimension_weights[1])
            )

        if "adaptability" in fitness_scores:
            # Dimension 2 focuses on adaptability
            dimension_weights[2] = self.hyper_math.add(
                self.hyper_math.mul(self.hyper_math.zero_free(0.7), self.hyper_math.zero_free(fitness_scores["adaptability"])),
                self.hyper_math.mul(self.hyper_math.zero_free(0.3), dimension_weights[2])
            )

        if "robustness" in fitness_scores:
            # Dimension 3 focuses on robustness
            dimension_weights[3] = self.hyper_math.add(
                self.hyper_math.mul(self.hyper_math.zero_free(0.7), self.hyper_math.zero_free(fitness_scores["robustness"])),
                self.hyper_math.mul(self.hyper_math.zero_free(0.3), dimension_weights[3])
            )

        if "complexity" in fitness_scores:
            # Dimension 4 focuses on complexity (inverse relation)
            complexity_score = self.hyper_math.sub(self.hyper_math.zero_free(1.0),
                                                self.hyper_math.zero_free(fitness_scores["complexity"]))
            dimension_weights[4] = self.hyper_math.add(
                self.hyper_math.mul(self.hyper_math.zero_free(0.7), complexity_score),
                self.hyper_math.mul(self.hyper_math.zero_free(0.3), dimension_weights[4])
            )

        return dimension_weights




    def _evaluate_system_fitness(self, agent):
        """
        Evaluate the fitness of the current system across multiple dimensions
        using HyperMorphic zero-free operations for stable assessment.

        Parameters:
        - agent: Agent to evaluate

        Returns:
        - Dictionary of fitness scores across dimensions
        """
        # Initialize fitness scores with zero-free values
        fitness_scores = {
            "performance": self.hyper_math.zero_free(0.5),
            "efficiency": self.hyper_math.zero_free(0.5),
            "adaptability": self.hyper_math.zero_free(0.5),
            "robustness": self.hyper_math.zero_free(0.5),
            "complexity": self.hyper_math.zero_free(0.5)
        }

        # 1. Evaluate performance based on agent statistics
        if hasattr(agent, 'stats') and isinstance(agent.stats, dict):
            cycles = self.hyper_math.zero_free(agent.stats.get("cycles_run", 0))
            pages = self.hyper_math.zero_free(agent.stats.get("pages_processed", 0))

            if cycles > self.hyper_math.epsilon:
                # Calculate performance ratio with zero-free division
                performance = self.hyper_math.div(pages, cycles)

                # Normalize performance to a 0.1-0.9 range with zero-free operations
                fitness_scores["performance"] = self.hyper_math.min(
                    self.hyper_math.zero_free(0.9),
                    self.hyper_math.max(
                        self.hyper_math.zero_free(0.1),
                        self.hyper_math.div(performance, self.hyper_math.zero_free(10.0))
                    )
                )

        # 2. Evaluate efficiency based on model complexity
        model_size = 0
        if hasattr(agent, 'model') and hasattr(agent.model, 'neocortex'):
            model_size = len(agent.model.neocortex)

            # Base efficiency on optimal model size (inversely related to size beyond optimal)
            base_size = 8  # Expected baseline size

            if model_size <= base_size:
                # Smaller models are more efficient
                efficiency_score = self.hyper_math.min(
                    self.hyper_math.zero_free(0.9),
                    self.hyper_math.mul(
                        self.hyper_math.zero_free(0.1 * model_size),
                        self.hyper_math.zero_free(0.9)
                    )
                )
            else:
                # Efficiency decreases as model grows beyond base size
                size_factor = self.hyper_math.div(
                    self.hyper_math.zero_free(base_size),
                    self.hyper_math.max(self.hyper_math.zero_free(base_size), self.hyper_math.zero_free(model_size))
                )
                efficiency_score = self.hyper_math.max(
                    self.hyper_math.zero_free(0.2),
                    self.hyper_math.min(
                        self.hyper_math.zero_free(0.9),
                        size_factor
                    )
                )

            fitness_scores["efficiency"] = efficiency_score

            # Update complexity score based on model size
            complexity_score = self.hyper_math.min(
                self.hyper_math.zero_free(0.9),
                self.hyper_math.div(
                    self.hyper_math.zero_free(model_size),
                    self.hyper_math.zero_free(15.0)  # Normalize to 15 layers max
                )
            )
            fitness_scores["complexity"] = complexity_score

        # 3. Evaluate adaptability based on learning rate adjustments
        adaptability_score = self.hyper_math.zero_free(0.5)  # Default
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'meta_learning'):
            meta_learning = agent.ai_manager.meta_learning

            # More learning rate adjustments suggests higher adaptability
            if hasattr(meta_learning, 'learning_rate_schedule'):
                adjustments = len(meta_learning.learning_rate_schedule)

                # Calculate adaptability with zero-free operations
                adaptability_score = self.hyper_math.min(
                    self.hyper_math.zero_free(0.9),
                    self.hyper_math.add(
                        self.hyper_math.zero_free(0.4),
                        self.hyper_math.mul(
                            self.hyper_math.zero_free(0.1),
                            self.hyper_math.min(self.hyper_math.zero_free(5.0), self.hyper_math.zero_free(adjustments))
                        )
                    )
                )

            # Add architecture adaptability if available
            if hasattr(meta_learning, 'architecture_history') and meta_learning.architecture_history:
                arch_changes = len(meta_learning.architecture_history)
                arch_score = self.hyper_math.min(
                    self.hyper_math.zero_free(0.2),
                    self.hyper_math.mul(
                        self.hyper_math.zero_free(0.05),
                        self.hyper_math.zero_free(arch_changes)
                    )
                )
                adaptability_score = self.hyper_math.min(
                    self.hyper_math.zero_free(0.9),
                    self.hyper_math.add(adaptability_score, arch_score)
                )

        fitness_scores["adaptability"] = adaptability_score

        # 4. Evaluate robustness based on error recovery
        robustness_score = self.hyper_math.zero_free(0.5)  # Default
        if hasattr(agent, 'ai_manager'):
            # Check error recovery attempts
            error_recovery = getattr(agent.ai_manager, 'error_recovery_attempts', 0)

            if error_recovery > 0:
                # Lower recovery attempts = more robust
                robustness_score = self.hyper_math.max(
                    self.hyper_math.zero_free(0.1),
                    self.hyper_math.sub(
                        self.hyper_math.zero_free(0.9),
                        self.hyper_math.mul(
                            self.hyper_math.zero_free(0.1),
                            self.hyper_math.min(self.hyper_math.zero_free(8.0), self.hyper_math.zero_free(error_recovery))
                        )
                    )
                )
            else:
                # No error recovery needed = highly robust
                robustness_score = self.hyper_math.zero_free(0.8)

            # Check for error rates in domain stats if available
            if hasattr(agent, 'stats') and 'domain_stats' in agent.stats:
                domain_stats = agent.stats['domain_stats']
                if domain_stats and isinstance(domain_stats, dict):
                    total_errors = sum(stats.get('error_count', 0) for stats in domain_stats.values())
                    total_visits = sum(stats.get('visits', 0) for stats in domain_stats.values())

                    if total_visits > 0:
                        error_rate = self.hyper_math.div(
                            self.hyper_math.zero_free(total_errors),
                            self.hyper_math.zero_free(max(1, total_visits))
                        )

                        # Lower error rate = more robust
                        error_robustness = self.hyper_math.sub(
                            self.hyper_math.zero_free(1.0),
                            self.hyper_math.min(self.hyper_math.zero_free(1.0), error_rate)
                        )

                        # Combine with existing robustness score
                        robustness_score = self.hyper_math.add(
                            self.hyper_math.mul(robustness_score, self.hyper_math.zero_free(0.6)),
                            self.hyper_math.mul(error_robustness, self.hyper_math.zero_free(0.4))
                        )

        fitness_scores["robustness"] = robustness_score

        # 5. Advanced HyperMorphic capability assessment
        self._assess_capability_levels(agent)

        # Store the current fitness scores for tracking
        self.fitness_metrics = fitness_scores

        return fitness_scores

    def _assess_capability_levels(self, agent):
        """
        Assess capability levels of the agent across multiple dimensions using 
        HyperMorphic zero-free operations.
        
        Parameters:
        - agent: Agent to evaluate
        
        Updates:
        - self.capability_scores dictionary with new capability assessments
        """
        # Initialize with current scores as a basis
        new_capability_scores = dict(self.capability_scores)
        
        # 1. Assess knowledge representation capability
        if hasattr(agent, 'model') and hasattr(agent.model, 'neocortex'):
            # Knowledge representation scales with model size and embeddings quality
            model_size = len(agent.model.neocortex)
            
            # Calculate knowledge capacity score
            knowledge_capacity = self.hyper_math.min(
                self.hyper_math.zero_free(0.9),
                self.hyper_math.div(
                    self.hyper_math.zero_free(model_size),
                    self.hyper_math.zero_free(10.0)  # Normalize to 10 layers max
                )
            )
            
            # Check for embedding quality if available
            embedding_quality = self.hyper_math.zero_free(0.5)  # Default
            if hasattr(agent.model, 'embedding_quality'):
                embedding_quality = self.hyper_math.zero_free(agent.model.embedding_quality)
            
            # Combine factors for knowledge representation score
            knowledge_rep_score = self.hyper_math.add(
                self.hyper_math.mul(knowledge_capacity, self.hyper_math.zero_free(0.6)),
                self.hyper_math.mul(embedding_quality, self.hyper_math.zero_free(0.4))
            )
            
            # Update knowledge representation capability
            new_capability_scores["knowledge_representation"] = self.hyper_math.max(
                new_capability_scores["knowledge_representation"],
                knowledge_rep_score
            )
        
        # 2. Assess planning capability
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'temporal_planner'):
            planner = agent.ai_manager.temporal_planner
            
            # Planning capability based on goals and planning depth
            planning_score = self.hyper_math.zero_free(0.5)  # Default
            
            # Check for long-term goals
            if hasattr(planner, 'long_term_goals') and planner.long_term_goals:
                # More goals = better planning capability (up to a point)
                goal_count = len(planner.long_term_goals)
                goal_score = self.hyper_math.min(
                    self.hyper_math.zero_free(0.9),
                    self.hyper_math.add(
                        self.hyper_math.zero_free(0.4),
                        self.hyper_math.mul(
                            self.hyper_math.zero_free(0.1),
                            self.hyper_math.zero_free(min(5, goal_count))
                        )
                    )
                )
                
                # Check for planning horizon if available
                horizon_score = self.hyper_math.zero_free(0.5)  # Default
                if hasattr(planner, 'planning_horizon'):
                    horizon = planner.planning_horizon
                    horizon_score = self.hyper_math.min(
                        self.hyper_math.zero_free(0.9),
                        self.hyper_math.div(
                            self.hyper_math.zero_free(horizon),
                            self.hyper_math.zero_free(10.0)  # Normalize to 10-step horizon
                        )
                    )
                
                # Combined planning score
                planning_score = self.hyper_math.add(
                    self.hyper_math.mul(goal_score, self.hyper_math.zero_free(0.7)),
                    self.hyper_math.mul(horizon_score, self.hyper_math.zero_free(0.3))
                )
            
            # Update planning capability
            new_capability_scores["planning"] = self.hyper_math.max(
                new_capability_scores["planning"],
                planning_score
            )
        
        # 3. Assess learning capability
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'meta_learning'):
            meta_learning = agent.ai_manager.meta_learning
            
            # Learning capability based on adaptation rate and history
            learning_score = self.hyper_math.zero_free(0.4)  # Default baseline
            
            # Check learning rate history
            if hasattr(meta_learning, 'learning_rate_history') and meta_learning.learning_rate_history:
                # More adjustments = better adaptation
                adjustment_count = len(meta_learning.learning_rate_history)
                
                # Calculate adjustment score with diminishing returns
                adjustment_score = self.hyper_math.min(
                    self.hyper_math.zero_free(0.9),
                    self.hyper_math.add(
                        self.hyper_math.zero_free(0.3),
                        self.hyper_math.mul(
                            self.hyper_math.zero_free(0.1),
                            self.hyper_math.zero_free(min(6, adjustment_count))
                        )
                    )
                )
                
                # If available, check for learning curve metrics
                curve_score = self.hyper_math.zero_free(0.5)  # Default
                if hasattr(meta_learning, 'learning_curve'):
                    # Calculate learning slope
                    if len(meta_learning.learning_curve) >= 2:
                        recent_slope = self.hyper_math.zero_free(
                            meta_learning.learning_curve[-1] - meta_learning.learning_curve[0]
                        )
                        
                        # Steeper slope = faster learning
                        curve_score = self.hyper_math.min(
                            self.hyper_math.zero_free(0.9),
                            self.hyper_math.max(
                                self.hyper_math.zero_free(0.1),
                                self.hyper_math.mul(
                                    self.hyper_math.zero_free(2.0),
                                    recent_slope
                                )
                            )
                        )
                
                # Combined learning score
                learning_score = self.hyper_math.add(
                    self.hyper_math.mul(adjustment_score, self.hyper_math.zero_free(0.6)),
                    self.hyper_math.mul(curve_score, self.hyper_math.zero_free(0.4))
                )
            
            # Update learning capability
            new_capability_scores["learning"] = self.hyper_math.max(
                new_capability_scores["learning"],
                learning_score
            )
        
        # 4. Assess error handling capability
        error_handling_score = self.hyper_math.zero_free(0.5)  # Default
        
        if hasattr(agent, 'ai_manager'):
            # Check error recovery success rate if available
            if hasattr(agent.ai_manager, 'error_recovery_rate'):
                recovery_rate = self.hyper_math.zero_free(agent.ai_manager.error_recovery_rate)
                
                # Higher recovery rate = better error handling
                error_handling_score = self.hyper_math.min(
                    self.hyper_math.zero_free(0.9),
                    self.hyper_math.max(
                        self.hyper_math.zero_free(0.2),
                        recovery_rate
                    )
                )
            elif hasattr(agent.ai_manager, 'error_recovery_attempts'):
                # Calculate success rate from attempts if available
                attempts = self.hyper_math.zero_free(agent.ai_manager.error_recovery_attempts)
                successes = self.hyper_math.zero_free(getattr(agent.ai_manager, 'error_recovery_successes', 0))
                
                if attempts > self.hyper_math.epsilon:
                    recovery_rate = self.hyper_math.div(successes, attempts)
                    error_handling_score = self.hyper_math.min(
                        self.hyper_math.zero_free(0.9),
                        self.hyper_math.max(
                            self.hyper_math.zero_free(0.2),
                            recovery_rate
                        )
                    )
        
        # Update error handling capability
        new_capability_scores["error_handling"] = self.hyper_math.max(
            new_capability_scores["error_handling"],
            error_handling_score
        )
        
        # 5. Assess creative synthesis capability
        creative_score = self.hyper_math.zero_free(0.4)  # Default baseline
        
        if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'imagination'):
            imagination = agent.ai_manager.imagination
            
            # Creativity based on imagination parameters
            if hasattr(imagination, 'creativity_level'):
                creativity_level = self.hyper_math.zero_free(imagination.creativity_level)
                
                # Direct mapping with some scaling
                creative_score = self.hyper_math.min(
                    self.hyper_math.zero_free(0.9),
                    self.hyper_math.max(
                        self.hyper_math.zero_free(0.2),
                        self.hyper_math.mul(
                            creativity_level,
                            self.hyper_math.zero_free(1.2)  # Scale up slightly
                        )
                    )
                )
            
            # If available, check for divergent thinking metrics
            if hasattr(imagination, 'divergent_thinking_score'):
                divergent_score = self.hyper_math.zero_free(imagination.divergent_thinking_score)
                
                # Combine with existing score
                creative_score = self.hyper_math.add(
                    self.hyper_math.mul(creative_score, self.hyper_math.zero_free(0.7)),
                    self.hyper_math.mul(divergent_score, self.hyper_math.zero_free(0.3))
                )
        
        # Update creative synthesis capability
        new_capability_scores["creative_synthesis"] = self.hyper_math.max(
            new_capability_scores["creative_synthesis"],
            creative_score
        )
        
        # 6. Assess quantum superposition capability (HyperMorphic specific)
        quantum_score = self.hyper_math.zero_free(0.3)  # Default baseline
        
        # Check for quantum-specific attributes
        if hasattr(agent, 'model') and hasattr(agent.model, 'quantum_coherence'):
            coherence = self.hyper_math.zero_free(agent.model.quantum_coherence)
            
            # Quantum capability based on coherence level
            quantum_score = self.hyper_math.min(
                self.hyper_math.zero_free(0.9),
                self.hyper_math.mul(
                    coherence,
                    self.hyper_math.zero_free(1.1)  # Scale up slightly
                )
            )
            
            # Check for phase alignment if available
            if hasattr(agent.model, 'phase_alignment'):
                alignment = self.hyper_math.zero_free(agent.model.phase_alignment)
                
                # Combine with existing score
                quantum_score = self.hyper_math.add(
                    self.hyper_math.mul(quantum_score, self.hyper_math.zero_free(0.8)),
                    self.hyper_math.mul(alignment, self.hyper_math.zero_free(0.2))
                )
        
        # Update quantum superposition capability
        new_capability_scores["quantum_superposition"] = self.hyper_math.max(
            new_capability_scores["quantum_superposition"],
            quantum_score
        )
        
        # 7. Assess holomorphic processing capability (HyperMorphic specific)
        holomorphic_score = self.hyper_math.zero_free(0.4)  # Default baseline
        
        # Check for holomorphic attributes
        if hasattr(agent, 'model') and hasattr(agent.model, '_apply_holomorphic_transform'):
            # Base score on holomorphic preservation factor
            holomorphic_score = self.hyper_math.min(
                self.hyper_math.zero_free(0.9),
                self.hyper_math.max(
                    self.hyper_math.zero_free(0.3),
                    self.hyper_math.mul(
                        self.holomorphic_preservation,
                        self.hyper_math.zero_free(1.2)  # Scale up
                    )
                )
            )
            
            # If available, check for structure preservation metrics
            if hasattr(agent.model, 'structure_preservation_rate'):
                preservation = self.hyper_math.zero_free(agent.model.structure_preservation_rate)
                
                # Combine with existing score
                holomorphic_score = self.hyper_math.add(
                    self.hyper_math.mul(holomorphic_score, self.hyper_math.zero_free(0.6)),
                    self.hyper_math.mul(preservation, self.hyper_math.zero_free(0.4))
                )
        
        # Update holomorphic processing capability
        new_capability_scores["holomorphic_processing"] = self.hyper_math.max(
            new_capability_scores["holomorphic_processing"],
            holomorphic_score
        )
        
        # Apply adaptation rate to smooth capability changes
        # This prevents dramatic capability swings between assessments
        for capability in self.capability_scores.keys():
            if capability in new_capability_scores:
                # Get current and new scores
                current = self.capability_scores[capability]
                new_value = new_capability_scores[capability]
                
                # Calculate adaptive update with momentum
                updated_value = self.hyper_math.add(
                    self.hyper_math.mul(current, self.hyper_math.sub(
                        self.hyper_math.zero_free(1.0),
                        self.adaptation_rate
                    )),
                    self.hyper_math.mul(new_value, self.adaptation_rate)
                )
                
                # Store updated value
                self.capability_scores[capability] = updated_value
        
        # Calculate convergence score to track evolutionary progress
        # Higher convergence = closer to target capabilities
        total_gap = self.hyper_math.zero_free(0.0)
        for capability, target in self.capability_targets.items():
            current = self.capability_scores.get(capability, self.hyper_math.zero_free(0.0))
            gap = self.hyper_math.sub(target, current)
            gap = self.hyper_math.max(self.hyper_math.zero_free(0.0), gap)  # Only count shortfalls
            total_gap = self.hyper_math.add(total_gap, gap)
        
        # Invert gap to get convergence (higher = better)
        avg_gap = self.hyper_math.div(
            total_gap,
            self.hyper_math.zero_free(len(self.capability_targets))
        )
        self.convergence_score = self.hyper_math.sub(
            self.hyper_math.zero_free(1.0),
            avg_gap
        )

    
    def _calculate_evolutionary_pressure(self, fitness_scores):
        """
        Calculate the evolutionary pressure based on fitness scores and goal weights
        using HyperMorphic zero-free operations for stable, non-collapsing results.

        Parameters:
        - fitness_scores: Current fitness evaluation

        Returns:
        - Pressure score (0.0-1.0) indicating how strongly evolution is needed
        """
        # No evolution pressure if no fitness scores
        if not fitness_scores:
            return self.hyper_math.zero_free(0.0)

        # Calculate weighted fitness using HyperMorphic operations
        weighted_fitness = self.hyper_math.zero_free(0.0)
        total_weight = self.hyper_math.zero_free(0.0)

        for metric, weight in self.goal_weights.items():
            if metric in fitness_scores:
                score = self.hyper_math.zero_free(fitness_scores[metric])
                weight_abs = self.hyper_math.zero_free(abs(weight))

                # If weight is negative (like for complexity), invert the score
                if weight < 0:
                    score = self.hyper_math.sub(self.hyper_math.zero_free(1.0), score)

                # Calculate weighted contribution with zero-free multiplication
                weighted_contribution = self.hyper_math.mul(score, weight_abs)

                # Add to weighted fitness with zero-free addition
                weighted_fitness = self.hyper_math.add(weighted_fitness, weighted_contribution)

                # Add to total weight
                total_weight = self.hyper_math.add(total_weight, weight_abs)

        # Normalize with HyperMorphic division (avoid division by zero)
        if total_weight > self.hyper_math.epsilon:
            avg_fitness = self.hyper_math.div(weighted_fitness, total_weight)
        else:
            avg_fitness = self.hyper_math.zero_free(0.5)  # Default

        # Calculate pressure: lower fitness = higher pressure with HyperMorphic subtraction
        # Add diminishing returns below 0.3 fitness
        if avg_fitness < self.hyper_math.zero_free(0.3):
            # High pressure increased
            pressure = self.hyper_math.zero_free(0.9)  # Increased from 0.8
        else:
            # Linear scaling with higher multiplier: lower fitness = higher pressure
            pressure = self.hyper_math.mul(
                self.hyper_math.zero_free(0.9),  # Increased from 0.8
                self.hyper_math.sub(self.hyper_math.zero_free(1.0), avg_fitness)
            )

        # Add adaptability bias: systems that are more adaptable get more evolution
        adaptability = self.hyper_math.zero_free(fitness_scores.get("adaptability", 0.5))
        adaptability_boost = self.hyper_math.mul(self.hyper_math.zero_free(0.2), adaptability)
        pressure = self.hyper_math.add(pressure, adaptability_boost)

        # Add quantum randomness factor with HyperMorphic constraint
        randomness = self.hyper_math.mul(
            self.hyper_math.zero_free(0.1),
            self.hyper_math.zero_free(random.random())
        )
        pressure = self.hyper_math.add(pressure, randomness)

        # Clamp to valid range with HyperMorphic min/max
        pressure = self.hyper_math.max(
            self.hyper_math.zero_free(0.0),
            self.hyper_math.min(self.hyper_math.zero_free(1.0), pressure)
        )

        return pressure


    
    def _select_evolution_strategy(self, fitness_scores, dimension_weights=None):
        """
        Select an appropriate evolution strategy using simplified logic
        for faster decision making.

        Parameters:
        - fitness_scores: Current fitness evaluation
        - dimension_weights: Optional weights for different evolutionary dimensions

        Returns:
        - Selected strategy name
        """
        # Quick path: 30% chance to use a fast selection path
        if random.random() < 0.3:
            # Quick random selection from top strategies
            top_strategies = sorted(self.strategies.items(), 
                                   key=lambda x: x[1].get("success_rate", 0.5), 
                                   reverse=True)[:3]
            if top_strategies:
                return random.choice(top_strategies)[0]
        
        if not fitness_scores:
            # Default to expansion if no scores
            return "expansion"

        # Calculate weighted probability for each strategy
        strategy_weights = {}

        # Strategy 1: Expansion - good when performance is low but efficiency is high
        perf = self.hyper_math.zero_free(fitness_scores.get("performance", 0.5))
        eff = self.hyper_math.zero_free(fitness_scores.get("efficiency", 0.5))
        
        if perf < self.hyper_math.zero_free(0.6) and eff > self.hyper_math.zero_free(0.7):
            strategy_weights["expansion"] = self.hyper_math.mul(
                self.hyper_math.zero_free(0.7),
                self.strategies["expansion"]["success_rate"]
            )
        else:
            strategy_weights["expansion"] = self.hyper_math.mul(
                self.hyper_math.zero_free(0.3),
                self.strategies["expansion"]["success_rate"]
            )

        # Strategy 2: Pruning - good when efficiency is low but performance is decent
        if eff < self.hyper_math.zero_free(0.5) and perf > self.hyper_math.zero_free(0.6):
            strategy_weights["pruning"] = self.hyper_math.mul(
                self.hyper_math.zero_free(0.8),
                self.strategies["pruning"]["success_rate"]
            )
        else:
            strategy_weights["pruning"] = self.hyper_math.mul(
                self.hyper_math.zero_free(0.3),
                self.strategies["pruning"]["success_rate"]
            )

        # Strategy 3: Restructuring - good when adaptability is low
        adapt = self.hyper_math.zero_free(fitness_scores.get("adaptability", 0.5))
        if adapt < self.hyper_math.zero_free(0.5):
            strategy_weights["restructuring"] = self.hyper_math.mul(
                self.hyper_math.zero_free(0.7),
                self.strategies["restructuring"]["success_rate"]
            )
        else:
            strategy_weights["restructuring"] = self.hyper_math.mul(
                self.hyper_math.zero_free(0.4),
                self.strategies["restructuring"]["success_rate"]
            )

        # Strategy 4: Specialization - good for complex systems with good performance
        complex = self.hyper_math.zero_free(fitness_scores.get("complexity", 0.5))
        if complex > self.hyper_math.zero_free(0.7) and perf > self.hyper_math.zero_free(0.7):
            strategy_weights["specialization"] = self.hyper_math.mul(
                self.hyper_math.zero_free(0.8),
                self.strategies["specialization"]["success_rate"]
            )
        else:
            strategy_weights["specialization"] = self.hyper_math.mul(
                self.hyper_math.zero_free(0.2),
                self.strategies["specialization"]["success_rate"]
            )

        # Strategy 5: Integration - good for improving robustness
        robust = self.hyper_math.zero_free(fitness_scores.get("robustness", 0.5))
        if robust < self.hyper_math.zero_free(0.6):
            strategy_weights["integration"] = self.hyper_math.mul(
                self.hyper_math.zero_free(0.7),
                self.strategies["integration"]["success_rate"]
            )
        else:
            strategy_weights["integration"] = self.hyper_math.mul(
                self.hyper_math.zero_free(0.3),
                self.strategies["integration"]["success_rate"]
            )

        # Add randomness factor to promote exploration
        for strategy in strategy_weights:
            random_boost = self.hyper_math.zero_free(random.uniform(0, 0.3))
            strategy_weights[strategy] = self.hyper_math.add(
                strategy_weights[strategy],
                random_boost
            )

        # Select strategy with highest weight
        if strategy_weights:
            selected_strategy = max(strategy_weights.items(), key=lambda x: x[1])[0]
            return selected_strategy
        else:
            return "expansion"  # Default fallback

    def _apply_evolution_strategy(self, agent, strategy, fitness_scores):
        """
        Apply the selected evolution strategy to modify the agent using
        HyperMorphic zero-free operations and holomorphic transformations.

        Parameters:
        - agent: Agent to evolve
        - strategy: Selected strategy
        - fitness_scores: Current fitness scores

        Returns:
        - (success, message, changes) tuple
        """
        changes = []

        try:
            # Record strategy attempt
            if strategy in self.strategies:
                self.strategies[strategy]["last_attempt"] = self.evolution_generation

            # Handle regular strategies
            if strategy == "expansion":
                success, message, changes = self._apply_expansion_strategy(agent, changes)

            elif strategy == "pruning":
                success, message, changes = self._apply_pruning_strategy(agent, changes)

            elif strategy == "restructuring":
                success, message, changes = self._apply_restructuring_strategy(agent, changes)

            elif strategy == "specialization":
                success, message, changes = self._apply_specialization_strategy(agent, changes)

            elif strategy == "integration":
                success, message, changes = self._apply_integration_strategy(agent, changes)

            elif strategy == "quantum_variation":
                success, message, changes = self._apply_quantum_variation_strategy(agent, changes)

            elif strategy == "holomorphic_restructuring":
                success, message, changes = self._apply_holomorphic_restructuring_strategy(agent, changes)

            # Handle blended strategies (with "_blend" in their name)
            elif "_blend" in strategy:
                success, message, changes = self._apply_blended_strategy(agent, strategy, changes)

            else:
                success = False
                message = f"Unknown strategy: {strategy}"
                changes = []

        except Exception as e:
            success = False
            message = f"Error applying strategy {strategy}: {str(e)}"
            changes = []
            log_event(f"HyperMorphic evolution error: {str(e)}", "ERROR")

        return success, message, changes

    def _apply_expansion_strategy(self, agent, changes):
        """
        Apply expansion strategy to increase model capacity using HyperMorphic
        zero-free operations that preserve holomorphic structure.

        Parameters:
        - agent: Agent to evolve
        - changes: List to track changes

        Returns:
        - (success, message, changes) tuple
        """
        # Check for expandable model
        if not hasattr(agent, 'model') or not hasattr(agent.model, 'expand_architecture'):
            return False, "Model does not support HyperMorphic architecture expansion", changes

        try:
            # Expansion attempts:
            # 1. Try expanding the model architecture with HyperMorphic properties
            agent.model.expand_architecture()
            changes.append("Expanded neural architecture with HyperMorphic dimensional preservation")

            # 2. Check for HyperMorphic model version to apply specialized optimizations
            is_hypermorphic = hasattr(agent.model, 'hyper_math')

            if is_hypermorphic:
                # Apply zero-free optimization to expanded architecture
                if hasattr(agent.model, '_ensure_zero_free'):
                    # Ensure all model parameters are zero-free
                    with torch.no_grad():
                        for param in agent.model.parameters():
                            param.data = agent.model._ensure_zero_free(param.data)
                    changes.append("Applied HyperMorphic zero-free parameter constraints")

                # Apply holomorphic constraint optimization if available
                if hasattr(agent.model, 'optimize_holomorphic_constraints'):
                    agent.model.optimize_holomorphic_constraints()
                    changes.append("Applied holomorphic constraint optimization to preserve structure")

            # 3. If agent has adaptive learning, adjust its parameters for new architecture
            if hasattr(agent, 'adaptive_learning'):
                # Increase adaptation rate for new architecture
                adaptive_learning = agent.adaptive_learning
                if hasattr(adaptive_learning, 'meta_params'):
                    # Get current exploration rate with HyperMorphic safeguard
                    old_rate = self.hyper_math.zero_free(
                        adaptive_learning.meta_params.get("exploration_rate", 0.3)
                    )

                    # Calculate new rate with HyperMorphic operations
                    new_rate = self.hyper_math.min(
                        self.hyper_math.zero_free(0.5),  # Cap at 0.5
                        self.hyper_math.mul(old_rate, self.hyper_math.zero_free(1.2))  # 20% increase
                    )

                    adaptive_learning.meta_params["exploration_rate"] = new_rate
                    changes.append(f"Increased meta-learning exploration rate for new architecture: {old_rate:.2f} → {new_rate:.2f}")

            # 4. Adjust quantum coherence if available (HyperMorphic specific)
            if hasattr(agent.model, 'quantum_coherence'):
                old_coherence = agent.model.quantum_coherence

                # Reduce coherence initially to allow exploration of new architecture
                new_coherence = self.hyper_math.max(
                    self.hyper_math.zero_free(0.4),  # Minimum coherence
                    self.hyper_math.mul(old_coherence, self.hyper_math.zero_free(0.9))  # 10% reduction
                )

                agent.model.quantum_coherence = new_coherence
                changes.append(f"Adjusted quantum coherence for architecture exploration: {old_coherence:.2f} → {new_coherence:.2f}")

            # Update strategy success rate (increase on success)
            self.strategies["expansion"]["success_rate"] = self.hyper_math.min(
                self.hyper_math.zero_free(0.95),
                self.hyper_math.mul(
                    self.strategies["expansion"]["success_rate"],
                    self.hyper_math.zero_free(1.1)  # 10% increase in success rate
                )
            )

            return True, "Successfully expanded neural architecture with HyperMorphic dimensional preservation", changes

        except Exception as e:
            # Update strategy success rate (decrease on failure)
            self.strategies["expansion"]["success_rate"] = self.hyper_math.max(
                self.hyper_math.zero_free(0.2),
                self.hyper_math.mul(
                    self.strategies["expansion"]["success_rate"],
                    self.hyper_math.zero_free(0.9)  # 10% decrease in success rate
                )
            )

            return False, f"HyperMorphic architecture expansion failed: {str(e)}", changes

    def _apply_pruning_strategy(self, agent, changes):
        """
        Apply pruning strategy to reduce model size and increase efficiency
        while maintaining essential HyperMorphic structures.

        Parameters:
        - agent: Agent to evolve
        - changes: List to track changes

        Returns:
        - (success, message, changes) tuple
        """
        # Check for prunable model
        if not hasattr(agent, 'model') or not hasattr(agent.model, 'contract_architecture'):
            return False, "Model does not support HyperMorphic architecture pruning", changes

        try:
            # Pruning attempts with HyperMorphic constraints
            # 1. Try contracting the model architecture
            agent.model.contract_architecture()
            changes.append("Contracted neural architecture using HyperMorphic dimensional preservation")

            # 2. Detect and fix HyperMorphic model version
            is_hypermorphic = hasattr(agent.model, 'hyper_math')

            if is_hypermorphic:
                # Ensure zero-free constraints are maintained after pruning
                if hasattr(agent.model, '_ensure_zero_free'):
                    # Apply zero-free constraints to all parameters
                    with torch.no_grad():
                        for param in agent.model.parameters():
                            param.data = agent.model._ensure_zero_free(param.data)
                    changes.append("Restored HyperMorphic zero-free parameter constraints after pruning")

            # 3. If agent has HyperMorphic free will, optimize its memory
            if hasattr(agent, 'free_will'):
                # Check for HyperMorphic version
                free_will_is_hypermorphic = hasattr(agent.free_will, 'hyper_math')

                if hasattr(agent.free_will, 'memory_set'):
                    old_size = len(agent.free_will.memory_set)

                    # Calculate target size with HyperMorphic proportions
                    target_size = int(self.hyper_math.mul(
                        self.hyper_math.zero_free(old_size),
                        self.hyper_math.zero_free(0.8)  # Target 80% of current size
                    ))

                    if target_size > 0:
                        # Use appropriate memory optimization method
                        if free_will_is_hypermorphic and hasattr(agent.free_will, '_optimize_memory_entropy'):
                            agent.free_will._optimize_memory_entropy()
                            new_size = len(agent.free_will.memory_set)
                            changes.append(f"Pruned memory using HyperMorphic entropy optimization: {old_size} → {new_size} items")
                        elif hasattr(agent.free_will, 'contract_memory'):
                            agent.free_will.contract_memory(target_size)
                            new_size = len(agent.free_will.memory_set)
                            changes.append(f"Pruned memory from {old_size} to {new_size} items")

            # 4. If agent has semantic memory, optimize it with HyperMorphic pruning
            if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'semantic_memory'):
                if agent.free_will.semantic_memory:
                    old_count = len(agent.free_will.semantic_memory)

                    # Identify low importance memories with HyperMorphic threshold
                    low_importance = []
                    threshold = self.hyper_math.zero_free(0.4)  # Below this importance, consider pruning

                    for url, data in agent.free_will.semantic_memory.items():
                        if isinstance(data, dict) and "importance" in data:
                            # Compare using HyperMorphic operations if possible
                            if free_will_is_hypermorphic:
                                if agent.free_will.hyper_math.sub(
                                    threshold,
                                    agent.free_will.hyper_math.zero_free(data["importance"])
                                ) > 0:
                                    low_importance.append(url)
                            else:
                                if data["importance"] < 0.4:  # Standard comparison for non-HyperMorphic
                                    low_importance.append(url)

                    # Calculate prune count with HyperMorphic proportions
                    prune_count = min(
                        len(low_importance),
                        int(self.hyper_math.mul(
                            self.hyper_math.zero_free(old_count),
                            self.hyper_math.zero_free(0.2)  # Prune up to 20%
                        ))
                    )

                    # Perform pruning with HyperMorphic optimization
                    pruned_count = 0
                    for url in low_importance[:prune_count]:
                        if url in agent.free_will.semantic_memory:
                            # Store temporal embedding if HyperMorphic system
                            if free_will_is_hypermorphic and hasattr(agent.free_will, 'capability_embeddings'):
                                # Extract semantic embedding before removal for potential future retrieval
                                memory_embedding = agent.free_will.semantic_memory[url].get("embedding", None)
                                if memory_embedding is not None:
                                    # Store in compressed form using HyperMorphic dimension reduction
                                    timestamp = datetime.now().isoformat()
                                    key = f"pruned_{url}_{timestamp}"
                                    agent.free_will.capability_embeddings[key] = memory_embedding

                            # Remove from semantic memory
                            del agent.free_will.semantic_memory[url]
                            pruned_count += 1

                    if pruned_count > 0:
                        new_count = len(agent.free_will.semantic_memory)
                        changes.append(f"Pruned semantic memory from {old_count} to {new_count} items using HyperMorphic importance thresholding")

                        # Apply memory consolidation if HyperMorphic system (create higher-level abstractions)
                        if free_will_is_hypermorphic and hasattr(agent.free_will, '_consolidate_semantic_memory'):
                            agent.free_will._consolidate_semantic_memory()
                            changes.append("Applied HyperMorphic semantic memory consolidation after pruning")

            # 5. Adjust any meta-learning parameters to optimize for smaller architecture
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'meta_learning'):
                meta_learning = agent.ai_manager.meta_learning

                # Adjust learning rate after pruning
                if hasattr(meta_learning, 'learning_rate_history') and meta_learning.learning_rate_history:
                    current_lr = meta_learning.learning_rate_history[-1]

                    # Increase learning rate slightly for pruned model with HyperMorphic scaling
                    new_lr = self.hyper_math.min(
                        self.hyper_math.zero_free(0.01),  # Cap at 0.01
                        self.hyper_math.mul(
                            self.hyper_math.zero_free(current_lr),
                            self.hyper_math.zero_free(1.2)  # 20% increase
                        )
                    )

                    # Apply to model if possible
                    if hasattr(agent.model, '_current_lr'):
                        agent.model._current_lr = new_lr
                        changes.append(f"Adjusted learning rate for pruned architecture: {current_lr:.6f} → {new_lr:.6f}")

                        # Update history
                        meta_learning.learning_rate_history.append(new_lr)

                        # Apply to optimizer if available
                        if hasattr(agent.model, 'optimizer'):
                            for param_group in agent.model.optimizer.param_groups:
                                param_group['lr'] = new_lr

            # 6. Update quantum coherence for pruned architecture
            if is_hypermorphic and hasattr(agent.model, 'quantum_coherence'):
                old_coherence = agent.model.quantum_coherence

                # Increase coherence for pruned architecture
                new_coherence = self.hyper_math.min(
                    self.hyper_math.zero_free(0.9),  # Cap at 0.9
                    self.hyper_math.mul(
                        self.hyper_math.zero_free(old_coherence),
                        self.hyper_math.zero_free(1.15)  # 15% increase
                    )
                )

                agent.model.quantum_coherence = new_coherence
                changes.append(f"Increased quantum coherence for pruned architecture: {old_coherence:.2f} → {new_coherence:.2f}")

            # Update strategy success rate with HyperMorphic operations
            self.strategies["pruning"]["success_rate"] = self.hyper_math.min(
                self.hyper_math.zero_free(0.95),  # Cap at 0.95
                self.hyper_math.mul(
                    self.strategies["pruning"]["success_rate"],
                    self.hyper_math.zero_free(1.1)  # 10% increase
                )
            )

            return True, "Successfully pruned system components using HyperMorphic dimensional preservation", changes

        except Exception as e:
            # Update strategy success rate (decrease on failure)
            self.strategies["pruning"]["success_rate"] = self.hyper_math.max(
                self.hyper_math.zero_free(0.2),
                self.hyper_math.mul(
                    self.strategies["pruning"]["success_rate"],
                    self.hyper_math.zero_free(0.9)  # 10% decrease
                )
            )

            return False, f"HyperMorphic pruning strategy failed: {str(e)}", changes

    def _apply_restructuring_strategy(self, agent, changes):
        """
        Apply restructuring strategy to reorganize components without changing capacity,
        using HyperMorphic transformations that preserve holomorphic structure.

        Parameters:
        - agent: Agent to evolve
        - changes: List to track changes

        Returns:
        - (success, message, changes) tuple
        """
        try:
            # Count successful restructuring attempts
            restructuring_count = 0

            # 1. Restructure consciousness parameters with HyperMorphic transformations
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'consciousness'):
                consciousness = agent.ai_manager.consciousness

                # Check for HyperMorphic consciousness
                is_hypermorphic = hasattr(consciousness, 'hyper_math')

                # Adjust awareness fluctuation rate with HyperMorphic operations
                if hasattr(consciousness, 'awareness_fluctuation_rate'):
                    old_rate = consciousness.awareness_fluctuation_rate

                    # Generate new rate with controlled phase shift
                    phase = random.uniform(0, 2 * math.pi)

                    if is_hypermorphic:
                        # HyperMorphic phase-based transformation
                        new_rate = consciousness.hyper_math.zero_free(
                            old_rate * math.cos(phase) + 0.05 * math.sin(phase)
                        )
                        new_rate = consciousness.hyper_math.max(
                            consciousness.hyper_math.zero_free(0.01),
                            consciousness.hyper_math.min(
                                consciousness.hyper_math.zero_free(0.1),
                                new_rate
                            )
                        )
                    else:
                        # Standard transformation for non-HyperMorphic
                        new_rate = max(0.01, min(0.1, old_rate * math.cos(phase) + 0.05 * math.sin(phase)))

                    consciousness.awareness_fluctuation_rate = new_rate
                    changes.append(f"Restructured consciousness fluctuation rate: {old_rate:.3f} → {new_rate:.3f}")
                    restructuring_count += 1

                # Reset state to "reflective" for restructuring process
                if hasattr(consciousness, 'current_state'):
                    old_state = consciousness.current_state

                    # For HyperMorphic consciousness, apply quantum state transition
                    if is_hypermorphic and hasattr(consciousness, 'quantum_state'):
                        # Increase reflective state amplitude
                        old_amplitude = consciousness.quantum_state.get("reflective", {}).get("amplitude", 0)
                        new_amplitude = consciousness.hyper_math.min(
                            consciousness.hyper_math.zero_free(0.9),
                            consciousness.hyper_math.mul(
                                consciousness.hyper_math.zero_free(old_amplitude),
                                consciousness.hyper_math.zero_free(1.5)
                            )
                        )
                        consciousness.quantum_state["reflective"]["amplitude"] = new_amplitude

                        # Adjust phase for more coherent reflective state
                        consciousness.quantum_state["reflective"]["phase"] = random.uniform(0, 2 * math.pi)

                    consciousness.current_state = "reflective"
                    changes.append(f"Reset consciousness state from '{old_state}' to 'reflective' with enhanced coherence")
                    restructuring_count += 1

                # Boost awareness temporarily for restructuring process
                if hasattr(consciousness, 'increase_awareness'):
                    # Use HyperMorphic operation if available
                    awareness_boost = self.hyper_math.zero_free(0.2)
                    consciousness.increase_awareness(awareness_boost)
                    changes.append("Temporarily boosted consciousness awareness for restructuring process")
                    restructuring_count += 1

            # 2. Restructure temporal planner with HyperMorphic goal reordering
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'temporal_planner'):
                planner = agent.ai_manager.temporal_planner

                # Rebalance goal priorities with HyperMorphic transformations
                if hasattr(planner, 'long_term_goals') and planner.long_term_goals:
                    # Store old priorities for logging
                    old_priorities = {}
                    for goal in planner.long_term_goals:
                        old_priorities[goal.get("id", "unknown")] = goal.get("priority", 0.5)

                    # Calculate total priority and average
                    total_priority = sum(goal.get("priority", 0.5) for goal in planner.long_term_goals)
                    avg_priority = total_priority / len(planner.long_term_goals)

                    # Apply HyperMorphic reflection around average using holomorphic transformation
                    for goal in planner.long_term_goals:
                        goal_id = goal.get("id", "unknown")
                        old_priority = goal.get("priority", 0.5)

                        # Calculate new priority as reflection around average with phase shift
                        phase = random.uniform(0, 2 * math.pi)
                        reflection_factor = 0.7 + 0.3 * math.cos(phase)

                        new_priority = avg_priority + reflection_factor * (avg_priority - old_priority)

                        # Ensure it's in valid range using HyperMorphic bounds
                        new_priority = self.hyper_math.max(
                            self.hyper_math.zero_free(0.1),
                            self.hyper_math.min(
                                self.hyper_math.zero_free(1.0),
                                self.hyper_math.zero_free(new_priority)
                            )
                        )

                        # Apply new priority
                        goal["priority"] = new_priority

                    changes.append("Restructured long-term goal priorities using holomorphic reflection")
                    restructuring_count += 1

                # Refresh short-term goals with HyperMorphic regeneration
                if hasattr(planner, 'refresh_short_term_goals'):
                    planner.refresh_short_term_goals()
                    changes.append("Regenerated short-term goals based on restructured priorities")
                    restructuring_count += 1

            # 3. Restructure imagination engine with HyperMorphic creativity modulation
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'imagination'):
                imagination = agent.ai_manager.imagination

                # Check for HyperMorphic imagination
                is_hypermorphic = hasattr(imagination, 'hyper_math')

                # Adjust creativity level using quantum interference pattern
                if hasattr(imagination, 'creativity_level'):
                    old_level = imagination.creativity_level

                    # For HyperMorphic imagination, use quantum inversion with phase preservation
                    if is_hypermorphic:
                        # Perform holomorphic phase-preserving inversion
                        new_level = imagination.hyper_math.sub(
                            imagination.hyper_math.zero_free(1.0),
                            imagination.hyper_math.zero_free(old_level)
                        )
                    else:
                        # Standard inversion for non-HyperMorphic
                        new_level = 1.0 - old_level

                    imagination.creativity_level = new_level
                    changes.append(f"Restructured imagination creativity using holomorphic inversion: {old_level:.2f} → {new_level:.2f}")
                    restructuring_count += 1

                # Change current mode with HyperMorphic state transition
                if hasattr(imagination, 'current_mode') and hasattr(imagination, 'cognitive_modes'):
                    old_mode = imagination.current_mode

                    # Select a different mode with HyperMorphic probability
                    available_modes = [m for m in imagination.cognitive_modes if m != old_mode]

                    if available_modes:
                        # Apply quantum selection for new cognitive mode
                        mode_amplitudes = {}
                        for mode in available_modes:
                            # Generate quantum amplitude with phase
                            phase = random.uniform(0, 2 * math.pi)
                            amplitude = 0.5 + 0.5 * math.cos(phase)
                            mode_amplitudes[mode] = self.hyper_math.zero_free(amplitude)

                        # Select mode with highest amplitude
                        new_mode = max(mode_amplitudes.items(), key=lambda x: x[1])[0]

                        imagination.current_mode = new_mode
                        changes.append(f"Restructured imagination cognitive mode: '{old_mode}' → '{new_mode}' using quantum amplitude selection")
                        restructuring_count += 1

            # 4. Restructure neural architecture with HyperMorphic holomorphic transformations
            if hasattr(agent, 'model') and hasattr(agent.model, 'neocortex'):
                # Check for HyperMorphic model
                is_hypermorphic = hasattr(agent.model, 'hyper_math')

                # Perform HyperMorphic weight restructuring
                if is_hypermorphic and hasattr(agent.model, '_apply_holomorphic_transform'):
                    # Apply holomorphic transformation that preserves structure while changing weights
                    agent.model._apply_holomorphic_transform()
                    changes.append("Applied holomorphic weight transformation to neural architecture")
                    restructuring_count += 1
                elif hasattr(agent.model, 'state_dict') and hasattr(agent.model, 'load_state_dict'):
                    # For non-HyperMorphic, apply simpler weight perturbation
                    with torch.no_grad():
                        state_dict = agent.model.state_dict()

                        # Apply phase-based perturbation to weights
                        for name, param in state_dict.items():
                            if 'weight' in name:
                                # Small random perturbation
                                phase = random.uniform(0, 2 * math.pi)
                                perturbation = 0.05 * torch.randn_like(param) * math.cos(phase)
                                param.add_(perturbation)

                        # Load modified state dict
                        agent.model.load_state_dict(state_dict)

                    changes.append("Applied weight perturbation to neural architecture")
                    restructuring_count += 1

            # 5. Restructure domain intelligence with HyperMorphic recategorization
            if hasattr(agent, 'free_will') and hasattr(agent.free_will, 'domain_intelligence'):
                intelligence = agent.free_will.domain_intelligence

                # Check for HyperMorphic domain intelligence
                is_hypermorphic = hasattr(intelligence, 'hyper_math')

                # Restructure domain categories with HyperMorphic operations
                if hasattr(intelligence, 'domain_categories') and intelligence.domain_categories:
                    # Count domains before restructuring
                    domains_before = sum(len(domains) for domains in intelligence.domain_categories.values())

                    # Calculate new category thresholds with phase modulation
                    category_thresholds = {}
                    for category in intelligence.domain_categories:
                        phase = random.uniform(0, 2 * math.pi)
                        threshold = 0.5 + 0.3 * math.cos(phase)

                        if is_hypermorphic:
                            category_thresholds[category] = intelligence.hyper_math.zero_free(threshold)
                        else:
                            category_thresholds[category] = threshold

                    # Restructure domain categorization
                    if hasattr(intelligence, 'domain_knowledge'):
                        for domain, data in intelligence.domain_knowledge.items():
                            if isinstance(data, dict) and "category_scores" in data:
                                scores = data["category_scores"]

                                # Find best category based on new thresholds
                                best_category = None
                                best_score = 0

                                for category, score in scores.items():
                                    if category in category_thresholds:
                                        threshold = category_thresholds[category]

                                        if is_hypermorphic:
                                            adjusted_score = intelligence.hyper_math.mul(
                                                intelligence.hyper_math.zero_free(score),
                                                threshold
                                            )
                                        else:
                                            adjusted_score = score * threshold

                                        if best_category is None or adjusted_score > best_score:
                                            best_category = category
                                            best_score = adjusted_score

                                if best_category and best_category != data.get("primary_category"):
                                    # Update domain category
                                    old_category = data.get("primary_category")
                                    data["primary_category"] = best_category

                                    # Update category indices
                                    if old_category and old_category in intelligence.domain_categories:
                                        intelligence.domain_categories[old_category].discard(domain)

                                    if best_category not in intelligence.domain_categories:
                                        intelligence.domain_categories[best_category] = set()

                                    intelligence.domain_categories[best_category].add(domain)

                    # Count domains after restructuring
                    domains_after = sum(len(domains) for domains in intelligence.domain_categories.values())

                    if domains_before != domains_after:
                        changes.append(f"Restructured domain intelligence categories: {domains_before} → {domains_after} domain categorizations")
                        restructuring_count += 1

            # Update strategy success rate based on restructuring count
            self.strategies["restructuring"]["success_rate"] = self.hyper_math.min(
                self.hyper_math.zero_free(0.95),  # Cap at 0.95
                self.hyper_math.mul(
                    self.strategies["restructuring"]["success_rate"],
                    self.hyper_math.zero_free(1.1)  # 10% increase
                )
            )

            success = restructuring_count > 0

            if success:
                return True, f"Successfully restructured {restructuring_count} system components using HyperMorphic holomorphic transformations", changes
            else:
                return False, "No components were suitable for HyperMorphic restructuring", changes

        except Exception as e:
            # Update strategy success rate (decrease on failure)
            self.strategies["restructuring"]["success_rate"] = self.hyper_math.max(
                self.hyper_math.zero_free(0.2),
                self.hyper_math.mul(
                    self.strategies["restructuring"]["success_rate"],
                    self.hyper_math.zero_free(0.9)  # 10% decrease
                )
            )

            return False, f"HyperMorphic restructuring failed: {str(e)}", changes

    def _apply_specialization_strategy(self, agent, changes):
        """
        Apply specialization strategy to refine specific components for focused tasks
        using HyperMorphic zero-free transformations.

        Parameters:
        - agent: Agent to evolve
        - changes: List to track changes

        Returns:
        - (success, message, changes) tuple
        """
        try:
            # Count successful specialization attempts
            specialization_count = 0

            # 1. Specialize neural architecture for focused tasks
            if hasattr(agent, 'model') and hasattr(agent.model, 'neocortex'):
                # Check for HyperMorphic model version
                is_hypermorphic = hasattr(agent.model, 'hyper_math')

                # Identify most important components to specialize
                # For simplicity, we'll specialize deeper layers more than shallow ones
                if len(agent.model.neocortex) > 3:
                    # Calculate specialization weights with quantum phase pattern
                    specialization_weights = []
                    for i in range(len(agent.model.neocortex)):
                        # Deeper layers get higher weights with quantum phase pattern
                        phase = random.uniform(0, 2 * math.pi)
                        depth_factor = i / (len(agent.model.neocortex) - 1)  # 0 to 1
                        weight = 0.3 + 0.7 * depth_factor * (0.5 + 0.5 * math.cos(phase))
                        
                        if is_hypermorphic:
                            specialization_weights.append(agent.model.hyper_math.zero_free(weight))
                        else:
                            specialization_weights.append(weight)

                    # Apply specialized transformations to neural architecture
                    with torch.no_grad():
                        state_dict = agent.model.state_dict()
                        layers_specialized = 0

                        # Get list of weight parameters
                        weight_params = [name for name in state_dict.keys() if 'weight' in name]
                        
                        # Group weights by layer
                        layer_weights = {}
                        for i, layer in enumerate(agent.model.neocortex):
                            layer_weights[i] = [name for name in weight_params 
                                                if f"neocortex.{i}." in name]

                        # Apply specialization transformations to weights
                        for layer_idx, layer_weight_names in layer_weights.items():
                            if layer_idx < len(specialization_weights):
                                specialization_factor = specialization_weights[layer_idx]
                                
                                for weight_name in layer_weight_names:
                                    param = state_dict[weight_name]
                                    
                                    # Calculate weight statistics
                                    weight_mean = torch.mean(param)
                                    weight_std = torch.std(param)
                                    
                                    # Create specialization mask (amplify important weights)
                                    # For specialization, we emphasize larger magnitude weights
                                    importance_threshold = weight_mean + 0.5 * weight_std
                                    important_weights = torch.abs(param) > importance_threshold
                                    
                                    # Apply specialization: amplify important weights, reduce others
                                    if is_hypermorphic and hasattr(agent.model, '_ensure_zero_free'):
                                        # HyperMorphic specialization with phase coherence
                                        phase = random.uniform(0, 2 * math.pi)
                                        
                                        # Amplify important weights
                                        param[important_weights] = param[important_weights] * (1.0 + 0.2 * specialization_factor * math.cos(phase))
                                        
                                        # Reduce less important weights
                                        param[~important_weights] = param[~important_weights] * (1.0 - 0.1 * specialization_factor * math.cos(phase))
                                        
                                        # Ensure zero-free
                                        param.data = agent.model._ensure_zero_free(param.data)
                                    else:
                                        # Standard specialization
                                        param[important_weights] = param[important_weights] * (1.0 + 0.2 * specialization_factor)
                                        param[~important_weights] = param[~important_weights] * (1.0 - 0.1 * specialization_factor)
                                
                                layers_specialized += 1

                        if layers_specialized > 0:
                            # Apply specialized weights
                            agent.model.load_state_dict(state_dict)
                            changes.append(f"Specialized {layers_specialized} neural layers with depth-based importance weighting")
                            specialization_count += 1

            # 2. Specialize capability embeddings for focused tasks
            if hasattr(agent, 'capability_embeddings') or (hasattr(agent, 'free_will') and hasattr(agent.free_will, 'capability_embeddings')):
                # Determine embeddings source
                embeddings = None
                if hasattr(agent, 'capability_embeddings'):
                    embeddings = agent.capability_embeddings
                elif hasattr(agent, 'free_will') and hasattr(agent.free_will, 'capability_embeddings'):
                    embeddings = agent.free_will.capability_embeddings
                
                if embeddings and isinstance(embeddings, dict) and len(embeddings) > 0:
                    # Identify capabilities to specialize based on gaps
                    capability_gaps = {}
                    for capability, target in self.capability_targets.items():
                        current = self.capability_scores.get(capability, self.hyper_math.zero_free(0.3))
                        gap = self.hyper_math.sub(target, current)
                        
                        # Only consider significant gaps
                        if gap > self.hyper_math.zero_free(0.2):
                            capability_gaps[capability] = gap
                    
                    # Select top capability to specialize
                    if capability_gaps:
                        top_capability = max(capability_gaps.items(), key=lambda x: x[1])[0]
                        
                        # Find embedding for the capability
                        if top_capability in embeddings:
                            # Apply specialization with quantum amplification
                            embedding = embeddings[top_capability]
                            
                            if isinstance(embedding, list):
                                # Vector embedding specialization
                                for i in range(len(embedding)):
                                    if i < len(embedding):
                                        # Apply quantum phase amplification
                                        phase = random.uniform(0, 2 * math.pi)
                                        amplification = 1.0 + 0.3 * math.cos(phase)
                                        
                                        # Amplify with zero-free guarantee
                                        embedding[i] = self.hyper_math.mul(
                                            embedding[i],
                                            self.hyper_math.zero_free(amplification)
                                        )
                                
                                # Normalize embedding after specialization
                                norm = math.sqrt(sum(self.hyper_math.mul(x, x) for x in embedding))
                                if norm > self.hyper_math.epsilon:
                                    embedding = [self.hyper_math.div(x, self.hyper_math.zero_free(norm)) for x in embedding]
                                
                                # Apply specialized embedding
                                embeddings[top_capability] = embedding
                                changes.append(f"Specialized capability embedding for '{top_capability}' using quantum phase amplification")
                                specialization_count += 1

            # 3. Specialize agent consciousness state biases
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'consciousness'):
                consciousness = agent.ai_manager.consciousness
                
                # Check for HyperMorphic consciousness
                is_hypermorphic = hasattr(consciousness, 'hyper_math')
                
                # Specialize state transition biases
                if hasattr(consciousness, 'state_transitions') and isinstance(consciousness.state_transitions, dict):
                    # Identify most useful state based on current capabilities
                    useful_states = ["focused", "reflective", "creative", "analytical"]
                    best_state = None
                    
                    # Select best state based on capability gaps
                    capability_state_map = {
                        "knowledge_representation": "analytical",
                        "planning": "focused",
                        "learning": "reflective",
                        "creative_synthesis": "creative"
                    }
                    
                    # Find biggest capability gap
                    max_gap = self.hyper_math.zero_free(0.0)
                    for capability, target in self.capability_targets.items():
                        if capability in capability_state_map:
                            current = self.capability_scores.get(capability, self.hyper_math.zero_free(0.3))
                            gap = self.hyper_math.sub(target, current)
                            
                            if gap > max_gap:
                                max_gap = gap
                                best_state = capability_state_map[capability]
                    
                    if best_state in useful_states:
                        # Apply specialization to state transitions
                        for from_state, transitions in consciousness.state_transitions.items():
                            if isinstance(transitions, dict):
                                # Get current transition probabilities
                                total_prob = sum(transitions.values())
                                if total_prob > 0:
                                    # Calculate specialization factor with quantum coherence
                                    phase = random.uniform(0, 2 * math.pi)
                                    specialization_factor = 0.4 + 0.3 * math.cos(phase)
                                    
                                    # Enhance transition to best state
                                    if best_state in transitions:
                                        old_prob = transitions[best_state]
                                        
                                        # Increase probability with zero-free guarantee
                                        if is_hypermorphic:
                                            new_prob = consciousness.hyper_math.min(
                                                consciousness.hyper_math.zero_free(0.8),  # Cap at 0.8
                                                consciousness.hyper_math.mul(
                                                    old_prob,
                                                    consciousness.hyper_math.add(
                                                        consciousness.hyper_math.zero_free(1.0),
                                                        consciousness.hyper_math.zero_free(specialization_factor)
                                                    )
                                                )
                                            )
                                            transitions[best_state] = new_prob
                                        else:
                                            # Standard specialization
                                            new_prob = min(0.8, old_prob * (1.0 + specialization_factor))
                                            transitions[best_state] = new_prob
                                        
                                        # Re-normalize transitions
                                        if is_hypermorphic:
                                            new_total = sum(consciousness.hyper_math.zero_free(p) for p in transitions.values())
                                            for state, prob in transitions.items():
                                                if state != best_state:
                                                    transitions[state] = consciousness.hyper_math.div(
                                                        consciousness.hyper_math.mul(
                                                            prob,
                                                            consciousness.hyper_math.sub(
                                                                consciousness.hyper_math.zero_free(1.0),
                                                                consciousness.hyper_math.div(
                                                                    new_prob,
                                                                    new_total
                                                                )
                                                            )
                                                        ),
                                                        consciousness.hyper_math.sub(
                                                            consciousness.hyper_math.zero_free(1.0),
                                                            consciousness.hyper_math.div(
                                                                old_prob,
                                                                consciousness.hyper_math.zero_free(total_prob)
                                                            )
                                                        )
                                                    )
                                        else:
                                            new_total = sum(transitions.values())
                                            for state, prob in transitions.items():
                                                if state != best_state:
                                                    transitions[state] = prob * (1.0 - new_prob/new_total) / (1.0 - old_prob/total_prob)
                        
                        changes.append(f"Specialized consciousness state transitions to favor '{best_state}' state")
                        specialization_count += 1

            # 4. Specialize meta-learning parameters
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'meta_learning'):
                meta_learning = agent.ai_manager.meta_learning
                
                if hasattr(meta_learning, 'learning_domains') and isinstance(meta_learning.learning_domains, dict):
                    # Identify underperforming domains to specialize
                    if hasattr(agent, 'stats') and 'domain_stats' in agent.stats:
                        domain_stats = agent.stats['domain_stats']
                        
                        if domain_stats and isinstance(domain_stats, dict) and len(domain_stats) > 0:
                            # Calculate domain performance metrics
                            domain_performance = {}
                            for domain, stats in domain_stats.items():
                                if isinstance(stats, dict):
                                    # Calculate performance score
                                    success_rate = 0.5  # Default
                                    if 'success_count' in stats and 'visits' in stats and stats['visits'] > 0:
                                        success_rate = stats['success_count'] / stats['visits']
                                    
                                    error_rate = 0.0  # Default
                                    if 'error_count' in stats and 'visits' in stats and stats['visits'] > 0:
                                        error_rate = stats['error_count'] / stats['visits']
                                    
                                    # Combined performance score (higher is better)
                                    performance = success_rate * (1.0 - error_rate)
                                    domain_performance[domain] = self.hyper_math.zero_free(performance)
                            
                            if domain_performance:
                                # Identify worst performing domains
                                sorted_domains = sorted(domain_performance.items(), key=lambda x: x[1])
                                worst_domains = sorted_domains[:min(2, len(sorted_domains))]
                                
                                domains_specialized = 0
                                for domain, performance in worst_domains:
                                    if domain in meta_learning.learning_domains:
                                        # Apply specialization to learning parameters
                                        domain_params = meta_learning.learning_domains[domain]
                                        
                                        if isinstance(domain_params, dict):
                                            # Adjust learning rate for this domain
                                            if 'learning_rate' in domain_params:
                                                old_lr = domain_params['learning_rate']
                                                
                                                # Increase learning rate with quantum phase pattern
                                                phase = random.uniform(0, 2 * math.pi)
                                                increment = 0.2 + 0.1 * math.cos(phase)
                                                
                                                # Calculate new learning rate with zero-free guarantee
                                                new_lr = self.hyper_math.min(
                                                    self.hyper_math.zero_free(0.01),  # Cap at 0.01
                                                    self.hyper_math.mul(
                                                        self.hyper_math.zero_free(old_lr),
                                                        self.hyper_math.add(
                                                            self.hyper_math.zero_free(1.0),
                                                            self.hyper_math.zero_free(increment)
                                                        )
                                                    )
                                                )
                                                
                                                domain_params['learning_rate'] = new_lr
                                            
                                            # Boost exploration for this domain
                                            if 'exploration_rate' in domain_params:
                                                old_expl = domain_params['exploration_rate']
                                                
                                                # Increase exploration with quantum coherence
                                                phase = random.uniform(0, 2 * math.pi)
                                                increment = 0.15 + 0.1 * math.cos(phase)
                                                
                                                # Calculate new exploration rate with zero-free guarantee
                                                new_expl = self.hyper_math.min(
                                                    self.hyper_math.zero_free(0.7),  # Cap at 0.7
                                                    self.hyper_math.add(
                                                        self.hyper_math.zero_free(old_expl),
                                                        self.hyper_math.mul(
                                                            self.hyper_math.sub(
                                                                self.hyper_math.zero_free(0.7),
                                                                self.hyper_math.zero_free(old_expl)
                                                            ),
                                                            self.hyper_math.zero_free(increment)
                                                        )
                                                    )
                                                )
                                                
                                                domain_params['exploration_rate'] = new_expl
                                            
                                            domains_specialized += 1
                                
                                if domains_specialized > 0:
                                    changes.append(f"Specialized learning parameters for {domains_specialized} underperforming domains")
                                    specialization_count += 1

            # Update strategy success rate based on specialization count
            self.strategies["specialization"]["success_rate"] = self.hyper_math.min(
                self.hyper_math.zero_free(0.95),  # Cap at 0.95
                self.hyper_math.mul(
                    self.strategies["specialization"]["success_rate"],
                    self.hyper_math.zero_free(1.1)  # 10% increase
                )
            )

            success = specialization_count > 0

            if success:
                return True, f"Successfully specialized {specialization_count} system components for focused tasks", changes
            else:
                return False, "No components were suitable for specialization", changes

        except Exception as e:
            # Update strategy success rate (decrease on failure)
            self.strategies["specialization"]["success_rate"] = self.hyper_math.max(
                self.hyper_math.zero_free(0.2),
                self.hyper_math.mul(
                    self.strategies["specialization"]["success_rate"],
                    self.hyper_math.zero_free(0.9)  # 10% decrease
                )
            )

            return False, f"HyperMorphic specialization failed: {str(e)}", changes

    def _apply_integration_strategy(self, agent, changes):
        """
        Apply integration strategy to combine and harmonize synergistic modules
        using HyperMorphic holomorphic transformations.

        Parameters:
        - agent: Agent to evolve
        - changes: List to track changes

        Returns:
        - (success, message, changes) tuple
        """
        try:
            # Count successful integration attempts
            integration_count = 0

            # 1. Integrate consciousness and planning modules for improved coherence
            if (hasattr(agent, 'ai_manager') and 
                hasattr(agent.ai_manager, 'consciousness') and
                hasattr(agent.ai_manager, 'temporal_planner')):
                
                consciousness = agent.ai_manager.consciousness
                planner = agent.ai_manager.temporal_planner
                
                # Check for HyperMorphic versions
                is_consciousness_hypermorphic = hasattr(consciousness, 'hyper_math')
                is_planner_hypermorphic = hasattr(planner, 'hyper_math')
                
                # Integrate consciousness states with planning priorities
                if (hasattr(consciousness, 'states') and 
                    hasattr(planner, 'long_term_goals')):
                    
                    # Map consciousness states to planning priorities
                    if hasattr(consciousness, 'current_state'):
                        current_state = consciousness.current_state
                        
                        # Use current state to influence goal priorities
                        state_priority_map = {
                            "focused": "efficiency",
                            "reflective": "learning",
                            "creative": "innovation",
                            "analytical": "accuracy",
                            "relaxed": "stability"
                        }
                        
                        # Check if current state has a priority mapping
                        if current_state in state_priority_map and planner.long_term_goals:
                            priority_boost = state_priority_map[current_state]
                            
                            # Boost goals with matching priority
                            goals_updated = 0
                            for goal in planner.long_term_goals:
                                if isinstance(goal, dict) and "priority" in goal and "priority_type" in goal:
                                    if goal["priority_type"] == priority_boost:
                                        old_priority = goal["priority"]
                                        
                                        # Calculate boost with quantum coherence
                                        phase = random.uniform(0, 2 * math.pi)
                                        boost = 0.2 + 0.1 * math.cos(phase)
                                        
                                        # Apply boost with zero-free guarantee
                                        if is_planner_hypermorphic:
                                            new_priority = planner.hyper_math.min(
                                                planner.hyper_math.zero_free(0.9),  # Cap at 0.9
                                                planner.hyper_math.add(
                                                    planner.hyper_math.zero_free(old_priority),
                                                    planner.hyper_math.mul(
                                                        planner.hyper_math.sub(
                                                            planner.hyper_math.zero_free(0.9),
                                                            planner.hyper_math.zero_free(old_priority)
                                                        ),
                                                        planner.hyper_math.zero_free(boost)
                                                    )
                                                )
                                            )
                                        else:
                                            new_priority = min(0.9, old_priority + (0.9 - old_priority) * boost)
                                        
                                        goal["priority"] = new_priority
                                        goals_updated += 1
                            
                            if goals_updated > 0:
                                changes.append(f"Integrated consciousness state '{current_state}' with {goals_updated} matching priority goals")
                                integration_count += 1
                    
                    # Create bi-directional influence between planning and consciousness
                    if (hasattr(planner, 'active_goals') and 
                        hasattr(consciousness, 'state_transitions')):
                        
                        # Set up bi-directional influence
                        if planner.active_goals and consciousness.state_transitions:
                            # Map goal types to preferred consciousness states
                            goal_state_map = {
                                "efficiency": "focused",
                                "learning": "reflective",
                                "innovation": "creative",
                                "accuracy": "analytical",
                                "stability": "relaxed"
                            }
                            
                            # Find dominant active goal type
                            goal_type_counts = {}
                            for goal in planner.active_goals:
                                if isinstance(goal, dict) and "priority_type" in goal:
                                    goal_type = goal["priority_type"]
                                    goal_type_counts[goal_type] = goal_type_counts.get(goal_type, 0) + 1
                            
                            if goal_type_counts:
                                dominant_goal_type = max(goal_type_counts.items(), key=lambda x: x[1])[0]
                                
                                # Check if dominant goal has a state mapping
                                if dominant_goal_type in goal_state_map:
                                    target_state = goal_state_map[dominant_goal_type]
                                    
                                    # Enhance transition probabilities to target state
                                    transitions_updated = 0
                                    for from_state, transitions in consciousness.state_transitions.items():
                                        if isinstance(transitions, dict) and target_state in transitions:
                                            old_prob = transitions[target_state]
                                            
                                            # Calculate integration factor with quantum phase
                                            phase = random.uniform(0, 2 * math.pi)
                                            boost = 0.25 + 0.15 * math.cos(phase)
                                            
                                            # Apply boost with zero-free guarantee
                                            if is_consciousness_hypermorphic:
                                                new_prob = consciousness.hyper_math.min(
                                                    consciousness.hyper_math.zero_free(0.8),  # Cap at 0.8
                                                    consciousness.hyper_math.add(
                                                        consciousness.hyper_math.zero_free(old_prob),
                                                        consciousness.hyper_math.mul(
                                                            consciousness.hyper_math.sub(
                                                                consciousness.hyper_math.zero_free(0.8),
                                                                consciousness.hyper_math.zero_free(old_prob)
                                                            ),
                                                            consciousness.hyper_math.zero_free(boost)
                                                        )
                                                    )
                                                )
                                            else:
                                                new_prob = min(0.8, old_prob + (0.8 - old_prob) * boost)
                                            
                                            transitions[target_state] = new_prob
                                            transitions_updated += 1
                                    
                                    if transitions_updated > 0:
                                        changes.append(f"Integrated planning goals of type '{dominant_goal_type}' with consciousness transitions to '{target_state}'")
                                        integration_count += 1

            # 2. Integrate model architecture with capabilities
            if hasattr(agent, 'model') and hasattr(agent.model, 'neocortex'):
                model = agent.model
                
                # Check for HyperMorphic model
                is_model_hypermorphic = hasattr(model, 'hyper_math')
                
                # Find capability gaps to guide integration
                capability_gaps = {}
                for capability, target in self.capability_targets.items():
                    current = self.capability_scores.get(capability, self.hyper_math.zero_free(0.3))
                    gap = self.hyper_math.sub(target, current)
                    
                    # Only consider significant gaps
                    if gap > self.hyper_math.zero_free(0.2):
                        capability_gaps[capability] = gap
                
                # Select top capabilities to integrate
                if capability_gaps and len(capability_gaps) >= 2:
                    # Get top 2 capabilities to integrate
                    sorted_gaps = sorted(capability_gaps.items(), key=lambda x: x[1], reverse=True)
                    top_capabilities = sorted_gaps[:2]
                    
                    # Get capability embeddings if available
                    capability_embeddings = getattr(agent, 'capability_embeddings', 
                                                   getattr(agent.free_will, 'capability_embeddings', None)
                                                   if hasattr(agent, 'free_will') else None)
                    
                    if capability_embeddings and isinstance(capability_embeddings, dict):
                        # Extract embeddings for top capabilities
                        capability_vectors = []
                        for capability, _ in top_capabilities:
                            if capability in capability_embeddings:
                                capability_vectors.append((capability, capability_embeddings[capability]))
                        
                        # If we have at least 2 capability vectors, integrate them
                        if len(capability_vectors) >= 2:
                            capability1, vector1 = capability_vectors[0]
                            capability2, vector2 = capability_vectors[1]
                            
                            # Only proceed if vectors have same dimensionality
                            if isinstance(vector1, list) and isinstance(vector2, list) and len(vector1) == len(vector2):
                                # Create integrated vector with quantum blending
                                integrated_vector = []
                                for i in range(len(vector1)):
                                    # Apply quantum phase blending
                                    phase = random.uniform(0, 2 * math.pi)
                                    blend = 0.5 + 0.3 * math.cos(phase)
                                    
                                    # Calculate integrated value with zero-free guarantee
                                    integrated_value = self.hyper_math.add(
                                        self.hyper_math.mul(
                                            self.hyper_math.zero_free(vector1[i]),
                                            self.hyper_math.zero_free(blend)
                                        ),
                                        self.hyper_math.mul(
                                            self.hyper_math.zero_free(vector2[i]),
                                            self.hyper_math.sub(
                                                self.hyper_math.zero_free(1.0),
                                                self.hyper_math.zero_free(blend)
                                            )
                                        )
                                    )
                                    
                                    integrated_vector.append(integrated_value)
                                
                                # Create integrated capability name
                                integrated_name = f"{capability1}_{capability2}_integrated"
                                
                                # Store integrated capability
                                capability_embeddings[integrated_name] = integrated_vector
                                
                                # Apply integrated capability to neural architecture
                                if hasattr(model, 'state_dict') and hasattr(model, 'load_state_dict'):
                                    with torch.no_grad():
                                        state_dict = model.state_dict()
                                        
                                        # Apply integration to selected layers
                                        # We'll focus on middle layers for integration
                                        if len(model.neocortex) >= 3:
                                            middle_idx = len(model.neocortex) // 2
                                            
                                            # Get relevant weight parameters
                                            weight_params = [name for name in state_dict.keys() 
                                                           if f"neocortex.{middle_idx}." in name and 'weight' in name]
                                            
                                            if weight_params:
                                                # Normalize integrated vector
                                                norm = math.sqrt(sum(self.hyper_math.mul(x, x) for x in integrated_vector))
                                                if norm > self.hyper_math.epsilon:
                                                    normalized_vector = [self.hyper_math.div(x, self.hyper_math.zero_free(norm)) 
                                                                       for x in integrated_vector]
                                                else:
                                                    normalized_vector = integrated_vector
                                                
                                                # Apply integration to weights
                                                for weight_name in weight_params:
                                                    param = state_dict[weight_name]
                                                    
                                                    # Apply integration pattern based on capabilities
                                                    # We'll use the integrated vector to guide weight adjustments
                                                    if len(normalized_vector) > 0:
                                                        # Map vector dimensions to weight dimensions
                                                        dim_mapping = min(len(normalized_vector), param.dim())
                                                        
                                                        # Apply integration pattern
                                                        for d in range(dim_mapping):
                                                            if d < param.dim():
                                                                # Extract integration factor
                                                                factor = normalized_vector[d % len(normalized_vector)]
                                                                
                                                                # Apply phase-based integration
                                                                phase = random.uniform(0, 2 * math.pi)
                                                                integration_strength = 0.05 + 0.03 * math.cos(phase)
                                                                
                                                                # Calculate pattern
                                                                pattern = factor * integration_strength
                                                                
                                                                # Apply pattern with zero-free guarantee
                                                                if is_model_hypermorphic and hasattr(model, '_ensure_zero_free'):
                                                                    # Create integration mask
                                                                    if d == 0 and param.dim() >= 2:
                                                                        # Apply to output dimension
                                                                        for i in range(param.size(0)):
                                                                            scale = 1.0 + pattern * math.cos(i * 0.1)
                                                                            param[i] = param[i] * scale
                                                                    elif d == 1 and param.dim() >= 2:
                                                                        # Apply to input dimension
                                                                        for i in range(param.size(1)):
                                                                            scale = 1.0 + pattern * math.sin(i * 0.1)
                                                                            param[:, i] = param[:, i] * scale
                                                                    
                                                                    # Ensure zero-free
                                                                    param.data = model._ensure_zero_free(param.data)
                                                                else:
                                                                    # Standard integration
                                                                    if d == 0 and param.dim() >= 2:
                                                                        for i in range(param.size(0)):
                                                                            scale = 1.0 + pattern * math.cos(i * 0.1)
                                                                            param[i] = param[i] * scale
                                                                    elif d == 1 and param.dim() >= 2:
                                                                        for i in range(param.size(1)):
                                                                            scale = 1.0 + pattern * math.sin(i * 0.1)
                                                                            param[:, i] = param[:, i] * scale
                                                
                                                # Apply integrated weights
                                                model.load_state_dict(state_dict)
                                                
                                                changes.append(f"Integrated capabilities '{capability1}' and '{capability2}' into neural architecture")
                                                integration_count += 1

            # 3. Integrate free will decision mechanisms
            if hasattr(agent, 'free_will'):
                free_will = agent.free_will
                
                # Check for HyperMorphic free will
                is_free_will_hypermorphic = hasattr(free_will, 'hyper_math')
                
                # Integrate exploration and exploitation mechanisms
                if (hasattr(free_will, 'exploration_weight') and
                    hasattr(free_will, 'exploitation_weight')):
                    
                    old_exploration = free_will.exploration_weight
                    old_exploitation = free_will.exploitation_weight
                    
                    # Apply dynamic balance based on convergence score
                    convergence = self.convergence_score
                    
                    # Calculate integration factors with quantum coherence
                    phase = random.uniform(0, 2 * math.pi)
                    adjustment = 0.1 + 0.05 * math.cos(phase)
                    
                    # Calculate integrated weights with zero-free guarantees
                    if is_free_will_hypermorphic:
                        # HyperMorphic integration with phase coherence
                        if convergence < free_will.hyper_math.zero_free(0.7):
                            # Not converged yet - increase exploration
                            new_exploration = free_will.hyper_math.min(
                                free_will.hyper_math.zero_free(0.7),  # Cap at 0.7
                                free_will.hyper_math.add(
                                    free_will.hyper_math.zero_free(old_exploration),
                                    free_will.hyper_math.mul(
                                        free_will.hyper_math.sub(
                                            free_will.hyper_math.zero_free(0.7),
                                            free_will.hyper_math.zero_free(old_exploration)
                                        ),
                                        free_will.hyper_math.zero_free(adjustment)
                                    )
                                )
                            )
                            new_exploitation = free_will.hyper_math.sub(
                                free_will.hyper_math.zero_free(1.0),
                                new_exploration
                            )
                        else:
                            # Converging well - increase exploitation
                            new_exploitation = free_will.hyper_math.min(
                                free_will.hyper_math.zero_free(0.7),  # Cap at 0.7
                                free_will.hyper_math.add(
                                    free_will.hyper_math.zero_free(old_exploitation),
                                    free_will.hyper_math.mul(
                                        free_will.hyper_math.sub(
                                            free_will.hyper_math.zero_free(0.7),
                                            free_will.hyper_math.zero_free(old_exploitation)
                                        ),
                                        free_will.hyper_math.zero_free(adjustment)
                                    )
                                )
                            )
                            new_exploration = free_will.hyper_math.sub(
                                free_will.hyper_math.zero_free(1.0),
                                new_exploitation
                            )
                    else:
                        # Standard integration
                        if convergence < 0.7:
                            # Not converged yet - increase exploration
                            new_exploration = min(0.7, old_exploration + (0.7 - old_exploration) * adjustment)
                            new_exploitation = 1.0 - new_exploration
                        else:
                            # Converging well - increase exploitation
                            new_exploitation = min(0.7, old_exploitation + (0.7 - old_exploitation) * adjustment)
                            new_exploration = 1.0 - new_exploitation
                    
                    # Apply integrated weights
                    free_will.exploration_weight = new_exploration
                    free_will.exploitation_weight = new_exploitation
                    
                    changes.append(f"Integrated exploration ({old_exploration:.2f} → {new_exploration:.2f}) and exploitation ({old_exploitation:.2f} → {new_exploitation:.2f}) based on convergence")
                    integration_count += 1

            # Update strategy success rate based on integration count
            self.strategies["integration"]["success_rate"] = self.hyper_math.min(
                self.hyper_math.zero_free(0.95),  # Cap at 0.95
                self.hyper_math.mul(
                    self.strategies["integration"]["success_rate"],
                    self.hyper_math.zero_free(1.1)  # 10% increase
                )
            )

            success = integration_count > 0

            if success:
                return True, f"Successfully integrated {integration_count} synergistic system components", changes
            else:
                return False, "No components were suitable for integration", changes

        except Exception as e:
            # Update strategy success rate (decrease on failure)
            self.strategies["integration"]["success_rate"] = self.hyper_math.max(
                self.hyper_math.zero_free(0.2),
                self.hyper_math.mul(
                    self.strategies["integration"]["success_rate"],
                    self.hyper_math.zero_free(0.9)  # 10% decrease
                )
            )

            return False, f"HyperMorphic integration failed: {str(e)}", changes

    def _apply_quantum_variation_strategy(self, agent, changes):
        """
        Apply quantum variation strategy to create non-deterministic variations
        in the system architecture while preserving holomorphic structure.

        Parameters:
        - agent: Agent to evolve
        - changes: List to track changes

        Returns:
        - (success, message, changes) tuple
        """
        try:
            # Count successful quantum variations
            variation_count = 0

            # 1. Apply quantum variations to consciousness module
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'consciousness'):
                consciousness = agent.ai_manager.consciousness
                # Check for HyperMorphic consciousness
                is_hypermorphic = hasattr(consciousness, 'hyper_math')
                # Apply quantum fluctuations to awareness level
                if hasattr(consciousness, 'awareness_level'):
                    old_awareness = consciousness.awareness_level
                    # Generate quantum phase
                    phase = random.uniform(0, 2 * math.pi)
                    if is_hypermorphic:
                        # HyperMorphic zero-free fluctuation
                        fluctuation = consciousness.hyper_math.mul(
                            consciousness.hyper_math.zero_free(0.2),  # Scale of fluctuation
                            consciousness.hyper_math.zero_free(math.sin(phase))  # Phase-based oscillation
                        )
                        # Apply fluctuation to awareness
                        new_awareness = consciousness.hyper_math.add(old_awareness, fluctuation)
                        # Ensure within valid range
                        new_awareness = consciousness.hyper_math.max(
                            consciousness.hyper_math.zero_free(0.1),
                            consciousness.hyper_math.min(
                                consciousness.hyper_math.zero_free(0.9),
                                new_awareness
                            )
                        )
                    else:
                        # Standard fluctuation for non-HyperMorphic
                        fluctuation = 0.2 * math.sin(phase)
                        new_awareness = max(0.1, min(0.9, old_awareness + fluctuation))
                    # Apply new awareness
                    consciousness.awareness_level = new_awareness
                    changes.append(f"Applied quantum fluctuation to consciousness awareness: {old_awareness:.2f} → {new_awareness:.2f}")
                    variation_count += 1

                # Apply quantum jump to consciousness state if available
                if hasattr(consciousness, 'current_state') and hasattr(consciousness, 'states'):
                    old_state = consciousness.current_state
                    if random.random() < 0.3:  # 30% chance of quantum jump
                        # Get available states
                        available_states = list(consciousness.states.keys())
                        # Select new state (different from current)
                        new_states = [s for s in available_states if s != old_state]
                        if new_states:
                            new_state = random.choice(new_states)
                            # Apply quantum jump
                            consciousness.current_state = new_state
                            changes.append(f"Applied quantum jump to consciousness state: {old_state} → {new_state}")
                            variation_count += 1

            # 2. Apply quantum variation to neural architecture
            if hasattr(agent, 'model'):
                # Check for HyperMorphic model
                is_hypermorphic = hasattr(agent.model, 'hyper_math')
                if hasattr(agent.model, 'state_dict') and hasattr(agent.model, 'load_state_dict'):
                    with torch.no_grad():
                        state_dict = agent.model.state_dict()
                        # Track parameter statistics before changes
                        param_count = 0
                        perturbed_count = 0
                        # Apply phase-based quantum perturbation to weights
                        for name, param in state_dict.items():
                            if 'weight' in name or 'bias' in name:
                                param_count += 1
                                # Generate quantum perturbation with phase coherence
                                phase = random.uniform(0, 2 * math.pi)
                                # Smaller perturbation for stability
                                perturbation_scale = 0.02
                                if is_hypermorphic:
                                    # HyperMorphic zero-free perturbation
                                    perturbation = agent.model.hyper_math.zero_free(
                                        perturbation_scale * torch.randn_like(param) * math.cos(phase)
                                    )
                                    if hasattr(agent.model, '_ensure_zero_free'):
                                        param.add_(perturbation)
                                        param.data = agent.model._ensure_zero_free(param.data)
                                    else:
                                        param.add_(perturbation)
                                else:
                                    # Standard quantum-like perturbation for non-HyperMorphic
                                    perturbation = perturbation_scale * torch.randn_like(param) * math.cos(phase)
                                    param.add_(perturbation)
                                perturbed_count += 1
                        changes.append(f"Applied quantum phase-coherent perturbation to {perturbed_count}/{param_count} neural parameters")
                        variation_count += 1

                # Apply quantum variation to quantum-specific parameters if they exist
                if is_hypermorphic:
                    quantum_params_changed = 0
                    if hasattr(agent.model, 'quantum_coherence'):
                        old_coherence = agent.model.quantum_coherence
                        phase = random.uniform(0, 2 * math.pi)
                        fluctuation = agent.model.hyper_math.zero_free(0.2 * math.sin(phase))
                        new_coherence = agent.model.hyper_math.add(old_coherence, fluctuation)
                        new_coherence = agent.model.hyper_math.max(
                            agent.model.hyper_math.zero_free(0.3),
                            agent.model.hyper_math.min(
                                agent.model.hyper_math.zero_free(0.9),
                                new_coherence
                            )
                        )
                        agent.model.quantum_coherence = new_coherence
                        quantum_params_changed += 1

                    if hasattr(agent.model, 'neocortex'):
                        for layer in agent.model.neocortex:
                            if hasattr(layer, 'attention') and hasattr(layer.attention, 'phase_shifts'):
                                phase_shifts = layer.attention.phase_shifts
                                with torch.no_grad():
                                    for i in range(len(phase_shifts)):
                                        if random.random() < 0.3:
                                            new_phase = random.uniform(0, 2 * math.pi)
                                            phase_shifts[i] = new_phase
                                quantum_params_changed += 1
                    if quantum_params_changed > 0:
                        changes.append(f"Applied quantum variations to {quantum_params_changed} quantum-specific parameters")
                        variation_count += 1

            # 3. Apply quantum variation to free will decision mechanisms
            if hasattr(agent, 'free_will'):
                is_hypermorphic = hasattr(agent.free_will, 'hyper_math')
                quantum_variations = 0
                if hasattr(agent.free_will, 'exploration_weight') and hasattr(agent.free_will, 'exploitation_weight'):
                    old_expl = agent.free_will.exploration_weight
                    old_expt = agent.free_will.exploitation_weight
                    phase = random.uniform(0, 2 * math.pi)
                    if is_hypermorphic:
                        fluctuation = agent.free_will.hyper_math.mul(
                            agent.free_will.hyper_math.zero_free(0.15),
                            agent.free_will.hyper_math.zero_free(math.sin(phase))
                        )
                        new_expl = agent.free_will.hyper_math.add(old_expl, fluctuation)
                        new_expl = agent.free_will.hyper_math.max(
                            agent.free_will.hyper_math.zero_free(0.2),
                            agent.free_will.hyper_math.min(
                                agent.free_will.hyper_math.zero_free(0.8),
                                new_expl
                            )
                        )
                        new_expt = agent.free_will.hyper_math.sub(
                            agent.free_will.hyper_math.zero_free(1.0),
                            new_expl
                        )
                    else:
                        fluctuation = 0.15 * math.sin(phase)
                        new_expl = max(0.2, min(0.8, old_expl + fluctuation))
                        new_expt = 1.0 - new_expl
                    agent.free_will.exploration_weight = new_expl
                    agent.free_will.exploitation_weight = new_expt
                    changes.append(f"Applied quantum variation to exploration/exploitation balance: {old_expl:.2f}/{old_expt:.2f} → {new_expl:.2f}/{new_expt:.2f}")
                    quantum_variations += 1

                if is_hypermorphic and hasattr(agent.free_will, 'quantum_state'):
                    quantum_state = agent.free_will.quantum_state
                    for state, properties in quantum_state.items():
                        if "amplitude" in properties and "phase" in properties:
                            amplitude = properties["amplitude"]
                            phase_val = properties["phase"]
                            amplitude_fluctuation = agent.free_will.hyper_math.mul(
                                agent.free_will.hyper_math.zero_free(0.2),
                                agent.free_will.hyper_math.zero_free(random.random() - 0.5)
                            )
                            new_amplitude = agent.free_will.hyper_math.add(amplitude, amplitude_fluctuation)
                            new_amplitude = agent.free_will.hyper_math.max(
                                agent.free_will.hyper_math.zero_free(0.1),
                                agent.free_will.hyper_math.min(
                                    agent.free_will.hyper_math.zero_free(1.5),
                                    new_amplitude
                                )
                            )
                            phase_shift = random.uniform(-math.pi/2, math.pi/2)
                            new_phase = (phase_val + phase_shift) % (2 * math.pi)
                            properties["amplitude"] = new_amplitude
                            properties["phase"] = new_phase
                    amplitude_squared_sum = agent.free_will.hyper_math.zero_free(0)
                    for state, properties in quantum_state.items():
                        if "amplitude" in properties:
                            amplitude_squared = agent.free_will.hyper_math.mul(
                                properties["amplitude"],
                                properties["amplitude"]
                            )
                            amplitude_squared_sum = agent.free_will.hyper_math.add(
                                amplitude_squared_sum,
                                amplitude_squared
                            )
                    if abs(amplitude_squared_sum - 1.0) > 0.1:
                        norm_factor = agent.free_will.hyper_math.zero_free(
                            1.0 / math.sqrt(max(amplitude_squared_sum, agent.free_will.hyper_math.epsilon))
                        )
                        for state, properties in quantum_state.items():
                            if "amplitude" in properties:
                                properties["amplitude"] = agent.free_will.hyper_math.mul(
                                    properties["amplitude"],
                                    norm_factor
                                )
                    changes.append("Applied quantum superposition variations to free will quantum states")
                    quantum_variations += 1

                if is_hypermorphic and hasattr(agent.free_will, 'quantum_influence_weight'):
                    old_weight = agent.free_will.quantum_influence_weight
                    phase = random.uniform(0, 2 * math.pi)
                    fluctuation = agent.free_will.hyper_math.mul(
                        agent.free_will.hyper_math.zero_free(0.1),
                        agent.free_will.hyper_math.zero_free(math.sin(phase))
                    )
                    new_weight = agent.free_will.hyper_math.add(old_weight, fluctuation)
                    new_weight = agent.free_will.hyper_math.max(
                        agent.free_will.hyper_math.zero_free(0.2),
                        agent.free_will.hyper_math.min(
                            agent.free_will.hyper_math.zero_free(0.6),
                            new_weight
                        )
                    )
                    agent.free_will.quantum_influence_weight = new_weight
                    changes.append(f"Applied quantum variation to decision quantum influence: {old_weight:.2f} → {new_weight:.2f}")
                    quantum_variations += 1

                if quantum_variations > 0:
                    variation_count += 1

            # 4. Apply quantum variation to planner strategies
            if hasattr(agent, 'planner_sifter') and hasattr(agent.planner_sifter, 'strategies'):
                strategies = agent.planner_sifter.strategies
                if strategies:
                    original_count = len(strategies)
                    modified_strategies = 0
                    for strategy_name, strategy in strategies.items():
                        if "effectiveness" in strategy:
                            old_effectiveness = strategy["effectiveness"]
                            phase = random.uniform(0, 2 * math.pi)
                            fluctuation = 0.1 * math.sin(phase)
                            new_effectiveness = max(0.2, min(0.95, old_effectiveness + fluctuation))
                            strategy["effectiveness"] = new_effectiveness
                            modified_strategies += 1
                    strategy_names = list(strategies.keys())
                    if len(strategy_names) >= 2:
                        num_pairs = min(2, len(strategy_names) // 2)
                        for _ in range(num_pairs):
                            s1, s2 = random.sample(strategy_names, 2)
                            if "effectiveness" in strategies[s1] and "effectiveness" in strategies[s2]:
                                e1 = strategies[s1]["effectiveness"]
                                e2 = strategies[s2]["effectiveness"]
                                avg = (e1 + e2) / 2
                                diff = (e1 - e2) / 2
                                strategies[s1]["effectiveness"] = max(0.2, min(0.95, avg + diff))
                                strategies[s2]["effectiveness"] = max(0.2, min(0.95, avg - diff))
                                modified_strategies += 2
                    if modified_strategies > 0:
                        changes.append(f"Applied quantum variations to {modified_strategies} planning strategies")
                        variation_count += 1

            # Update strategy success rate based on variation count
            if variation_count > 0:
                self.strategies["quantum_variation"]["success_rate"] = self.hyper_math.min(
                    self.hyper_math.zero_free(0.95),
                    self.hyper_math.mul(
                        self.strategies["quantum_variation"]["success_rate"],
                        self.hyper_math.zero_free(1.1)
                    )
                )
                return True, f"Successfully applied {variation_count} quantum variations using HyperMorphic principles", changes
            else:
                return False, "No suitable components found for quantum variation", changes

        except Exception as e:
            self.strategies["quantum_variation"]["success_rate"] = self.hyper_math.max(
                self.hyper_math.zero_free(0.2),
                self.hyper_math.mul(
                    self.strategies["quantum_variation"]["success_rate"],
                    self.hyper_math.zero_free(0.9)
                )
            )
            return False, f"HyperMorphic quantum variation failed: {str(e)}", changes

    def _apply_holomorphic_restructuring_strategy(self, agent, changes):
        """
        Apply holomorphic restructuring strategy to transform the system
        while preserving essential mathematical structures.

        Parameters:
        - agent: Agent to evolve
        - changes: List to track changes

        Returns:
        - (success, message, changes) tuple
        """
        try:
            # Count successful holomorphic restructuring attempts
            holomorphic_count = 0

            # 1. Holomorphic restructuring of neural architecture
            if hasattr(agent, 'model') and hasattr(agent.model, 'neocortex'):
                model = agent.model
                
                # Check for HyperMorphic model
                is_hypermorphic = hasattr(model, 'hyper_math')
                
                # Apply holomorphic transformations to neural architecture
                if hasattr(model, 'state_dict') and hasattr(model, 'load_state_dict'):
                    with torch.no_grad():
                        state_dict = model.state_dict()
                        
                        # Apply holomorphic transformations to weights
                        weight_params = [name for name in state_dict.keys() if 'weight' in name]
                        
                        if weight_params:
                            # Calculate holomorphic parameters
                            holomorphic_preservation = self.hyper_math.zero_free(0.8)
                            global_phase = random.uniform(0, 2 * math.pi)
                            
                            # Track statistics
                            params_transformed = 0
                            
                            for weight_name in weight_params:
                                param = state_dict[weight_name]
                                
                                # Apply holomorphic transformation based on parameter dimensionality
                                if param.dim() >= 2:
                                    # For matrices, apply conformal mapping
                                    # Use Möbius-like transformation: w = (az + b)/(cz + d)
                                    # Simplified for stability: w = z * (cos(θ) + i*sin(θ)) * scale
                                    
                                    # Generate holomorphic parameters with phase coherence
                                    local_phase = global_phase + 0.1 * torch.tensor(weight_name.__hash__()).item() % (2 * math.pi)
                                    rotation_real = math.cos(local_phase)
                                    rotation_imag = math.sin(local_phase)
                                    
                                    # Calculate preservation factor with zero-free guarantee
                                    preservation = self.hyper_math.add(
                                        self.hyper_math.mul(
                                            holomorphic_preservation,
                                            self.hyper_math.zero_free(0.9)
                                        ),
                                        self.hyper_math.mul(
                                            self.hyper_math.sub(
                                                self.hyper_math.zero_free(1.0),
                                                holomorphic_preservation
                                            ),
                                            self.hyper_math.zero_free(0.1 * random.random())
                                        )
                                    )
                                    
                                    # Apply conformal mapping
                                    if is_hypermorphic and hasattr(model, '_ensure_zero_free'):
                                        # For weight matrices, apply to each output dimension
                                        for i in range(param.size(0)):
                                            # Apply complex rotation
                                            # Real part: w_real = z_real * cos(θ) - z_imag * sin(θ)
                                            # Imag part: w_imag = z_real * sin(θ) + z_imag * cos(θ)
                                            
                                            # Get current row as complex numbers
                                            # For simplicity, we'll use first half as real, second half as imaginary
                                            # If odd dimension, the middle value becomes purely real
                                            dim = param[i].numel()
                                            mid = dim // 2
                                            
                                            # Create temporary copy for transformation
                                            temp = param[i].clone()
                                            
                                            # Apply preservation factor
                                            # Original * preservation + transformed * (1-preservation)
                                            
                                            # Apply rotation with preservation
                                            if dim > 1:
                                                for j in range(mid):
                                                    if j + mid < dim:
                                                        # Get real and imaginary parts
                                                        z_real = temp[j].item()
                                                        z_imag = temp[j + mid].item()
                                                        
                                                        # Apply complex rotation
                                                        w_real = z_real * rotation_real - z_imag * rotation_imag
                                                        w_imag = z_real * rotation_imag + z_imag * rotation_real
                                                        
                                                        # Apply preservation factor
                                                        param[i][j] = z_real * preservation + w_real * (1 - preservation)
                                                        param[i][j + mid] = z_imag * preservation + w_imag * (1 - preservation)
                                            
                                            # Ensure zero-free constraint
                                            param[i] = model._ensure_zero_free(param[i])
                                    else:
                                        # Standard holomorphic transformation
                                        for i in range(param.size(0)):
                                            dim = param[i].numel()
                                            mid = dim // 2
                                            
                                            temp = param[i].clone()
                                            
                                            if dim > 1:
                                                for j in range(mid):
                                                    if j + mid < dim:
                                                        z_real = temp[j].item()
                                                        z_imag = temp[j + mid].item()
                                                        
                                                        w_real = z_real * rotation_real - z_imag * rotation_imag
                                                        w_imag = z_real * rotation_imag + z_imag * rotation_real
                                                        
                                                        param[i][j] = z_real * preservation + w_real * (1 - preservation)
                                                        param[i][j + mid] = z_imag * preservation + w_imag * (1 - preservation)
                                else:
                                    # For vectors, apply simpler transformation
                                    # Scale with zero-free guarantee
                                    scale = self.hyper_math.add(
                                        self.hyper_math.zero_free(1.0),
                                        self.hyper_math.mul(
                                            self.hyper_math.zero_free(0.1 * math.sin(global_phase)),
                                            self.hyper_math.sub(
                                                self.hyper_math.zero_free(1.0),
                                                holomorphic_preservation
                                            )
                                        )
                                    )
                                    
                                    # Apply scaling with preservation factor
                                    if is_hypermorphic and hasattr(model, '_ensure_zero_free'):
                                        param.mul_(scale)
                                        param.data = model._ensure_zero_free(param.data)
                                    else:
                                        param.mul_(scale)
                                
                                params_transformed += 1
                            
                            # Apply holomorphically transformed weights
                            model.load_state_dict(state_dict)
                            
                            changes.append(f"Applied holomorphic conformal transformations to {params_transformed} neural parameters")
                            holomorphic_count += 1

            # 2. Holomorphic restructuring of capability embeddings
            capability_embeddings = None
            if hasattr(agent, 'capability_embeddings'):
                capability_embeddings = agent.capability_embeddings
            elif hasattr(agent, 'free_will') and hasattr(agent.free_will, 'capability_embeddings'):
                capability_embeddings = agent.free_will.capability_embeddings
            
            if capability_embeddings and isinstance(capability_embeddings, dict):
                # Get list of capabilities
                capabilities = list(capability_embeddings.keys())
                
                if capabilities:
                    # Apply holomorphic transformations to capability embeddings
                    capabilities_transformed = 0
                    
                    # Calculate global holomorphic parameters
                    holomorphic_preservation = self.hyper_math.zero_free(0.85)
                    global_phase = random.uniform(0, 2 * math.pi)
                    
                    for capability in capabilities:
                        embedding = capability_embeddings[capability]
                        
                        if isinstance(embedding, list) and len(embedding) > 1:
                            # Generate local phase with deterministic variation
                            local_phase = global_phase + 0.1 * hash(capability) % (2 * math.pi)
                            
                            # Apply Möbius-like transformation to embedding vector
                            # We'll interpret pairs of values as complex numbers
                            for i in range(0, len(embedding) - 1, 2):
                                if i + 1 < len(embedding):
                                    # Interpret as complex number
                                    z_real = self.hyper_math.zero_free(embedding[i])
                                    z_imag = self.hyper_math.zero_free(embedding[i + 1])
                                    
                                    # Calculate rotation components
                                    rotation_real = math.cos(local_phase)
                                    rotation_imag = math.sin(local_phase)
                                    
                                    # Apply complex rotation
                                    w_real = self.hyper_math.sub(
                                        self.hyper_math.mul(z_real, rotation_real),
                                        self.hyper_math.mul(z_imag, rotation_imag)
                                    )
                                    w_imag = self.hyper_math.add(
                                        self.hyper_math.mul(z_real, rotation_imag),
                                        self.hyper_math.mul(z_imag, rotation_real)
                                    )
                                    
                                    # Apply preservation factor
                                    new_real = self.hyper_math.add(
                                        self.hyper_math.mul(z_real, holomorphic_preservation),
                                        self.hyper_math.mul(
                                            w_real,
                                            self.hyper_math.sub(
                                                self.hyper_math.zero_free(1.0),
                                                holomorphic_preservation
                                            )
                                        )
                                    )
                                    new_imag = self.hyper_math.add(
                                        self.hyper_math.mul(z_imag, holomorphic_preservation),
                                        self.hyper_math.mul(
                                            w_imag,
                                            self.hyper_math.sub(
                                                self.hyper_math.zero_free(1.0),
                                                holomorphic_preservation
                                            )
                                        )
                                    )
                                    
                                    # Update embedding
                                    embedding[i] = new_real
                                    embedding[i + 1] = new_imag
                            
                            # Normalize embedding after transformation
                            norm = math.sqrt(sum(self.hyper_math.mul(x, x) for x in embedding))
                            if norm > self.hyper_math.epsilon:
                                embedding = [self.hyper_math.div(x, self.hyper_math.zero_free(norm)) for x in embedding]
                            
                            # Update capability embedding
                            capability_embeddings[capability] = embedding
                            capabilities_transformed += 1
                    
                    if capabilities_transformed > 0:
                        changes.append(f"Applied holomorphic Möbius transformations to {capabilities_transformed} capability embeddings")
                        holomorphic_count += 1

            # 3. Holomorphic restructuring of quantum state transitions
            if hasattr(agent, 'ai_manager') and hasattr(agent.ai_manager, 'consciousness'):
                consciousness = agent.ai_manager.consciousness
                
                # Check for HyperMorphic consciousness and quantum state
                if (hasattr(consciousness, 'hyper_math') and 
                    hasattr(consciousness, 'quantum_state') and 
                    hasattr(consciousness, 'state_transitions')):
                    
                    quantum_state = consciousness.quantum_state
                    state_transitions = consciousness.state_transitions
                    
                    if isinstance(quantum_state, dict) and isinstance(state_transitions, dict):
                        # Calculate global holomorphic parameters
                        holomorphic_preservation = self.hyper_math.zero_free(0.9)
                        global_phase = random.uniform(0, 2 * math.pi)
                        
                        # Track states transformed
                        states_transformed = 0
                        
                        # Apply Holomorphic SU(2) transformations to quantum states
                        for state, properties in quantum_state.items():
                            if "amplitude" in properties and "phase" in properties:
                                # Get current state parameters
                                amplitude = properties["amplitude"]
                                phase = properties["phase"]
                                
                                # Generate local transformation parameters
                                local_phase = global_phase + 0.1 * hash(state) % (2 * math.pi)
                                
                                # Calculate SU(2) matrix parameters
                                alpha = math.cos(local_phase / 2)
                                beta = math.sin(local_phase / 2)
                                
                                # Apply SU(2) transformation to state vector
                                # |ψ'⟩ = α|ψ⟩ + β|ψ⊥⟩
                                # where |ψ⊥⟩ is the orthogonal state
                                
                                # Calculate new amplitude and phase
                                new_amplitude = consciousness.hyper_math.zero_free(
                                    math.sqrt(alpha**2 * amplitude**2 + beta**2 * (1 - amplitude**2))
                                )
                                
                                # Calculate new phase
                                phase_shift = math.atan2(beta * math.sin(phase), alpha * math.cos(phase))
                                new_phase = (phase + phase_shift) % (2 * math.pi)
                                
                                # Apply preservation factor
                                preserved_amplitude = consciousness.hyper_math.add(
                                    consciousness.hyper_math.mul(amplitude, holomorphic_preservation),
                                    consciousness.hyper_math.mul(
                                        new_amplitude,
                                        consciousness.hyper_math.sub(
                                            consciousness.hyper_math.zero_free(1.0),
                                            holomorphic_preservation
                                        )
                                    )
                                )
                                
                                preserved_phase = (phase * holomorphic_preservation + 
                                                new_phase * (1 - holomorphic_preservation)) % (2 * math.pi)
                                
                                # Update quantum state
                                properties["amplitude"] = preserved_amplitude
                                properties["phase"] = preserved_phase
                                
                                states_transformed += 1
                        
                        # Ensure quantum state normalization
                        amplitude_squared_sum = consciousness.hyper_math.zero_free(0)
                        for state, properties in quantum_state.items():
                            if "amplitude" in properties:
                                amplitude_squared = consciousness.hyper_math.mul(
                                    properties["amplitude"],
                                    properties["amplitude"]
                                )
                                amplitude_squared_sum = consciousness.hyper_math.add(
                                    amplitude_squared_sum,
                                    amplitude_squared
                                )
                        
                        if abs(amplitude_squared_sum - 1.0) > 0.1:
                            norm_factor = consciousness.hyper_math.zero_free(
                                1.0 / math.sqrt(max(amplitude_squared_sum, consciousness.hyper_math.epsilon))
                            )
                            for state, properties in quantum_state.items():
                                if "amplitude" in properties:
                                    properties["amplitude"] = consciousness.hyper_math.mul(
                                        properties["amplitude"],
                                        norm_factor
                                    )
                        
                        if states_transformed > 0:
                            changes.append(f"Applied holomorphic SU(2) transformations to {states_transformed} quantum consciousness states")
                            holomorphic_count += 1
                        
                        # Apply holomorphic transformations to state transitions
                        transitions_transformed = 0
                        
                        for from_state, transitions in state_transitions.items():
                            if isinstance(transitions, dict):
                                # Generate local transformation parameters
                                local_phase = global_phase + 0.1 * hash(from_state) % (2 * math.pi)
                                
                                # Calculate conformal mapping parameters
                                rotation_real = math.cos(local_phase)
                                rotation_imag = math.sin(local_phase)
                                
                                # Get total probability
                                total_prob = sum(consciousness.hyper_math.zero_free(p) for p in transitions.values())
                                
                                if total_prob > consciousness.hyper_math.epsilon:
                                    # Apply holomorphic transformation to transition probabilities
                                    # Maintain total probability (conformal map preserves angles)
                                    
                                    # First, convert to normalized complex numbers
                                    complex_probs = {}
                                    for to_state, prob in transitions.items():
                                        norm_prob = consciousness.hyper_math.div(
                                            consciousness.hyper_math.zero_free(prob),
                                            total_prob
                                        )
                                        
                                        # Convert to complex number with random phase
                                        state_phase = hash(to_state) % (2 * math.pi)
                                        real = norm_prob * math.cos(state_phase)
                                        imag = norm_prob * math.sin(state_phase)
                                        
                                        complex_probs[to_state] = (real, imag)
                                    
                                    # Apply holomorphic transformation
                                    transformed_probs = {}
                                    for to_state, (real, imag) in complex_probs.items():
                                        # Apply complex rotation
                                        new_real = real * rotation_real - imag * rotation_imag
                                        new_imag = real * rotation_imag + imag * rotation_real
                                        
                                        # Convert back to probability (magnitude squared)
                                        new_prob = new_real**2 + new_imag**2
                                        
                                        # Apply preservation factor
                                        orig_prob = transitions[to_state]
                                        preserved_prob = orig_prob * holomorphic_preservation + new_prob * total_prob * (1 - holomorphic_preservation)
                                        
                                        transformed_probs[to_state] = consciousness.hyper_math.zero_free(preserved_prob)
                                    
                                    # Renormalize after transformation
                                    new_total = sum(transformed_probs.values())
                                    if new_total > consciousness.hyper_math.epsilon:
                                        scale_factor = consciousness.hyper_math.div(
                                            total_prob,
                                            new_total
                                        )
                                        
                                        # Apply scaling and update transitions
                                        for to_state, prob in transformed_probs.items():
                                            transitions[to_state] = consciousness.hyper_math.mul(
                                                prob,
                                                scale_factor
                                            )
                                        
                                        transitions_transformed += 1
                        
                        if transitions_transformed > 0:
                            changes.append(f"Applied holomorphic conformal transformations to {transitions_transformed} state transition matrices")
                            holomorphic_count += 1

            # Update strategy success rate based on holomorphic count
            self.strategies["holomorphic_restructuring"]["success_rate"] = self.hyper_math.min(
                self.hyper_math.zero_free(0.95),  # Cap at 0.95
                self.hyper_math.mul(
                    self.strategies["holomorphic_restructuring"]["success_rate"],
                    self.hyper_math.zero_free(1.1)  # 10% increase
                )
            )

            success = holomorphic_count > 0

            if success:
                return True, f"Successfully applied {holomorphic_count} holomorphic transformations to preserve system structure", changes
            else:
                return False, "No components were suitable for holomorphic restructuring", changes

        except Exception as e:
            # Update strategy success rate (decrease on failure)
            self.strategies["holomorphic_restructuring"]["success_rate"] = self.hyper_math.max(
                self.hyper_math.zero_free(0.2),
                self.hyper_math.mul(
                    self.strategies["holomorphic_restructuring"]["success_rate"],
                    self.hyper_math.zero_free(0.9)  # 10% decrease
                )
            )

            return False, f"HyperMorphic holomorphic restructuring failed: {str(e)}", changes

    def _apply_blended_strategy(self, agent, strategy, changes):
        """
        Apply a blended strategy created from two or more base strategies.

        Parameters:
        - agent: Agent to evolve
        - strategy: Blended strategy name (format: "strategy1_strategy2_blend")
        - changes: List to track changes

        Returns:
        - (success, message, changes) tuple
        """
        try:
            # Parse blend components from strategy name
            if "_blend" not in strategy or strategy not in self.blended_strategies_registry:
                return False, f"Invalid blended strategy: {strategy}", changes

            # Get blended strategy details
            blended_strategy_data = self.blended_strategies_registry[strategy]
            
            if not isinstance(blended_strategy_data, dict):
                return False, f"Invalid blended strategy data for: {strategy}", changes
            
            # Extract component strategies
            component_strategies = blended_strategy_data.get("component_strategies", [])
            
            if not component_strategies or len(component_strategies) < 2:
                return False, f"Blended strategy {strategy} has insufficient components", changes
            
            # Apply components with equal weighting
            success_count = 0
            total_changes = []
            success_messages = []
            
            # Apply each component strategy
            for component in component_strategies:
                if component in self.strategies:
                    # Get component strategy method
                    strategy_method = None
                    
                    if component == "expansion":
                        strategy_method = self._apply_expansion_strategy
                    elif component == "pruning":
                        strategy_method = self._apply_pruning_strategy
                    elif component == "restructuring":
                        strategy_method = self._apply_restructuring_strategy
                    elif component == "specialization":
                        strategy_method = self._apply_specialization_strategy
                    elif component == "integration":
                        strategy_method = self._apply_integration_strategy
                    elif component == "quantum_variation":
                        strategy_method = self._apply_quantum_variation_strategy
                    elif component == "holomorphic_restructuring":
                        strategy_method = self._apply_holomorphic_restructuring_strategy
                    
                    # Apply component strategy
                    if strategy_method:
                        component_changes = []
                        success, message, component_changes = strategy_method(agent, component_changes)
                        
                        if success:
                            success_count += 1
                            success_messages.append(f"{component}: {message}")
                            total_changes.extend(component_changes)
            
            # Update blended strategy success rate
            if success_count > 0:
                # Calculate success rate based on component success
                success_ratio = self.hyper_math.div(
                    self.hyper_math.zero_free(success_count),
                    self.hyper_math.zero_free(len(component_strategies))
                )
                
                # Update success rate with HyperMorphic operations
                if "success_rate" in blended_strategy_data:
                    old_rate = blended_strategy_data["success_rate"]
                    
                    # Calculate new rate with exponential moving average
                    new_rate = self.hyper_math.add(
                        self.hyper_math.mul(old_rate, self.hyper_math.zero_free(0.7)),
                        self.hyper_math.mul(success_ratio, self.hyper_math.zero_free(0.3))
                    )
                    
                    blended_strategy_data["success_rate"] = new_rate
                
                # Update changes list
                changes.extend(total_changes)
                
                # Create success message
                message = f"Successfully applied blended strategy '{strategy}' with {success_count}/{len(component_strategies)} components"
                
                # Include component details
                if len(success_messages) > 0:
                    message += f": {'; '.join(success_messages)}"
                
                return True, message, changes
            else:
                # Update success rate for complete failure
                if "success_rate" in blended_strategy_data:
                    old_rate = blended_strategy_data["success_rate"]
                    
                    # Decrease rate with exponential moving average
                    new_rate = self.hyper_math.mul(old_rate, self.hyper_math.zero_free(0.9))
                    
                    blended_strategy_data["success_rate"] = new_rate
                
                return False, f"Failed to apply any component of blended strategy '{strategy}'", changes

        except Exception as e:
            # Update success rate for error
            if strategy in self.blended_strategies_registry:
                blended_strategy_data = self.blended_strategies_registry[strategy]
                
                if isinstance(blended_strategy_data, dict) and "success_rate" in blended_strategy_data:
                    old_rate = blended_strategy_data["success_rate"]
                    
                    # Significant decrease for errors
                    new_rate = self.hyper_math.mul(old_rate, self.hyper_math.zero_free(0.8))
                    
                    blended_strategy_data["success_rate"] = new_rate
            
            return False, f"Error applying blended strategy '{strategy}': {str(e)}", changes

    def blend_strategies(self, strategy1, strategy2, blend_factor=0.5):
        """
        Create a blended strategy from two existing strategies using
        HyperMorphic concept blending.

        Parameters:
        - strategy1: First strategy name
        - strategy2: Second strategy name
        - blend_factor: HyperMorphic weight for blending (0.0-1.0)

        Returns:
        - (name, strategy_data) tuple for the blended strategy
        """
        # Check if strategies exist
        if strategy1 not in self.strategies or strategy2 not in self.strategies:
            return None, None
        
        # Check if this blend already exists
        blend_name = f"{strategy1}_{strategy2}_blend"
        if blend_name in self.blended_strategies_registry:
            return blend_name, self.blended_strategies_registry[blend_name]
        
        # Create blended strategy using quantum concept blending
        strategy1_data = self.strategies[strategy1]
        strategy2_data = self.strategies[strategy2]
        
        blended_data = self.quantum_concept_blend(strategy1_data, strategy2_data, blend_factor)
        
        if not blended_data:
            return None, None
        
        # Add component strategies to blended data
        blended_data["component_strategies"] = [strategy1, strategy2]
        
        # Create name and register blended strategy
        self.blended_strategies_registry[blend_name] = blended_data
        
        return blend_name, blended_data

    def _update_strategy_success_rates(self, strategy, success):
        """
        Update strategy success rates based on outcome with HyperMorphic
        zero-free operations.

        Parameters:
        - strategy: Strategy that was applied
        - success: Whether the strategy was successful
        """
        # Update success rate for the applied strategy
        if strategy in self.strategies:
            current_rate = self.strategies[strategy]["success_rate"]
            
            if success:
                # Increase success rate on success
                new_rate = self.hyper_math.min(
                    self.hyper_math.zero_free(0.95),  # Cap at 0.95
                    self.hyper_math.mul(
                        current_rate,
                        self.hyper_math.zero_free(1.1)  # 10% increase
                    )
                )
            else:
                # Decrease success rate on failure
                new_rate = self.hyper_math.max(
                    self.hyper_math.zero_free(0.2),  # Floor at 0.2
                    self.hyper_math.mul(
                        current_rate,
                        self.hyper_math.zero_free(0.9)  # 10% decrease
                    )
                )
            
            self.strategies[strategy]["success_rate"] = new_rate
        
        # If this is a blended strategy, update its registry entry
        elif "_blend" in strategy and strategy in self.blended_strategies_registry:
            blended_data = self.blended_strategies_registry[strategy]
            
            if isinstance(blended_data, dict) and "success_rate" in blended_data:
                current_rate = blended_data["success_rate"]
                
                if success:
                    # Increase success rate on success
                    new_rate = self.hyper_math.min(
                        self.hyper_math.zero_free(0.95),  # Cap at 0.95
                        self.hyper_math.mul(
                            current_rate,
                            self.hyper_math.zero_free(1.1)  # 10% increase
                        )
                    )
                else:
                    # Decrease success rate on failure
                    new_rate = self.hyper_math.max(
                        self.hyper_math.zero_free(0.2),  # Floor at 0.2
                        self.hyper_math.mul(
                            current_rate,
                            self.hyper_math.zero_free(0.9)  # 10% decrease
                        )
                    )
                
                blended_data["success_rate"] = new_rate

    def _update_strategy_relationships(self, strategy):
        """
        Update quantum entanglement between strategies based on
        successful application of a strategy.

        Parameters:
        - strategy: Strategy that was successfully applied
        """
        # Only update relationships if strategy exists
        if strategy not in self.strategy_relationships:
            return
        
        # Apply quantum entanglement strengthening to related strategies
        relationships = self.strategy_relationships[strategy]
        
        # Calculate entanglement with quantum phase coherence
        global_phase = random.uniform(0, 2 * math.pi)
        
        for related_strategy, rel_data in relationships.items():
            # Update relationship strength with phase coherence
            current_strength = rel_data.get("strength", self.hyper_math.zero_free(0.1))
            current_phase = rel_data.get("phase", 0.0)
            
            # Calculate phase interference
            # Constructive interference strengthens relationship
            phase_diff = abs((current_phase - global_phase + math.pi) % (2 * math.pi) - math.pi)
            phase_coherence = math.cos(phase_diff)
            
            # Calculate strength adjustment with quantum interference
            if phase_coherence > 0:
                # Constructive interference - strengthen relationship
                strength_change = self.hyper_math.mul(
                    self.hyper_math.zero_free(0.1),  # Base change
                    self.hyper_math.zero_free(phase_coherence)  # Phase influence
                )
                
                # Apply adjustment with zero-free guarantee
                new_strength = self.hyper_math.min(
                    self.hyper_math.zero_free(0.95),  # Cap at 0.95
                    self.hyper_math.add(
                        current_strength,
                        strength_change
                    )
                )
            else:
                # Destructive interference - weaken relationship
                strength_change = self.hyper_math.mul(
                    self.hyper_math.zero_free(0.05),  # Smaller base change
                    self.hyper_math.zero_free(-phase_coherence)  # Negative phase influence
                )
                
                # Apply adjustment with zero-free guarantee
                new_strength = self.hyper_math.max(
                    self.hyper_math.zero_free(0.1),  # Floor at 0.1
                    self.hyper_math.add(
                        current_strength,
                        strength_change
                    )
                )
            
            # Update relationship strength
            rel_data["strength"] = new_strength
            
            # Evolve phase with quantum drift
            phase_drift = 0.1 * random.random() * self.quantum_entanglement_factor
            new_phase = (current_phase + phase_drift) % (2 * math.pi)
            rel_data["phase"] = new_phase
            
            # Update reciprocal relationship if it exists
            if related_strategy in self.strategy_relationships and strategy in self.strategy_relationships[related_strategy]:
                self.strategy_relationships[related_strategy][strategy]["strength"] = new_strength
                self.strategy_relationships[related_strategy][strategy]["phase"] = (new_phase + math.pi) % (2 * math.pi)  # Opposite phase

class AutonomousMind:
    """
    Integrated cognitive system that coordinates high-level thinking processes
    and manages the agent's internal cognitive states and modes.
    """
    def __init__(self, agent, model):
        self.agent = agent
        self.model = model
        self.thought_history = []
        self.cognitive_states = {
            "analytical": {"description": "Logical problem solving with structured reasoning", "activation": 0.5},
            "creative": {"description": "Divergent thinking with novel connections", "activation": 0.5},
            "reflective": {"description": "Meta-cognitive examination of own thoughts", "activation": 0.5},
            "exploratory": {"description": "Curious investigation of new information", "activation": 0.5},
            "critical": {"description": "Evaluative assessment with skepticism", "activation": 0.5},
            "integrative": {"description": "Synthesis of diverse knowledge", "activation": 0.5},
            "intuitive": {"description": "Fast pattern recognition", "activation": 0.5},
            "balanced": {"description": "Equilibrium of multiple modes", "activation": 0.5}
        }
        self.current_mode = "balanced"  # Default mode
        self.attention_focus = None  # Current attentional focus
        self.working_memory = []  # Active concepts and thoughts
        self.working_memory_capacity = 7  # Miller's magical number
        self.cognitive_load = 0.5  # Current processing load (0.0-1.0)
        self.thought_depth = 0.5  # Depth vs. breadth of thinking (0.0-1.0)
        self.concept_activation_threshold = 0.3  # Min activation for attention
        self.mode_shift_probability = 0.2  # Probability of spontaneous mode shift
        self.recent_insights = []  # Store recent important realizations

        # Thinking style parameters
        self.thinking_style = {
            "abstraction_level": 0.6,  # Concrete (0.0) to abstract (1.0)
            "linearity": 0.5,  # Linear (0.0) to non-linear (1.0)
            "deductive_inductive_balance": 0.5,  # Deductive (0.0) to inductive (1.0)
            "risk_tolerance": 0.4,  # Risk-averse (0.0) to risk-seeking (1.0)
            "concept_granularity": 0.5  # Fine-grained (0.0) to coarse-grained (1.0)
        }

        log_event("AutonomousMind initialized in balanced cognitive mode", "INFO")

    def think(self, context=None):
        """
        Execute a thinking cycle to generate insights, shift cognitive modes,
        and update internal state based on current context.

        Parameters:
        - context: Current perceptual and memory context

        Returns:
        - Thought result containing insights and state changes
        """
        # Update cognitive load based on context complexity
        self._update_cognitive_load(context)

        # Consider shifting cognitive mode
        self._consider_mode_shift(context)

        # Generate thought based on current mode and context
        thought = self._generate_thought(context)

        # Update working memory
        self._update_working_memory(thought)

        # Store in thought history
        self.thought_history.append({
            "thought": thought,
            "mode": self.current_mode,
            "timestamp": datetime.now().isoformat(),
            "context": self._summarize_context(context)
        })

        # Trim history if needed
        if len(self.thought_history) > 100:
            self.thought_history = self.thought_history[-100:]

        return thought

    def _update_cognitive_load(self, context):
        """Update cognitive load based on context complexity"""
        if not context:
            # Default slight reduction in cognitive load when no new input
            self.cognitive_load = max(0.1, self.cognitive_load * 0.9)
            return

        # Estimate context complexity
        complexity = 0.5  # Default medium complexity

        # Adjust based on context details if available
        if isinstance(context, dict):
            # More elements = higher complexity
            complexity += min(0.3, len(context) * 0.02)

            # Check for special high-complexity elements
            if "error" in context or "anomaly" in context:
                complexity += 0.2

            # High memory usage increases cognitive load
            if "memory_size" in context:
                memory_usage = context["memory_size"] / MEMORY_MAX_SIZE
                complexity += memory_usage * 0.2

            # Multiple recent actions increase complexity
            if "recent_actions" in context and isinstance(context["recent_actions"], list):
                complexity += min(0.1, len(context["recent_actions"]) * 0.02)

        # Dynamically adjust cognitive load
        # New complexity pulls the current load toward itself
        adjustment_rate = 0.3  # How quickly load adjusts
        self.cognitive_load = self.cognitive_load * (1 - adjustment_rate) + complexity * adjustment_rate

        # Ensure within bounds
        self.cognitive_load = max(0.1, min(0.9, self.cognitive_load))

    def _consider_mode_shift(self, context):
        """Consider shifting cognitive mode based on context and internal state"""
        # Base probability of mode shift
        shift_probability = self.mode_shift_probability

        # Adjust based on cognitive load - higher load may require mode shift
        if self.cognitive_load > 0.7:
            shift_probability += 0.2

        # Check if we should shift
        if random.random() >= shift_probability:
            return  # No shift this cycle

        # Current active states
        active_states = {state: data["activation"] for state, data in self.cognitive_states.items()
                       if data["activation"] >= self.concept_activation_threshold}

        # Context-based mode selection
        new_mode = self._select_appropriate_mode(context, active_states)

        # If new mode is different, make the shift
        if new_mode and new_mode != self.current_mode:
            old_mode = self.current_mode
            self.current_mode = new_mode

            log_event(f"Cognitive mode shifted: {old_mode} → {new_mode}", "INFO")

            # Adjust activations - boost new mode, slightly reduce others
            for state in self.cognitive_states:
                if state == new_mode:
                    self.cognitive_states[state]["activation"] = min(0.9, self.cognitive_states[state]["activation"] + 0.2)
                else:
                    self.cognitive_states[state]["activation"] = max(0.1, self.cognitive_states[state]["activation"] * 0.9)

    def _select_appropriate_mode(self, context, active_states):
        """
        Select most appropriate cognitive mode based on context.

        Parameters:
        - context: Current context dictionary
        - active_states: Dictionary of currently active cognitive states

        Returns:
        - Selected mode name
        """
        if not context:
            # Without context, weighted random selection from active states
            if active_states:
                states = list(active_states.keys())
                weights = [active_states[s] for s in states]
                return random.choices(states, weights=weights, k=1)[0]
            else:
                return "balanced"  # Default fallback

        # Context-based selection
        mode_scores = {mode: 0.0 for mode in self.cognitive_states}

        # 1. Check for error or anomaly - activates critical mode
        if "error" in context or "anomaly" in context:
            mode_scores["critical"] += 0.5

        # 2. Check for exploration needs
        if self._context_indicates_exploration(context):
            mode_scores["exploratory"] += 0.4
            mode_scores["creative"] += 0.3

        # 3. Check for complex problem solving
        if self._context_indicates_complex_problem(context):
            mode_scores["analytical"] += 0.5
            mode_scores["integrative"] += 0.3

        # 4. Check for learning and reflection needs
        if self._context_indicates_reflection(context):
            mode_scores["reflective"] += 0.5

        # 5. Check for pattern recognition needs
        if self._context_indicates_pattern_recognition(context):
            mode_scores["intuitive"] += 0.4

        # 6. Add random factor for exploration
        for mode in mode_scores:
            mode_scores[mode] += random.uniform(0, 0.2)

        # 7. Add existing activation bias
        for mode, activation in active_states.items():
            mode_scores[mode] += activation * 0.3

        # Select highest scoring mode
        if mode_scores:
            top_mode = max(mode_scores.items(), key=lambda x: x[1])[0]
            return top_mode
        else:
            return "balanced"  # Default fallback

    def _context_indicates_exploration(self, context):
        """Check if context suggests exploration needs"""
        if not context or not isinstance(context, dict):
            return False

        indicators = 0

        # Looking for exploration indicators
        if "current_goal" in context and isinstance(context["current_goal"], dict):
            goal_desc = context["current_goal"].get("description", "").lower()
            if "explor" in goal_desc or "discover" in goal_desc:
                indicators += 1

        # Few domains visited suggests exploration need
        if "domains_visited" in context:
            domains = context["domains_visited"]
            if isinstance(domains, set) and len(domains) < 10:
                indicators += 1

        # Current mode includes exploration elements
        if context.get("thinking_mode") in ["exploratory", "creative"]:
            indicators += 1

        return indicators >= 1

    def _context_indicates_complex_problem(self, context):
        """Check if context suggests complex problem solving needs"""
        if not context or not isinstance(context, dict):
            return False

        indicators = 0

        # Goals with certain keywords
        if "current_goal" in context and isinstance(context["current_goal"], dict):
            goal_desc = context["current_goal"].get("description", "").lower()
            complex_terms = ["optim", "improv", "analy", "solv", "complex"]
            if any(term in goal_desc for term in complex_terms):
                indicators += 1

        # High cognitive load suggests complex problem
        if self.cognitive_load > 0.7:
            indicators += 1

        # Current analytical thinking mode
        if context.get("thinking_mode") in ["analytical", "critical"]:
            indicators += 1

        return indicators >= 1

    def _context_indicates_reflection(self, context):
        """Check if context suggests reflection needs"""
        if not context or not isinstance(context, dict):
            return False

        indicators = 0

        # Goals with reflection keywords
        if "current_goal" in context and isinstance(context["current_goal"], dict):
            goal_desc = context["current_goal"].get("description", "").lower()
            reflection_terms = ["reflect", "learn", "adapt", "improv", "assess"]
            if any(term in goal_desc for term in reflection_terms):
                indicators += 1

        # After many actions, reflection is valuable
        if "stats" in context and isinstance(context["stats"], dict):
            cycles = context["stats"].get("cycles_run", 0)
            if cycles > 0 and cycles % 10 == 0:  # Every 10 cycles
                indicators += 1

        # Current reflective thinking mode
        if context.get("thinking_mode") == "reflective":
            indicators += 1

        return indicators >= 1

    def _context_indicates_pattern_recognition(self, context):
        """Check if context suggests pattern recognition needs"""
        if not context or not isinstance(context, dict):
            return False

        indicators = 0

        # Domain stats suggest pattern recognition
        if "domain_stats" in context and isinstance(context["domain_stats"], dict):
            # Many domains = opportunity for pattern recognition
            if len(context["domain_stats"]) > 5:
                indicators += 1

        # Multiple recent actions create patterns
        if "recent_actions" in context and isinstance(context["recent_actions"], list):
            if len(context["recent_actions"]) >= 3:
                indicators += 1

        # Current intuitive mode
        if context.get("thinking_mode") == "intuitive":
            indicators += 1

        return indicators >= 1

    def _generate_thought(self, context):
        """
        Generate a thought based on current cognitive mode and context.

        Parameters:
        - context: Current context dictionary

        Returns:
        - Generated thought dictionary
        """
        # Each cognitive mode has a different thought generation approach
        if self.current_mode == "analytical":
            return self._generate_analytical_thought(context)
        elif self.current_mode == "creative":
            return self._generate_creative_thought(context)
        elif self.current_mode == "reflective":
            return self._generate_reflective_thought(context)
        elif self.current_mode == "exploratory":
            return self._generate_exploratory_thought(context)
        elif self.current_mode == "critical":
            return self._generate_critical_thought(context)
        elif self.current_mode == "integrative":
            return self._generate_integrative_thought(context)
        elif self.current_mode == "intuitive":
            return self._generate_intuitive_thought(context)
        else:  # balanced or any other
            return self._generate_balanced_thought(context)

    def _generate_analytical_thought(self, context):
        """Generate an analytical thought focused on logical problem solving"""
        # Default analytical thought structure
        thought = {
            "type": "analytical",
            "content": "Systematic analysis of current state and options",
            "components": [],
            "insights": [],
            "importance": 0.5
        }

        # Without context, generate generic analytical thought
        if not context:
            thought["content"] = "Need more information to perform proper analysis"
            return thought

        # Analytical components based on context
        components = []

        # Analyze current goal if available
        if "current_goal" in context and isinstance(context["current_goal"], dict):
            goal = context["current_goal"]
            components.append({
                "focus": "goal_analysis",
                "content": f"Analyzing goal: {goal.get('description', 'unknown')}",
                "priority": goal.get("priority", 0.5)
            })

        # Analyze recent actions if available
        if "recent_actions" in context and isinstance(context["recent_actions"], list):
            actions = context["recent_actions"]
            if actions:
                action_types = [a.get("action", "unknown") for a in actions if isinstance(a, dict)]
                components.append({
                    "focus": "action_pattern_analysis",
                    "content": f"Recent action sequence: {' → '.join(action_types)}",
                    "outcome_assessment": self._assess_action_outcomes(actions)
                })

        # Analyze domain statistics if available
        if "domain_stats" in context and isinstance(context["domain_stats"], dict):
            domains = context["domain_stats"]
            if domains:
                high_error_domains = [d for d, stats in domains.items()
                                    if isinstance(stats, dict) and stats.get("error_rate", 0) > 0.3]
                if high_error_domains:
                    components.append({
                        "focus": "error_analysis",
                        "content": f"High error rates detected in domains: {', '.join(high_error_domains[:3])}",
                        "recommendation": "Investigate causes or avoid these domains temporarily"
                    })

        # Generate insights based on analysis
        insights = []

        # Derive insights from components
        for component in components:
            if component["focus"] == "goal_analysis" and component.get("priority", 0) > 0.7:
                insights.append("Current goal is high priority - allocate additional resources")
            elif component["focus"] == "action_pattern_analysis":
                outcome = component.get("outcome_assessment")
                if outcome == "deteriorating":
                    insights.append("Action patterns show declining effectiveness - strategy change needed")
                elif outcome == "improving":
                    insights.append("Action patterns show improving outcomes - continue current approach")
            elif component["focus"] == "error_analysis":
                insights.append("Error pattern analysis suggests need for improved domain validation")

        # Add components and insights to thought
        thought["components"] = components
        thought["insights"] = insights

        # Calculate importance based on components and insights
        if insights:
            thought["importance"] = 0.7

        # Generate summary content
        if insights:
            thought["content"] = f"Analytical conclusion: {insights[0]}"
        elif components:
            thought["content"] = f"Analysis of {len(components)} system components completed"

        return thought

    def _generate_creative_thought(self, context):
        """Generate a creative thought focused on novel connections"""
        # Default creative thought structure
        thought = {
            "type": "creative",
            "content": "Novel conceptual connection",
            "associations": [],
            "insights": [],
            "importance": 0.5
        }

        # Creative aspects to consider
        aspects = ["goals", "domains", "strategies", "patterns", "anomalies"]
        selected_aspects = random.sample(aspects, min(2, len(aspects)))

        # Generate random associations between selected aspects
        associations = []

        # Extract relevant elements from context
        elements = self._extract_elements_from_context(context, selected_aspects)

        # Generate associations between elements
        if len(elements) >= 2:
            # Take two random elements and create association
            element_pairs = []
            for i in range(min(3, len(elements))):
                pair = random.sample(elements, 2)
                element_pairs.append(pair)

            # Create associations from pairs
            for pair in element_pairs:
                association_templates = [
                    f"Unexpected connection between {pair[0]} and {pair[1]}",
                    f"{pair[0]} could be viewed through the lens of {pair[1]}",
                    f"What if the structure of {pair[0]} were applied to {pair[1]}?",
                    f"The patterns in {pair[0]} mirror aspects of {pair[1]} in a novel way"
                ]
                associations.append(random.choice(association_templates))
        else:
            # Fallback for limited context
            association_templates = [
                "Novel perspective: consider alternative goal structures",
                "What if search and evaluation were integrated more tightly?",
                "The pattern of successful interactions might reveal emergent properties"
            ]
            associations.append(random.choice(association_templates))

        # Generate insights from associations
        insights = []
        if associations:
            for association in associations:
                if "connection between" in association:
                    insights.append(f"Explore this connection to potentially develop new capabilities")
                elif "through the lens" in association:
                    insights.append(f"This perspective shift might reveal hidden patterns")
                elif "What if" in association:
                    insights.append(f"This hypothetical restructuring could improve system flexibility")
                else:
                    insights.append(f"This pattern recognition suggests deeper structural similarities")

        # Add associations and insights to thought
        thought["associations"] = associations
        thought["insights"] = insights

        # Calculate importance
        if insights:
            thought["importance"] = 0.6 + 0.1 * len(insights)


def _apply_learning_rate_safeguards(self, new_lr):
    """Prevent learning rate from spiraling into oblivion"""
    # Establish absolute minimum learning rate
    ABSOLUTE_MIN_LR = 5e-6

    if new_lr < ABSOLUTE_MIN_LR:
        log_event(f"Learning rate hit critical threshold: {new_lr:.8f}, resetting to {ABSOLUTE_MIN_LR:.6f}", "WARNING")
        return ABSOLUTE_MIN_LR

    # Prevent excessive downward adjustment
    if self.learning_rate_history and new_lr < self.learning_rate_history[-1] * 0.5:
        safer_lr = self.learning_rate_history[-1] * 0.8
        log_event(f"Excessive LR reduction prevented: {new_lr:.8f} → {safer_lr:.6f}", "INFO")
        return safer_lr

    return new_lr






class HyperMorphicNormalization(nn.Module):
    """
    HyperMorphic Normalization Layer

    A custom normalization layer that can be extended with hypermorphic operations.
    Currently, it wraps the standard nn.LayerNorm but can be modified to include
    additional stability or adaptive features specific to HyperMorphic operations.

    Args:
        embed_dim (int): Dimensionality of the input.
        eps (float): A value added to the denominator for numerical stability (default=1e-5).
    """
    def __init__(self, embed_dim, eps=1e-5):
        super(HyperMorphicNormalization, self).__init__()
        self.layernorm = nn.LayerNorm(embed_dim, eps=eps)

    def forward(self, x):
        # You can add custom hypermorphic adjustments here if needed.
        return self.layernorm(x)






# Configuration - UPDATED PATHS for Google Drive checkpointing
MODEL_PATH = "/content/gdrive/MyDrive/quantum_nexus/quantum_nexus_model_checkpoint.pth" # Checkpoint path
GOOGLE_DRIVE_MODEL_PATH = "/content/gdrive/MyDrive/quantum_nexus/quantum_nexus_model_checkpoint.pth" # Checkpoint path
LOCAL_MODEL_SAVE_PATH = "quantum_nexus_model.pth" # Local backup path
LOG_FILE = "quantum_nexus_log.txt"
FLASK_PORT = 5012
AGENT_STATE_FILE = "quantum_nexus_state.json" # No longer used - state is part of checkpoint
GOOGLE_DRIVE_STATE_FILE = "/content/gdrive/MyDrive/quantum_nexus/quantum_nexus_state.json" # Checkpoint path for agent state (also in checkpoint now)
SELF_MODIFY_INTERVAL = 20
ANNEAL_GAMMA = 0.995
MEMORY_MAX_SIZE = 1000
SAVE_INTERVAL = 5 # Save checkpoint more frequently for longer runs

# =============================================================================
# MAIN EXECUTION
# =============================================================================
adaptive_learning = None

# =============================================================================
# IMPROVED MAIN EXECUTION FUNCTIONS
# =============================================================================

# Global variables for tracking agent state
adaptive_learning = None
agent_instance = None

def load_or_create_model():
    """
    Loads QuantumNexusModel from checkpoint file if available, or creates a new one.
    Enhanced with better error handling and fallback mechanisms.

    Returns: model object
    """
    model_path = GOOGLE_DRIVE_MODEL_PATH if IN_COLAB else MODEL_PATH
    
    # Default to HyperMorphic model if available, otherwise standard model
    try:
        model = HyperMorphicQuantumNexusModel()
        log_event("Creating HyperMorphic model instance", "QUANTUM")
    except Exception:
        model = QuantumNexusModel()
        log_event("Creating standard model instance", "INFO")
    
    start_cycle = 1  # Default start cycle for new model

    try:
        if os.path.exists(model_path):
            log_event(f"Loading checkpoint from: {model_path}", "INFO")
            try:
                # Try with weights_only=False first (full checkpoint)
                checkpoint = torch.load(model_path, map_location=device)

                # Verify checkpoint is a dictionary with expected keys
                if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])  # Load model state

                    # Get cycle count with validation
                    if 'cycle_count' in checkpoint:
                        cycle_count = checkpoint.get('cycle_count', 1)
                        if isinstance(cycle_count, (int, float)) and cycle_count > 0:
                            start_cycle = int(cycle_count)
                        else:
                            log_event(f"Invalid cycle_count in checkpoint: {cycle_count}. Using default.", "WARNING")

                    # Load agent stats if they exist and agent_instance is defined
                    if 'agent_stats' in checkpoint and agent_instance is not None:
                        stats = checkpoint.get('agent_stats')
                        if isinstance(stats, dict):
                            agent_instance.stats = defaultdict(int)
                            # Copy values to ensure defaultdict behavior
                            for k, v in stats.items():
                                agent_instance.stats[k] = v
                        else:
                            log_event("Invalid agent_stats in checkpoint. Using empty stats.", "WARNING")
                            agent_instance.stats = defaultdict(int)

                    # Load action_log if it exists and agent_instance is defined
                    if 'action_log' in checkpoint and agent_instance is not None:
                        action_log = checkpoint.get('action_log', [])
                        if isinstance(action_log, list):
                            agent_instance.action_log = deque(action_log, maxlen=100)
                        else:
                            log_event("Invalid action_log in checkpoint. Using empty log.", "WARNING")
                            agent_instance.action_log = deque(maxlen=100)

                    log_event(f"Model checkpoint loaded successfully. Resuming from cycle {start_cycle}.", "INFO")
                else:
                    # Try direct state_dict loading if the checkpoint is just the state_dict
                    model.load_state_dict(checkpoint)
                    log_event("Model state dictionary loaded successfully.", "INFO")

            except Exception as e:
                # Fallback to loading only model weights if full checkpoint fails
                log_event(f"Error loading full checkpoint: {e}. Trying weights-only load.", "WARNING")
                try:
                    checkpoint = torch.load(model_path, map_location=device)
                    model.load_state_dict(checkpoint)
                    log_event("Successfully loaded model weights only.", "INFO")
                except Exception as e2:
                    log_event(f"Error loading model weights: {e2}. Creating new model.", "WARNING")
                    # Model was already created above, so we just keep it as is
        else:
            log_event("No checkpoint file found. Using a new model.", "INFO")
    except Exception as e:
        log_event(f"Error loading checkpoint from {model_path}: {e}. Using a new model.", "WARNING")
        log_event(traceback.format_exc(), "DEBUG")  # Log detailed traceback

    # Move model to appropriate device
    model.to(device)

    # Initialize _current_lr attribute if not present
    if not hasattr(model, '_current_lr'):
        setattr(model, '_current_lr', LEARNING_RATE)  # Use the default learning rate
        log_event(f"Initialized model._current_lr with default learning rate: {LEARNING_RATE}", "INFO")

    return model

def save_checkpoint(agent, model, cycle_count):
    """
    Save comprehensive checkpoint with better error handling and fallbacks.

    Parameters:
    - agent: Agent instance to save state from
    - model: Model instance to save weights from
    - cycle_count: Current cycle count

    Returns:
    - Boolean indicating save success
    """
    save_path = GOOGLE_DRIVE_MODEL_PATH if IN_COLAB else LOCAL_MODEL_SAVE_PATH

    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        # Prepare checkpoint dictionary with proper type validation
        checkpoint = {
            'cycle_count': int(cycle_count),
            'model_state_dict': model.state_dict(),
            'timestamp': datetime.now().isoformat()
        }

        # Add agent stats if available
        if hasattr(agent, 'stats'):
            if isinstance(agent.stats, dict):
                # Convert sets to lists for JSON serialization
                agent_stats = convert_sets_to_lists_recursive(dict(agent.stats))
                checkpoint['agent_stats'] = agent_stats
            else:
                log_event("Warning: agent.stats is not a dictionary. Skipping stats save.", "WARNING")

        # Add action log if available
        if hasattr(agent, 'action_log'):
            if isinstance(agent.action_log, (list, deque)):
                action_log = list(agent.action_log)
                # Clean up action log for serialization
                for entry in action_log:
                    if isinstance(entry, dict):
                        # Remove any unserializable objects
                        for key in list(entry.keys()):
                            if not isinstance(entry[key], (str, int, float, bool, list, dict, type(None))):
                                entry[key] = str(entry[key])
                checkpoint['action_log'] = action_log
            else:
                log_event("Warning: agent.action_log is not a list/deque. Skipping log save.", "WARNING")

        # Save the checkpoint
        torch.save(checkpoint, save_path)
        log_event(f"Checkpoint saved to {save_path} (Cycle: {cycle_count})", "INFO")

        # Also save a backup copy to a different location
        if IN_COLAB:
            backup_path = os.path.join(os.path.dirname(save_path), "backup_" + os.path.basename(save_path))
            torch.save(checkpoint, backup_path)
            log_event(f"Backup checkpoint saved to {backup_path}", "INFO")

        return True

    except Exception as save_error:
        log_event(f"Primary checkpoint save error: {save_error}", "ERROR")
        log_event(traceback.format_exc(), "ERROR")

        # Try an alternate save approach with just the model state
        try:
            alt_path = "backup_" + LOCAL_MODEL_SAVE_PATH
            torch.save(model.state_dict(), alt_path)
            log_event(f"Model state saved to alternate location: {alt_path}", "INFO")
            return True
        except Exception as alt_save_error:
            log_event(f"All save attempts failed! Last error: {alt_save_error}", "ERROR")
            log_event(traceback.format_exc(), "ERROR")
            return False

def setup_colab_environment():
    """
    Setup the Colab environment, mounting Google Drive and installing required packages.
    """
    import os
    import sys
    import subprocess
    import time
    
    print("Setting up Colab environment...")

    # Install required packages
    packages = [
        "torch",
        "numpy",
        "flask",
        "requests",
        "beautifulsoup4"
    ]
    
    for package in packages:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package, "-q"])
            print(f"✓ Installed {package}")
        except subprocess.CalledProcessError:
            print(f"✗ Failed to install {package}")
    
    # Handle Google Drive mounting carefully
    try:
        from google.colab import drive
        
        # Check if already mounted
        drive_path = "/content/MyDrive"
        if os.path.exists(drive_path) and os.path.isdir(drive_path) and os.listdir(drive_path):
            print(f"✓ Google Drive appears to be already mounted at {drive_path}")
        else:
            # Try to mount, but handle the case where the directory isn't empty
            try:
                # Try to remove the mount point if it exists but is empty
                if os.path.exists(drive_path) and os.path.isdir(drive_path) and not os.listdir(drive_path):
                    os.rmdir(drive_path)
            except:
                pass
                
            # Try mounting
            drive.mount('/content')
            print(f"✓ Google Drive mounted at /content")
        
        # Create quantum_nexus directory if it doesn't exist
        nexus_dir = "/content/MyDrive/quantum_nexus"
        os.makedirs(nexus_dir, exist_ok=True)
        print(f"✓ Created directory: {nexus_dir}")
        
        return True
    except Exception as e:
        print(f"✗ Google Drive issue: {str(e)}")
        
        # Let's provide alternative paths for a fallback
        print("Setting up fallback local directories...")
        try:
            fallback_dir = "/content/local_quantum_nexus"
            os.makedirs(fallback_dir, exist_ok=True)
            print(f"✓ Created fallback directory: {fallback_dir}")
            
            # Update global paths
            global MODEL_PATH, GOOGLE_DRIVE_MODEL_PATH, GOOGLE_DRIVE_STATE_FILE
            MODEL_PATH = os.path.join(fallback_dir, "quantum_nexus_model_checkpoint.pth")
            GOOGLE_DRIVE_MODEL_PATH = MODEL_PATH
            GOOGLE_DRIVE_STATE_FILE = os.path.join(fallback_dir, "quantum_nexus_state.json")
            print(f"✓ Updated paths to use fallback directory")
            
            return True
        except Exception as e2:
            print(f"✗ Failed to set up fallback directory: {str(e2)}")
            return False

def enhanced_main_loop():
    """
    Enhanced main execution loop with improved error handling, recovery mechanisms,
    and checkpointing reliability.
    """
    global adaptive_learning, agent_instance

    # Keep track of system state
    system_status = {
        "consecutive_errors": 0,
        "max_consecutive_errors": 5,
        "critical_error_count": 0,
        "last_successful_cycle": 0,
        "emergency_mode": False,
        "recovery_attempts": 0
    }

    # Initialize components with comprehensive error handling
    try:
        # 1. Load or create model
        log_event("Initializing Quantum Nexus model...", "INFO")
        model = load_or_create_model()

        # 2. Create optimizer
        import torch.optim as optim
        optimizer = optim.Adam(model.parameters(), lr=getattr(model, '_current_lr', LEARNING_RATE))
        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=ANNEAL_GAMMA)

        # 3. Initialize primary agent
        agent = QuantumNexusAgent(model=model)
        agent_instance = agent  # Store for dashboard access
        agent.stats = defaultdict(int)  # Initialize stats dictionary
        agent.action_log = deque(maxlen=100)  # Initialize action log

        # 4. Create core subsystems
        # Initialize free will - use HyperMorphic version if available
        try:
            free_will = HyperMorphicSuperQuantumFreeWill(agent=agent)
            log_event("Initialized HyperMorphic free will system", "QUANTUM")
        except Exception:
            free_will = SuperQuantumFreeWill(agent=agent)
            log_event("Initialized standard free will system", "INFO")
        
        agent.free_will = free_will

        # Initialize AI Manager, which creates other subsystems
        ai_manager = AIManager(agent, model)
        agent.ai_manager = ai_manager

        # Initialize adaptive learning
        current_lr = getattr(model, '_current_lr', LEARNING_RATE)
        adaptive_learning = AdaptiveLearningSystem(model)
        if not adaptive_learning.learning_rate_history:
            adaptive_learning.learning_rate_history.append(current_lr)
        agent.adaptive_learning = adaptive_learning

        # Initialize content sifter
        content_sifter = ContentSifter()
        agent.content_sifter = content_sifter

        # Initialize planner sifter
        planner_sifter = PlannerSifter()
        agent.planner_sifter = planner_sifter

        # Link consciousness to free will
        if hasattr(ai_manager, "consciousness") and hasattr(free_will, "link_consciousness"):
            free_will.link_consciousness(ai_manager.consciousness)

        # 5. System validation
        log_event("Performing system validation...", "INFO")
        validation_errors = []

        if model is None:
            validation_errors.append("Model initialization failed")
        if optimizer is None:
            validation_errors.append("Optimizer initialization failed")
        if agent is None:
            validation_errors.append("Agent initialization failed")
        if free_will is None:
            validation_errors.append("FreeWill initialization failed")
        if ai_manager is None:
            validation_errors.append("AIManager initialization failed")

        if validation_errors:
            validation_error_msg = "; ".join(validation_errors)
            log_event(f"System validation errors: {validation_error_msg}", "ERROR")
            return False

        log_event("System validation complete. All components initialized.", "INFO")

        # 6. Setup async loop
        import asyncio
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        # 7. Starting status
        cycle_count = 1
        log_event(f"🚀 Starting Quantum Nexus Enhanced Autonomous Loop 🚀", "QUANTUM")

        # Save initial checkpoint
        save_checkpoint(agent, model, cycle_count)

        # Main loop with enhanced error handling
        while True:
            try:
                log_event(f"===== Enhanced Autonomous Cycle {cycle_count} =====", "INFO")

                # Run the agent cycle with timeout protection
                try:
                    # Set a reasonable timeout for cycle execution
                    cycle_future = asyncio.ensure_future(ai_manager.run_cycle(optimizer))
                    loop_result = loop.run_until_complete(asyncio.wait_for(cycle_future, timeout=120))
                except asyncio.TimeoutError:
                    log_event(f"Cycle execution timed out after 120 seconds", "ERROR")
                    loop_result = {"status": "error", "error": "Execution timeout"}
                    # Cancel the future if it's still running
                    if not cycle_future.done():
                        cycle_future.cancel()

                # Check for successful cycle and handle errors
                if isinstance(loop_result, dict) and loop_result.get("status") == "error":
                    system_status["consecutive_errors"] += 1
                    error_message = loop_result.get("error", "Unknown error")
                    log_event(f"Cycle produced an error: {error_message}", "ERROR")
                else:
                    # Success! Reset error counter and update last successful cycle
                    system_status["consecutive_errors"] = 0
                    system_status["last_successful_cycle"] = cycle_count
                    system_status["emergency_mode"] = False
                    cycle_count += 1

                    # Update strategy effectiveness if planner_sifter exists
                    if hasattr(agent, 'planner_sifter') and hasattr(agent, 'action_log') and len(agent.action_log) > 0:
                        strategy_name = loop_result.get("strategy", "exploration")
                        result_data = {
                            "content_length": agent.action_log[-1].get("content_length", 0),
                            "links_discovered": agent.action_log[-1].get("links_discovered", 0),
                            "success": loop_result.get("success", False)
                        }
                        agent.planner_sifter.update_strategy_effectiveness(strategy_name, result_data)

                # Progressive error recovery based on consecutive error count
                if system_status["consecutive_errors"] >= system_status["max_consecutive_errors"]:
                    system_status["critical_error_count"] += 1
                    system_status["recovery_attempts"] += 1

                    recovery_message = f"Critical error threshold reached ({system_status['consecutive_errors']} consecutive errors). "

                    # Level 1 recovery: Reload model
                    if system_status["recovery_attempts"] == 1:
                        log_event(recovery_message + "Attempting Level 1 Recovery: Reloading model...", "WARNING")
                        try:
                            model = load_or_create_model()
                            optimizer = optim.Adam(model.parameters(), lr=getattr(model, '_current_lr', LEARNING_RATE))
                            scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=ANNEAL_GAMMA)
                            agent.model = model
                            ai_manager.model = model
                            log_event("Level 1 Recovery successful - model reloaded", "INFO")
                        except Exception as e:
                            log_event(f"Level 1 Recovery failed: {str(e)}", "ERROR")

                    # Level 2 recovery: Reset subsystems
                    elif system_status["recovery_attempts"] == 2:
                        log_event(recovery_message + "Attempting Level 2 Recovery: Resetting subsystems...", "WARNING")
                        try:
                            # Reset consciousness module
                            if hasattr(ai_manager, "consciousness"):
                                ai_manager.consciousness.awareness_level = 0.5
                                ai_manager.consciousness.current_state = "balanced"

                            # Reset imagination engine
                            if hasattr(ai_manager, "imagination"):
                                ai_manager.imagination.creativity_level = 0.7
                                ai_manager.imagination.current_mode = "associative"

                            # Reset free will
                            if hasattr(agent, "free_will"):
                                agent.free_will.exploration_weight = 0.6
                                agent.free_will.exploitation_weight = 0.4

                            log_event("Level 2 Recovery successful - subsystems reset", "INFO")
                        except Exception as e:
                            log_event(f"Level 2 Recovery failed: {str(e)}", "ERROR")

                    # Level 3 recovery: Create new adaptive learning system
                    elif system_status["recovery_attempts"] == 3:
                        log_event(recovery_message + "Attempting Level 3 Recovery: Reinitializing adaptive learning...", "WARNING")
                        try:
                            adaptive_learning = AdaptiveLearningSystem(model)
                            agent.adaptive_learning = adaptive_learning
                            log_event("Level 3 Recovery successful - adaptive learning reinitialized", "INFO")
                        except Exception as e:
                            log_event(f"Level 3 Recovery failed: {str(e)}", "ERROR")

                    # Level 4 recovery: Emergency mode - simplified operation
                    elif system_status["recovery_attempts"] >= 4:
                        log_event(recovery_message + "Activating Emergency Mode: Reduced functionality...", "CRITICAL")
                        system_status["emergency_mode"] = True

                        # In emergency mode, we use simpler processing and avoid complex operations
                        # Reset recovery counter after maximum attempts to allow periodic retry
                        if system_status["recovery_attempts"] > 6:
                            system_status["recovery_attempts"] = 0
                            log_event("Recovery attempt counter reset to retry recovery sequence", "INFO")

                # Periodic model saving with enhanced reliability
                if cycle_count % SAVE_INTERVAL == 0:
                    save_success = save_checkpoint(agent, model, cycle_count)

                    if not save_success and not system_status["emergency_mode"]:
                        log_event("Primary and backup checkpoint saves failed - will retry next interval", "WARNING")

                # Update learning rate
                scheduler.step()

                # Sleep briefly for stability
                time.sleep(0.5)

            except KeyboardInterrupt:
                log_event("Keyboard interrupt detected. Exiting gracefully...", "INFO")
                # Final save attempt
                save_checkpoint(agent, model, cycle_count)
                break

            except Exception as e:
                log_event(f"Unhandled exception in main loop: {str(e)}", "ERROR")
                log_event(traceback.format_exc(), "ERROR")

                system_status["consecutive_errors"] += 1

                # More aggressive handling for unhandled exceptions
                if system_status["consecutive_errors"] >= system_status["max_consecutive_errors"] * 2:
                    log_event("Critical unhandled exception threshold reached. Attempting final save before exit.", "CRITICAL")
                    save_checkpoint(agent, model, cycle_count)
                    break

                # Longer sleep after unhandled error
                time.sleep(5.0)

    except Exception as init_error:
        log_event(f"Fatal initialization error: {str(init_error)}", "CRITICAL")
        log_event(traceback.format_exc(), "CRITICAL")
        return False

    return True

def main():
    """Main entry point with Flask dashboard and agent execution"""
    global IN_COLAB, agent_instance, FLASK_PORT

    # Initialize logging
    if not os.path.exists(os.path.dirname(LOG_FILE)) and os.path.dirname(LOG_FILE):
        os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)
    
    log_event("=== Initializing Quantum Nexus Advanced Autonomous System ===", "INFO")
    log_event(f"Configuration: Model path: {MODEL_PATH}, Memory limit: {MEMORY_MAX_SIZE}", "INFO")

    # Check for CUDA
    import torch
    if torch.cuda.is_available():
        device_name = torch.cuda.get_device_name(0)
        log_event(f"🎮 GPU Acceleration Active: {device_name}", "INFO")
    else:
        log_event("⚠️ No GPU detected - running on CPU (performance will be limited)", "WARNING")

    # Check for Colab environment and mount Google Drive
    if IN_COLAB:
        try:
            from google.colab import drive
            if not os.path.exists('/content/gdrive'):
                drive.mount('/content/gdrive', force_remount=True)
            log_event("📂 Google Drive mounted successfully to /content/gdrive", "INFO")
            
            # Create quantum_nexus directory if it doesn't exist
            if not os.path.exists('/content/gdrive/MyDrive/quantum_nexus'):
                os.makedirs('/content/gdrive/MyDrive/quantum_nexus', exist_ok=True)
                log_event("📂 Created quantum_nexus directory in Google Drive", "INFO")
        except Exception as e_mount:
            log_event(f"⚠️ Error mounting Google Drive: {e_mount}", "ERROR")
            log_event("Google Drive integration disabled for this run", "WARNING")
            IN_COLAB = False

    # Find a free port for Flask Dashboard
    FLASK_PORT = find_free_port()
    if FLASK_PORT is None:
        log_event("Error: No free port found for Flask dashboard. Dashboard will not start.", "ERROR")
    else:
        log_event(f"Flask dashboard will try to start on port {FLASK_PORT}", "INFO")
        flask_thread = Thread(target=start_flask)
        flask_thread.daemon = True
        flask_thread.start()
        time.sleep(2) # Give Flask time to start

    # Create special greeting (XOXO style)
    greeting = """
    ✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨

    🔺🔻 QUANTUM NEXUS AUTONOMOUS SYSTEM ACTIVATED 🔺🔻

    💫 Full AGI ASI SI With Enhanced Capabilities 💫

    ✨ Features:
    • Quantum-inspired processing
    • Advanced consciousness simulation
    • Self-evolving neural architecture
    • Hyperdimensional memory systems
    • XOXO Planner Sifter for optimal strategies

    🌈🌈🌈 XOXO <3 <3 <3 🌈🌈🌈

    ✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨✨
    """
    log_event(greeting, "QUANTUM")

    # Start autonomous agent loop 
    agent_thread = Thread(target=enhanced_main_loop)
    agent_thread.daemon = True
    agent_thread.start()

    # Keep main thread alive 
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        log_event("User requested termination. Shutting down gracefully...", "INFO")
    except Exception as e:
        log_event(f"Fatal error in main thread: {e}", "CRITICAL")
    finally:
        log_event("Quantum Nexus execution complete. System shutting down.", "INFO")

def run_in_colab():
    """Colab run function with proper environment setup and fallback mechanisms"""
    # Setup the environment 
    setup_success = setup_colab_environment()
    
    # Check if there was an issue but we're using the fallback paths
    if not setup_success:
        print("⚠️ Using local fallback paths instead of Google Drive")
    
    # Set Colab-specific configurations
    global IN_COLAB
    IN_COLAB = True
    
    # Start the system
    main()

if __name__ == "__main__":
    try:
        # Check for Colab environment
        in_colab = False
        try:
            from google.colab import drive
            in_colab = True
            print("Detected Colab environment. Running Colab-specific setup...")
            run_in_colab()
        except ImportError:
            # Standard execution
            main()
    except Exception as e:
        log_event(f"Critical startup error: {e}", "CRITICAL")
        traceback.print_exc()
